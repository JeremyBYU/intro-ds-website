[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Week\nSlides\nClass Activity\nHW\n\n\n\n\nWeek 1\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 2\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 3\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 4\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 5\n🖥️\nNone\nNone\n\n\nWeek 6\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 7\n🖥️ 🖥️\n📋\n⌨️ ⌨️\n\n\nWeek 8\n🖥️ 🖥️\n📋\nProposal!!!\n\n\nWeek 9\n🖥️ 🖥️\n📋\nProposal, ⌨️\n\n\nWeek 10\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 11\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 12\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 13\n🖥️ 🖥️\n📋\n⌨️\n\n\nWeek 14"
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project Description (Core Capstone)",
    "section": "",
    "text": "Topic ideas due Wed, Feb 22\nProposal due Wed, Mar 22\nDraft report due Wed, Apr 12\nPeer review due Wed, Apr 19\nFinal report due Friday, Apr 28\nFinal Presentation due Final Exam Day"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project Description (Core Capstone)",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression or classification analysis.\nThe goal of the final project is for you to use data science methods to analyze data of your choosing. The data should broadly connect to topics you studied in the Core Curriculum (first-year seminar, foundations for the future and understanding the world domains, and the themed explorations). The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class and apply them to a data set to analyze it in a meaningful way.\nThis project will be your Core Capstone work. A Core Capstone Work is an assignment (or set of assignments) in students’ major capstone/seminar course where students integrate their Core Curriculum coursework with their major course work. Students will use their collected Signature Assignments (one assignment from each Core course, collected in Brightspace ePortfolios) to help in this work. This Core Capstone work will have six (6) deliverables that fulfill the learning outcomes of the Core Capstone."
  },
  {
    "objectID": "project-description.html#deliverables",
    "href": "project-description.html#deliverables",
    "title": "Project Description (Core Capstone)",
    "section": "Deliverables",
    "text": "Deliverables\nThe six deliverables for the final project are\n\nTopic Ideas - Identify 2-3 data sets that connect with the topics you covered in the Core Curriculum. Write a brief report of the data (Core Capstone Work LO1 and L02).\nProposal - Begin your analysis of your chosen dataset and its connection with two (2) of your signature assignments (Core Capstone Work LO1 and L02)\nDraft Report - First draft of your final report (Core Capstone Work LO1 and LO2)\nPeer Review - Formal peer review on another students project. This will provide important feedback to others and provide you an opportunity to serve and aid others (Core Capstone Work LO3).\nFinal Report - A written, reproducible report detailing your analysis (Core Capstone Work LO1). Include an ending discussion of how your coursework has helped you to be a leader in service to others (Core Capstone Work LO3)\nFinal Presentation - A brief 7-10 minutes presentation that summarizes your work (Core Capstone Work LO1).\n\nAll analyses must be done in Jupyter Notebooks stored in a shared Google Drive Folder. All components of the project must be reproducible (written in Google Collab notebook), except the presentation. The template for all deliverables can be found in here. You should copy each template component into your shared google drive folder and complete the assignment."
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project Description (Core Capstone)",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project that connect with the topics you covered in the Core Curriculum. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. Example project ideas and datasets can be found in the Example Project Ideas section. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by me.\nThe data sets should meet the following criteria:\n\nAt least 100 observations\nAt least 7 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable\n\nThe response variable can be quantitative or categorical.\nThis is the most important part of the dataset! Please identify this variable (column) and be clear that it is your response variable in your topic idea report!\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\n\nPlease ask me if you’re unsure whether your data set meets the criteria.\nPlease write a report in the file topic-ideas.ipynb which is in you shared Google Drive folder. Include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nDescribe the observations and the general characteristics being measured in the data.\nDescribe how the data set connect to your Core Studies.\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data. Remember the main focus of this class is to make a predictive model. You shoud frame the question as, “Can one reliabily predict X from Y”. Where X is some column in your dataset and Y are 6 or more different columns in your dataset.\n\n\n\nGlimpse of data\n\nUse the info function to provide an overview of each data set\n\nPrint your Jupyter notebook as PDF and submit to Brightspace\n\n\nSave Dataset\nPlease save each dataset in the data folder in your Google Drive Folder."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project Description (Core Capstone)",
    "section": "Project proposal",
    "text": "Project proposal\nPlease write a report in the file project-proposal.ipynb which is in you shared Google Drive folder.\nThe purpose of the project proposal is to help you think about your analysis strategy early. Here is a link to a sample proposal\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nbriefly describe its connection to Core classes\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nModel selection (linear regresssion, logistic regression, k-nearest neighbors, support vector machine, etc.)\n\nExplain which models might be suitable for making predictions\n\n\n\n\nSection 4 - Connect to Core Capstone Work\nDescribe how this dataset and analysis will connect with at least two (2) your signature assignments from the Core (Core Capstone Work LO2)\n\nDescribe each signature assignment\nDescribe how the dataset and analysis builds upon or connects with your prior work\n\nExample: You took Philosphy 106 (ethics) and you discussed the relationship between employers and employees. You submitted a signature assignment of a paper on unions (pro or against, it doesn’t matter). A possible dataset you could choose is the Right to Work Dataset and do an analysis on the relationship between state laws and their effect on union participation.\nNOTE - If you are a senior/junior and are are not particpating in the new Core you will not have any signature assignments. In this case, just describe in general how your dataset might connect with any general studies/core classes. I am leaving this very open.\n\n\nSection 5 - Data dictionary (aka code book)\nIn this section, you will create a markdown table that describes each varible in the dataset. Put your data in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPrint your Jupyter notebook as a PDF and submit to Brightspace.\n\n\nProposal grading\n\n\n\nTotal\n15 pts\n\n\n\n\nIntroduction\n5 pts\n\n\nData description\n4 pts\n\n\nAnalysis plan\n4 pts\n\n\nConnect Core\n1 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project Description (Core Capstone)",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.ipynb file in your shared Google Drive folder.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Include this papers connection to at least two core classes. Describe the data and definitions of key variables. It should also include some exploratory data analysis (EDA). All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults and Discussion\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics. If you are doing a classificaiton problem please include a confusion matrix.\nThis section also includes initial interpretations and conclusions drawn from the model.\nSubmission\nPrint your Jupyter notebook as a PDF and submit to Brightspace."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project Description (Core Capstone)",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process. Each invidual/team will be assigned two other teams’s projects to review. Each team should have finished their draft report by the due date. A class period in the following week will be dedicated to the peer review, and all reviews will be due by the end of that class session.\nThe peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\nI will assign which two teams your team will reivew shortely before the peer reivew process begins\n\nProcess and questions\nWrite your reviews inside your shared Google Drive folder called peer-review. Inside will be two Google documents for you to fill out for each team you review. Please ensure each review has the following:\n-   Names of team members that participated in this review: \\[FULL NAMES OF TEAM MEMBERS DOING THE REVIEW\\]\n\n-   Describe the goal of the project.\n\n-   Describe the data used or collected, if any.\n    If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\n\n-   Describe the approaches, tools, and methods that will be used.\n\n-   Is there anything that is unclear from the proposal?\n\n-   Provide constructive feedback on how the team might be able to improve their project.\n    Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\n\n-   What aspect of this project are you most interested in and would like to see highlighted in the presentation.\n\n-   Provide constructive feedback on any issues with file and/or code organization.\n\n-   (Optional) Any further comments or feedback?\nSubmission\nPrint your Google Docs as a PDF and submit to Brightspace. You should have two uploads for each team you review."
  },
  {
    "objectID": "project-description.html#final-report",
    "href": "project-description.html#final-report",
    "title": "Project Description (Core Capstone)",
    "section": "Final report",
    "text": "Final report\nYour written report must be completed in the written-report.ipynb file and must be reproducible. All team members should contribute to the report\nYou will submit the PDF of your final report on Brightspace.\nThe mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Include this papers connection to at least two core classes. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients or behavrior are used to support the key findings and conclusions. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed including the use of RMSE or confusion matrices.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Please also inlcude an ending discussion of how your coursework has helped (or prepared) you to be a leader in service to others\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages.\nSubmission\nExport your Jupyter notebook as a PDF and submit to Brightspace."
  },
  {
    "objectID": "project-description.html#presentation-slides",
    "href": "project-description.html#presentation-slides",
    "title": "Project Description (Core Capstone)",
    "section": "Presentation + slides",
    "text": "Presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nYour slides should be Google Slides and be created in your shared Google Drive folder presentation-slides\nThe slide deck should have no more than 8 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 8 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Highlights from EDA\nSlide 5: Final model\nSlide 6: Final model\nSlide 7: Interesting findings from the model\nSlide 8: Conclusions + future work\n\nSubmission\nExport your slides as a PDF and submit to brightsapce"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project Description (Core Capstone)",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the shared Google Drive should be neatly organized.\nHere is an example of how it should be organized:\n\n\n\nExample Google Drive Format\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the Google Drive folder."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project Description (Core Capstone)",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n15 pts\n\n\nDraft Report\n5 pts\n\n\nPeer review\n5 pts\n\n\nWritten report\n45 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "project-description.html#example-project-ideas",
    "href": "project-description.html#example-project-ideas",
    "title": "Project Description (Core Capstone)",
    "section": "Example Project Ideas",
    "text": "Example Project Ideas\n\nYou took Philosphy 106 (ethics) and you discussed the relationship between employers and employees. You submitted a signature assignment of a paper on unions (pro or against, it doesn’t matter). A possible dataset you could choose is the Right to Work Dataset and do an analysis on the relationship between state laws and their effect on union participation.\nYou took a BIO 101 class and discussed the biologcal mechanisms of diabetes. A possible dataset you could choose is the Diabetes Dataset and create a model that predict diabetes based on diagnostic measurements.\nYou took ARTS 104 (Creativity: Methods and Practices). A possible dataset you could choose is the Best Artworks of All Time and do a clustering analysis to see if there exists any patterns about great artists in our times (country, age, sex, etc.).\nYou took SOCIO 101. A possible dataset you could choose is the Student Performance Dataset that looks at student achievement in secondary education. You could create a regression model that predicts performance based upon a variety of factors."
  },
  {
    "objectID": "slides/lec-01.html#meet-the-professor",
    "href": "slides/lec-01.html#meet-the-professor",
    "title": "Welcome to CISC 482",
    "section": "Meet the professor",
    "text": "Meet the professor\n\n\n\nChemical Engineering & Computer Science @ BYU\nControl Systems Engineer & NASA Research Scientist\nRobotics PhD @ UMICH\n\nMachine Learning  Artificial Intelligence Computer Vision"
  },
  {
    "objectID": "slides/lec-01.html#welcome-to-computer-systems-seminar",
    "href": "slides/lec-01.html#welcome-to-computer-systems-seminar",
    "title": "Welcome to CISC 482",
    "section": "Welcome to Computer Systems Seminar",
    "text": "Welcome to Computer Systems Seminar\n\n\nThis course is designed to address various current technical and managerial problems encountered in computer information systems, including those dealing with hardware architecture, systems software, and applications software.\n\n\n\n\n\nTeach whatever you want!\n\n\n\n\n\nPicutre of James O’Brien"
  },
  {
    "objectID": "slides/lec-01.html#welcome-to-computer-systems-seminar-intro-to-data-science",
    "href": "slides/lec-01.html#welcome-to-computer-systems-seminar-intro-to-data-science",
    "title": "Welcome to CISC 482",
    "section": "Welcome to ~Computer Systems Seminar Intro to Data Science",
    "text": "Welcome to ~Computer Systems Seminar Intro to Data Science\n\nLogo for Class, Example of Group Separation"
  },
  {
    "objectID": "slides/lec-01.html#data-science",
    "href": "slides/lec-01.html#data-science",
    "title": "Welcome to CISC 482",
    "section": "Data Science",
    "text": "Data Science\n\n\nFirst Half\n\nProbability and Stats\nData Wrangling\nData Exploration and Viz\nRegression\nEvaluation Model Performance\n\n\nSecond Half\n\nSupervised Learning\nUnsupervised Learning\nCapstone Project Work"
  },
  {
    "objectID": "slides/lec-01.html#course-faq",
    "href": "slides/lec-01.html#course-faq",
    "title": "Welcome to CISC 482",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? Probability and Statistics and Basic Programming (Python)\nWill we be doing computing? Yes. We will use Python Jupyter Notebooks\nWill we learn the mathematical theory? Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression."
  },
  {
    "objectID": "slides/lec-01.html#course-learning-objectives",
    "href": "slides/lec-01.html#course-learning-objectives",
    "title": "Welcome to CISC 482",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nAnalyze real-world data to answer questions about multivariable relationships.\nFit and evaluate linear and logistic regression models.\nUnderstand and be able to use supervised and unsupervised learning methods\nAssess whether a proposed model is appropriate and describe its performance and limitations.\nUse Google Collab to write reproducible reports.\nCommunicate results from statistical analyses to a general audience."
  },
  {
    "objectID": "slides/lec-01.html#examples-of-data-science-in-practice",
    "href": "slides/lec-01.html#examples-of-data-science-in-practice",
    "title": "Welcome to CISC 482",
    "section": "Examples of data science in practice",
    "text": "Examples of data science in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model\nHow severe is the threat of wildfire to your home?"
  },
  {
    "objectID": "slides/lec-01.html#getting-to-know-you",
    "href": "slides/lec-01.html#getting-to-know-you",
    "title": "Welcome to CISC 482",
    "section": "Getting to know you",
    "text": "Getting to know you\n\nYour name, major\nIf you could write a book, what would it be about?\nWhat is your dream job?"
  },
  {
    "objectID": "slides/lec-01.html#zybooks",
    "href": "slides/lec-01.html#zybooks",
    "title": "Welcome to CISC 482",
    "section": "Zybooks",
    "text": "Zybooks\nOur required book is online: Zybooks\n\nSign in or create an account at learn.zybooks.com\nEnter zyBook code: SPRINGFIELDCISC482CastagnoSpring2023\nSubscribe (~$58)"
  },
  {
    "objectID": "slides/lec-01.html#zybook-walkthrough",
    "href": "slides/lec-01.html#zybook-walkthrough",
    "title": "Welcome to CISC 482",
    "section": "Zybook Walkthrough",
    "text": "Zybook Walkthrough"
  },
  {
    "objectID": "slides/lec-01.html#course-toolkit",
    "href": "slides/lec-01.html#course-toolkit",
    "title": "Welcome to CISC 482",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nZybook: https://learn.zybooks.com/)\nGoogle Collab (Free): https://colab.research.google.com/\nDiscussion forum: Use Brightspace Discussion. Activities -> Discussions\nAssignment submission and feedback: Submit through Brightspace\n\n\n\n\n\n\n\nImportant\n\n\nPlease purchase the book as soon as possible. Reading assignments are due this Sunday night!"
  },
  {
    "objectID": "slides/lec-01.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-01.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to CISC 482",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Introduce new content and prepare for lectures by completing the readings\nParticipate: Attend and actively participate in lectures and activities, office hours\nPractice: Practice applying statistical concepts and computing with application exercises during lecture (usually 30 minutes every Friday)\nPerform: Put together what you’ve learned to analyze real-world data\n\nHomework assignments x 6 (individual submission, but encouraged to collaborate)\nTwo exams\nTerm project (Core Capstone Work) presented during the final exam period"
  },
  {
    "objectID": "slides/lec-01.html#cadence",
    "href": "slides/lec-01.html#cadence",
    "title": "Welcome to CISC 482",
    "section": "Cadence",
    "text": "Cadence\n\n\nLectures: Posted before lecture. Encouraged to read before and follow along\nHWs: Posted Wednesday morning, due following Wednesday @ midnight\nProject: Deadlines throughout the semester, with some lab and lecture time dedicated to working on them, and most work done outside of class. This is a major part of this class."
  },
  {
    "objectID": "slides/lec-01.html#grading",
    "href": "slides/lec-01.html#grading",
    "title": "Welcome to CISC 482",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nClass Reading\n15%\n\n\nHomework\n30% (6% x 5)\n\n\nProject\n25%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-01.html#support",
    "href": "slides/lec-01.html#support",
    "title": "Welcome to CISC 482",
    "section": "Support",
    "text": "Support\n\nAttend office hours\nAsk and answer questions on the discussion forum on Brightspace\nAung Thet Htwe is our graduate assistant for this course.\n\nReserve and Appointment: https://calendly.com/aung-t-htwe/r-and-data-science-tutoring-with-aung"
  },
  {
    "objectID": "slides/lec-01.html#announcements",
    "href": "slides/lec-01.html#announcements",
    "title": "Welcome to CISC 482",
    "section": "Announcements",
    "text": "Announcements\n\nPosted on Brightspace and sent via email, be sure to check both regularly\nI’ll assume that you’ve read an announcement by the next “business” day"
  },
  {
    "objectID": "slides/lec-01.html#diversity-inclusion",
    "href": "slides/lec-01.html#diversity-inclusion",
    "title": "Welcome to CISC 482",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nIf you have a name that differs from those that appear in your official Springfield College records, please let me know!\nPlease let me know your preferred pronouns. You’ll also be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-01.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-01.html#late-work-waivers-regrades-policy",
    "title": "Welcome to CISC 482",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nNo Late Work is accepted unless do to illness or pre approved absence\nThree (3) of your lowest reading assignments will be dropped\nNo HW (labs) are dropped\nNo late submission for project work"
  },
  {
    "objectID": "slides/lec-01.html#collaboration-policy",
    "href": "slides/lec-01.html#collaboration-policy",
    "title": "Welcome to CISC 482",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice.\nExams must be completed individually. You may not discuss any aspect of the exam with peers. If you have questions, post as private questions on the course forum, only the teaching team will see and answer."
  },
  {
    "objectID": "slides/lec-01.html#sharing-reusing-code-policy",
    "href": "slides/lec-01.html#sharing-reusing-code-policy",
    "title": "Welcome to CISC 482",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course’s policy is that you may make use of any online resources (e.g. StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-01.html#most-importantly",
    "href": "slides/lec-01.html#most-importantly",
    "title": "Welcome to CISC 482",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you’re not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-01.html#five-tips-for-success",
    "href": "slides/lec-01.html#five-tips-for-success",
    "title": "Welcome to CISC 482",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work before class (reading, participation activity, challenge quizzes)\nAsk questions.\nDo the homeworks\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStart thinking about your project early"
  },
  {
    "objectID": "slides/lec-01.html#this-weeks-tasks",
    "href": "slides/lec-01.html#this-weeks-tasks",
    "title": "Welcome to CISC 482",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nSign up for the Zybook\nRead the syllabus\nHW1 - Markdown\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-20.html#schedule",
    "href": "slides/lec-20.html#schedule",
    "title": "CISC482 - Lecture20",
    "section": "Schedule",
    "text": "Schedule\n\nReading 8-1: April 12 @ 12PM, Wednesday\nReading 8-2: April 14 @ 12PM, Friday\nProject Draft Report: April 12 @ Midnight, Wednesday\nExample Report on Brightspace!!"
  },
  {
    "objectID": "slides/lec-20.html#today",
    "href": "slides/lec-20.html#today",
    "title": "CISC482 - Lecture20",
    "section": "Today",
    "text": "Today\n\nReview Draft Report\nSupport Vector Machine\nKernels"
  },
  {
    "objectID": "slides/lec-20.html#terms",
    "href": "slides/lec-20.html#terms",
    "title": "CISC482 - Lecture20",
    "section": "Terms",
    "text": "Terms\n\nSupport Vector Machine (SVM) is a supervised learning algorithm that uses hyperplanes to divide data into different classes.\nHyperplane is a flat surface that is one dimension lower than the input feature space.\n\n\n\nIn a two-dimensional feature space, a hyperplane is a _____\nIn a three-dimensional feature space, a hyperplane is a ____"
  },
  {
    "objectID": "slides/lec-20.html#visual-example-1",
    "href": "slides/lec-20.html#visual-example-1",
    "title": "CISC482 - Lecture20",
    "section": "Visual Example 1",
    "text": "Visual Example 1"
  },
  {
    "objectID": "slides/lec-20.html#visual-example-2",
    "href": "slides/lec-20.html#visual-example-2",
    "title": "CISC482 - Lecture20",
    "section": "Visual Example 2",
    "text": "Visual Example 2"
  },
  {
    "objectID": "slides/lec-20.html#visual-example-3",
    "href": "slides/lec-20.html#visual-example-3",
    "title": "CISC482 - Lecture20",
    "section": "Visual Example 3",
    "text": "Visual Example 3"
  },
  {
    "objectID": "slides/lec-20.html#separating-classes-with-planes",
    "href": "slides/lec-20.html#separating-classes-with-planes",
    "title": "CISC482 - Lecture20",
    "section": "Separating Classes with Planes",
    "text": "Separating Classes with Planes"
  },
  {
    "objectID": "slides/lec-20.html#separating-classes-with-planes-optimal",
    "href": "slides/lec-20.html#separating-classes-with-planes-optimal",
    "title": "CISC482 - Lecture20",
    "section": "Separating Classes with Planes (Optimal)",
    "text": "Separating Classes with Planes (Optimal)"
  },
  {
    "objectID": "slides/lec-20.html#advantages-ans-disadvantages",
    "href": "slides/lec-20.html#advantages-ans-disadvantages",
    "title": "CISC482 - Lecture20",
    "section": "Advantages ans Disadvantages",
    "text": "Advantages ans Disadvantages\nAdvantages:\n\nFlexible\nLow storage required\n\nDisadvantages:\n\nPrefers balanced data (we have workarounds)\nMany hyperparametes (very true)"
  },
  {
    "objectID": "slides/lec-20.html#more-terms",
    "href": "slides/lec-20.html#more-terms",
    "title": "CISC482 - Lecture20",
    "section": "More Terms!",
    "text": "More Terms!\n\nSupport vectors are the sample data points, which are closest to the hyperplane.\n\nThese data points will define the separating line\n\nMargin is a separation gap between the two lines on the closest data points"
  },
  {
    "objectID": "slides/lec-20.html#visual-example-of-terms",
    "href": "slides/lec-20.html#visual-example-of-terms",
    "title": "CISC482 - Lecture20",
    "section": "Visual Example of Terms",
    "text": "Visual Example of Terms\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=[[-2, 5], [0, 0]], cluster_std=1)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10.)\nmodel.fit(X, Y)\n\nplot_svm(X, Y, model, ax=ax, plot_df=False);\n# ax.legend([\"Class 0\", \"Class 1\"])"
  },
  {
    "objectID": "slides/lec-20.html#margin-terminology",
    "href": "slides/lec-20.html#margin-terminology",
    "title": "CISC482 - Lecture20",
    "section": "Margin Terminology",
    "text": "Margin Terminology\n\nA dataset is well-separated if a hyperplane can divide the dataset so that all the instances of one class fall on one side of the hyperplane, and all instances not in that class fall on the other side.\n\n\nWas the prior example well separated?\n\n\n\n\n\n\n\n\nTip\n\n\nThe closest instances to the hyperplane are the hyperplane’s support vectors. The support vectors are the only instances that determine, or support, the hyperplane."
  },
  {
    "objectID": "slides/lec-20.html#not-well-separated",
    "href": "slides/lec-20.html#not-well-separated",
    "title": "CISC482 - Lecture20",
    "section": "Not well Separated",
    "text": "Not well Separated\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=[[-2, 5], [0, 0]], cluster_std=1.5)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10.)\nmodel.fit(X, Y)\n\nplot_svm(X, Y, model, ax=ax, plot_df=False, plot_sv=False, plot_hp=False, plot_margin=False);"
  },
  {
    "objectID": "slides/lec-20.html#sklearn-svm",
    "href": "slides/lec-20.html#sklearn-svm",
    "title": "CISC482 - Lecture20",
    "section": "Sklearn SVM",
    "text": "Sklearn SVM\n\nWe have a hyperparameter named C that we can change to adjust how to handle misclassifications\nThe C parameter tells the SVM optimization how much you want to avoid misclassifying each training example\nsvm.SVC has as keyword argument C, the smaller C more penalized the model to make a smaller margin"
  },
  {
    "objectID": "slides/lec-20.html#visual-example",
    "href": "slides/lec-20.html#visual-example",
    "title": "CISC482 - Lecture20",
    "section": "Visual Example",
    "text": "Visual Example"
  },
  {
    "objectID": "slides/lec-20.html#example---large-c",
    "href": "slides/lec-20.html#example---large-c",
    "title": "CISC482 - Lecture20",
    "section": "Example - Large C",
    "text": "Example - Large C\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=[[-2, 5], [0, 0]], cluster_std=1.5)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10000.0)\nmodel.fit(X, Y)\n\nplot_svm(X, Y, model, ax=ax, plot_df=False, plot_sv=True, plot_hp=True, plot_margin=True);"
  },
  {
    "objectID": "slides/lec-20.html#example---small-c",
    "href": "slides/lec-20.html#example---small-c",
    "title": "CISC482 - Lecture20",
    "section": "Example - Small C",
    "text": "Example - Small C\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=[[-2, 5], [0, 0]], cluster_std=1.5)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=0.000001)\nmodel.fit(X, Y)\n\nplot_svm(X, Y, model, ax=ax, plot_df=False, plot_sv=True, plot_hp=True, plot_margin=True);"
  },
  {
    "objectID": "slides/lec-20.html#multiclass",
    "href": "slides/lec-20.html#multiclass",
    "title": "CISC482 - Lecture20",
    "section": "Multiclass",
    "text": "Multiclass\n\n\nWhat do you do if you have a multiple classes, not binary.\nCan one hyperplane separate a space into three regions?\nBut we have a trick! If we have three classes (A,B,C) we are trying to seperate, we just train three binary models!\n\nA vs (B,C)\nB vs (A,C)\nC vs (A,B)"
  },
  {
    "objectID": "slides/lec-20.html#one-vs-rest-ovr",
    "href": "slides/lec-20.html#one-vs-rest-ovr",
    "title": "CISC482 - Lecture20",
    "section": "One Vs Rest (OVR)",
    "text": "One Vs Rest (OVR)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=False, plot_sv=False, plot_hp=False, plot_margin=False, classes=['A', 'B', 'C']);"
  },
  {
    "objectID": "slides/lec-20.html#a-vs-bc",
    "href": "slides/lec-20.html#a-vs-bc",
    "title": "CISC482 - Lecture20",
    "section": "A vs (B,C)",
    "text": "A vs (B,C)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nmask = Y == 2\nY[mask] = 1\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_sv=True, plot_hp=True, plot_margin=True, classes=['A', 'B/C'], plot_df=False);"
  },
  {
    "objectID": "slides/lec-20.html#b-vs-ac",
    "href": "slides/lec-20.html#b-vs-ac",
    "title": "CISC482 - Lecture20",
    "section": "B vs (A,C)",
    "text": "B vs (A,C)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nmask = Y == 1\nY[~mask] = 2\nY[mask] = 0\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_sv=True, plot_hp=True, plot_margin=True, classes=['B', 'A/C'], plot_df=False);"
  },
  {
    "objectID": "slides/lec-20.html#c-vs-ab",
    "href": "slides/lec-20.html#c-vs-ab",
    "title": "CISC482 - Lecture20",
    "section": "C vs (A,B)",
    "text": "C vs (A,B)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nmask = Y == 2\nY[~mask] = 1\nY[mask] = 0\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_sv=True, plot_hp=True, plot_margin=True, classes=['C', 'A/B'], plot_df=False);"
  },
  {
    "objectID": "slides/lec-20.html#one-vs-rest-ovr-1",
    "href": "slides/lec-20.html#one-vs-rest-ovr-1",
    "title": "CISC482 - Lecture20",
    "section": "One Vs Rest (OVR)",
    "text": "One Vs Rest (OVR)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=False, plot_margin=False, classes=['A', 'B', 'C']);"
  },
  {
    "objectID": "slides/lec-20.html#motivation",
    "href": "slides/lec-20.html#motivation",
    "title": "CISC482 - Lecture20",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/lec-20.html#feature-space",
    "href": "slides/lec-20.html#feature-space",
    "title": "CISC482 - Lecture20",
    "section": "Feature Space",
    "text": "Feature Space\n\nThe feature space is the higher dimensional space that we can transform our data into!\nThe feature space can be much bigger than just 1 dimensional above our data\nSome features spaces can make it easier for us to seperate our data!\nIt can be expensive to create these feauture spaces\n\nComputation!\nMemory!"
  },
  {
    "objectID": "slides/lec-20.html#inner-product",
    "href": "slides/lec-20.html#inner-product",
    "title": "CISC482 - Lecture20",
    "section": "Inner Product",
    "text": "Inner Product\n\nThe SVM makes decisions about what class you belong to by determining what side of the hyperplane you are on\nI have never explained the process, but the basic idea is it used inner products (dot product).\nYou dont need to understand how it uses the inner products to make decisions. Just trust that that is what it does."
  },
  {
    "objectID": "slides/lec-20.html#dot-product-1",
    "href": "slides/lec-20.html#dot-product-1",
    "title": "CISC482 - Lecture20",
    "section": "Dot Product 1",
    "text": "Dot Product 1"
  },
  {
    "objectID": "slides/lec-20.html#dot-product-2",
    "href": "slides/lec-20.html#dot-product-2",
    "title": "CISC482 - Lecture20",
    "section": "Dot Product 2",
    "text": "Dot Product 2\n\nSo basically all we really need is to get the dot product of these points in the higher dimension."
  },
  {
    "objectID": "slides/lec-20.html#example-unseperable",
    "href": "slides/lec-20.html#example-unseperable",
    "title": "CISC482 - Lecture20",
    "section": "Example Unseperable",
    "text": "Example Unseperable"
  },
  {
    "objectID": "slides/lec-20.html#mapping",
    "href": "slides/lec-20.html#mapping",
    "title": "CISC482 - Lecture20",
    "section": "Mapping",
    "text": "Mapping"
  },
  {
    "objectID": "slides/lec-20.html#example-transformed",
    "href": "slides/lec-20.html#example-transformed",
    "title": "CISC482 - Lecture20",
    "section": "Example Transformed",
    "text": "Example Transformed"
  },
  {
    "objectID": "slides/lec-20.html#kernel-trick",
    "href": "slides/lec-20.html#kernel-trick",
    "title": "CISC482 - Lecture20",
    "section": "Kernel Trick",
    "text": "Kernel Trick"
  },
  {
    "objectID": "slides/lec-20.html#svm-kernel-example",
    "href": "slides/lec-20.html#svm-kernel-example",
    "title": "CISC482 - Lecture20",
    "section": "SVM Kernel Example",
    "text": "SVM Kernel Example"
  },
  {
    "objectID": "slides/lec-20.html#svm-with-linear-kernel",
    "href": "slides/lec-20.html#svm-with-linear-kernel",
    "title": "CISC482 - Lecture20",
    "section": "SVM With Linear Kernel",
    "text": "SVM With Linear Kernel\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=True, plot_margin=True, classes=['A', 'B']);"
  },
  {
    "objectID": "slides/lec-20.html#svm-with-polynomial-kernel",
    "href": "slides/lec-20.html#svm-with-polynomial-kernel",
    "title": "CISC482 - Lecture20",
    "section": "SVM With Polynomial Kernel",
    "text": "SVM With Polynomial Kernel\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"poly\", degree=2, C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=False, plot_margin=False, classes=['A', 'B']);"
  },
  {
    "objectID": "slides/lec-20.html#link-to-theory",
    "href": "slides/lec-20.html#link-to-theory",
    "title": "CISC482 - Lecture20",
    "section": "Link to Theory",
    "text": "Link to Theory\nDerivation\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-04.html#schedule",
    "href": "slides/lec-04.html#schedule",
    "title": "CISC482 - Lecture04",
    "section": "Schedule",
    "text": "Schedule\n\nReading 2-2: Jan 27 @ 12PM, Friday\nReading 2-3: Feb 1 @ 12PM, Wednesday\nHW2: Feb 1 @ Midnight, Wednesday\nReading 3-1: Feb 3 @ 12PM, Friday"
  },
  {
    "objectID": "slides/lec-04.html#cs-faculty-candidate",
    "href": "slides/lec-04.html#cs-faculty-candidate",
    "title": "CISC482 - Lecture04",
    "section": "CS Faculty Candidate",
    "text": "CS Faculty Candidate\n\nMedha Pujari is here today!\nPlease attend a meet and greet at 3:15 in SBSC 112\nExtra Credit!"
  },
  {
    "objectID": "slides/lec-04.html#terms",
    "href": "slides/lec-04.html#terms",
    "title": "CISC482 - Lecture04",
    "section": "Terms",
    "text": "Terms\n\nRandom Process - action or process which results in an outcome determined by chance\nOutcome - one possible result from a random process\nSample Space - set of all possible outcomes of a random process and denoted as \\(S\\)\nEvent - an outcome or collection of outcomes from a sample space. Typically denoted with Capital letters: \\(A,B,C\\), etc.\nProbability of an event A - denoted, \\(P(A)\\), number of outcomes of A divided by the total number of equally likely outcomes in the sample space \\(S\\). How often does \\(A\\) occur in \\(S\\)"
  },
  {
    "objectID": "slides/lec-04.html#visualizing-probability",
    "href": "slides/lec-04.html#visualizing-probability",
    "title": "CISC482 - Lecture04",
    "section": "Visualizing Probability",
    "text": "Visualizing Probability"
  },
  {
    "objectID": "slides/lec-04.html#operations",
    "href": "slides/lec-04.html#operations",
    "title": "CISC482 - Lecture04",
    "section": "Operations",
    "text": "Operations\n\nCompliment of A - denoted not \\(A\\), \\(A'\\), \\(\\bar{A}\\), \\(A^C\\), \\(\\neg A\\)\n\nWe will use \\(A'\\)\n\nUnion of two events \\(A\\) and \\(B\\) is denoted as \\(A\\) or \\(B\\). Consists of all outcomes in \\(A\\) or \\(B\\)\nIntersection of two events \\(A\\) and \\(B\\) is denoted as \\(A\\) and \\(B\\). Consists of only outcomes in \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "slides/lec-04.html#practice",
    "href": "slides/lec-04.html#practice",
    "title": "CISC482 - Lecture04",
    "section": "Practice",
    "text": "Practice\n\n\n\nCompliment\nUnion\nIntersection\nDifference"
  },
  {
    "objectID": "slides/lec-04.html#cheat-sheet",
    "href": "slides/lec-04.html#cheat-sheet",
    "title": "CISC482 - Lecture04",
    "section": "Cheat Sheet",
    "text": "Cheat Sheet"
  },
  {
    "objectID": "slides/lec-04.html#three-foundational-rules",
    "href": "slides/lec-04.html#three-foundational-rules",
    "title": "CISC482 - Lecture04",
    "section": "Three Foundational Rules",
    "text": "Three Foundational Rules\n\n\nThe probability of any event is non-negative, \\(P(A) >= 0\\)\nThe probability of the sample space is \\(P(S) = 1\\)\nIf A and be are disjoint events, \\(P(A \\; or \\; B) = P(A) + P(B)\\)\n\nNo outcomes in common."
  },
  {
    "objectID": "slides/lec-04.html#three-derived-rules",
    "href": "slides/lec-04.html#three-derived-rules",
    "title": "CISC482 - Lecture04",
    "section": "Three Derived Rules",
    "text": "Three Derived Rules\n\n\\(P(A') = 1 - P(A)\\)\n\\(P(A \\; or \\; B) = P(A) + P(B) - P(A \\; and \\; B)\\)\nindependent events: \\(P(A\\; and\\; B) = P(A) * P(B)\\) \n\n\n\n\n\n\n\nTip\n\n\nYou dont need to derive any of these, you just need to know them! Know the 6 rules"
  },
  {
    "objectID": "slides/lec-04.html#practice-1",
    "href": "slides/lec-04.html#practice-1",
    "title": "CISC482 - Lecture04",
    "section": "Practice",
    "text": "Practice\n\n\n\nSize\n1\n2\n3\n4\n5\n6\n7+\n\n\n\n\nProportion\n0.29\n0.35\n0.15\n0.12\n0.06\n0.02\n0.01\n\n\n\n\n\nFind the probability of randomly selecting a household with a size of more than 1\n\n0.71\n\nFind the probability of randomly selecting a household with a size of 1 or more than 1\n\n1.0\n\nFind the probability of randomly selecting a household with a size of 5 or more\n\n0.09\n\nFind the probability of randomly selecting a household with size 1 or 5 or more\n\n0.38\n\nOne household will be randomly selected from all households, and then a second household will be randomly selected from all households. Find the probability that both selected households are of size 1.\n\n0.08"
  },
  {
    "objectID": "slides/lec-04.html#prepare-to-be-amazed",
    "href": "slides/lec-04.html#prepare-to-be-amazed",
    "title": "CISC482 - Lecture04",
    "section": "Prepare to be amazed",
    "text": "Prepare to be amazed\n\nThe probability of an event occurring can also be determined under the condition of knowing another event has occurred.\nA conditioning probability is a measure of the likelihood of one event occurring, given another event occurred\n\nThe conditional probability of event \\(A\\) given event \\(B\\), denoted \\(P(A|B)\\)\n\\[\nP(A|B) = \\frac{P(A\\; and\\; B)}{P(B)} = \\frac{P(A \\cap B)}{P(B)}\n\\]"
  },
  {
    "objectID": "slides/lec-04.html#thinking-independently",
    "href": "slides/lec-04.html#thinking-independently",
    "title": "CISC482 - Lecture04",
    "section": "Thinking Independently",
    "text": "Thinking Independently\n\n\nWhat if we have independent events, \\(A,B\\)\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\\(P(A|B) = \\frac{P(A) * P(B)}{P(B)}\\)\n\\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "slides/lec-04.html#conditional-example",
    "href": "slides/lec-04.html#conditional-example",
    "title": "CISC482 - Lecture04",
    "section": "Conditional Example",
    "text": "Conditional Example"
  },
  {
    "objectID": "slides/lec-04.html#bayes-rule",
    "href": "slides/lec-04.html#bayes-rule",
    "title": "CISC482 - Lecture04",
    "section": "Bayes Rule",
    "text": "Bayes Rule\n\nSometimes you don’t have nice table with all these probabilities filled out\nYou may not know \\(P(A \\cap B)\\). Is all lost?\n\nNo Bayes rule to the rescue!\n\n\n\\[\nP(A | B) = \\frac{P(B | A) * P(A)}{P(B)}\n\\]\n\n\n\n\n\n\nTip\n\n\nThis rule is the foundation of data science and machine learning!"
  },
  {
    "objectID": "slides/lec-04.html#example",
    "href": "slides/lec-04.html#example",
    "title": "CISC482 - Lecture04",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lec-04.html#random-variables",
    "href": "slides/lec-04.html#random-variables",
    "title": "CISC482 - Lecture04",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom Variable - Defines numerical values for a random processes outcome.\nTypically denote them like: \\(X,Y, Z\\)\nDiscrete vs Continuous - Flip of coin (1 or 0), GPA of students\nProbability Distribution - gives probability of an occurrence for a a random variable\n\nThis distribution can be visualized! We often use histograms."
  },
  {
    "objectID": "slides/lec-04.html#normal-distribution",
    "href": "slides/lec-04.html#normal-distribution",
    "title": "CISC482 - Lecture04",
    "section": "Normal Distribution",
    "text": "Normal Distribution"
  },
  {
    "objectID": "slides/lec-04.html#normal-details",
    "href": "slides/lec-04.html#normal-details",
    "title": "CISC482 - Lecture04",
    "section": "Normal Details",
    "text": "Normal Details"
  },
  {
    "objectID": "slides/lec-04.html#bernoulli-distribution",
    "href": "slides/lec-04.html#bernoulli-distribution",
    "title": "CISC482 - Lecture04",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\nTrue or False, 1 or 0, Success or Failure\n\\(\\pi\\), determines the probaility of success\nWhat is \\(\\pi\\) for this bernoulli distribution"
  },
  {
    "objectID": "slides/lec-04.html#binomial-distribution",
    "href": "slides/lec-04.html#binomial-distribution",
    "title": "CISC482 - Lecture04",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nA random variable describing the number of “successes” from independent observations of a random process in which the probability of a success is \\(\\pi\\) follows a binomial distribution\nFlip a coin 10 times (trials) count how many heads.\n\nRepeat 10,000 times to create samples\n\n\\(n\\) how many trials, \\(\\pi\\) probability of one success\n\\(\\mu = n * \\pi\\)         \\(\\sigma^2 = n * \\pi * (1- \\pi)\\)"
  },
  {
    "objectID": "slides/lec-04.html#flipping-a-coin-10-times",
    "href": "slides/lec-04.html#flipping-a-coin-10-times",
    "title": "CISC482 - Lecture04",
    "section": "Flipping a Coin 10 Times",
    "text": "Flipping a Coin 10 Times\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nDoes this seem right?"
  },
  {
    "objectID": "slides/lec-24.html#schedule",
    "href": "slides/lec-24.html#schedule",
    "title": "CISC482 - Lecture24",
    "section": "Schedule",
    "text": "Schedule\n\nReturning Draft Reports\nFinal Report - April 28 @ Midnight\nFinal Presentation - May 5, Final"
  },
  {
    "objectID": "slides/lec-24.html#today",
    "href": "slides/lec-24.html#today",
    "title": "CISC482 - Lecture24",
    "section": "Today",
    "text": "Today\n\nRecap Peer Review\nReview Hierarchal Clustering\nPrincipal Component Analysis"
  },
  {
    "objectID": "slides/lec-24.html#my-review",
    "href": "slides/lec-24.html#my-review",
    "title": "CISC482 - Lecture24",
    "section": "My Review",
    "text": "My Review\n\nOverall we did a great job! You can find my and your peers feedback on Brightspace.\n\nIts also inside your shared Google Drive folder under PeerReview\n\nI particularly appreciated the detailed walkthrough that many of you did.\nIt was very interesting seeing all your ideas!\nI did see a few common errors that I wanted to bring up."
  },
  {
    "objectID": "slides/lec-24.html#common-issues-1",
    "href": "slides/lec-24.html#common-issues-1",
    "title": "CISC482 - Lecture24",
    "section": "Common Issues 1",
    "text": "Common Issues 1\n\nDon’t have empty cells.\nDon’t have grammatical errors.\nLabel all your axes in your figures.\nThe primary goal of this paper is to create a predictive model.\n\nYou need to be clear in your introduction what your response variable is and if you are doing classification or regression."
  },
  {
    "objectID": "slides/lec-24.html#common-issues-2",
    "href": "slides/lec-24.html#common-issues-2",
    "title": "CISC482 - Lecture24",
    "section": "Common Issues 2",
    "text": "Common Issues 2\n\nBe sure to have a good amount of detail in your report that explains WHY you are doing what you are doing.\nI recommend making tables showing your results. You can do this by creating a data frame of your results or by using this website to make markdown tables.\nPlease review this document to learn how to write equations in google collab."
  },
  {
    "objectID": "slides/lec-24.html#common-issues-3",
    "href": "slides/lec-24.html#common-issues-3",
    "title": "CISC482 - Lecture24",
    "section": "Common Issues 3",
    "text": "Common Issues 3\n\nIf you are doing regression, please make Residual Plots and report \\(R^2\\) metric.\nIf you are doing classification, please report metrics such as accuracy, precision, recall, and f1 score."
  },
  {
    "objectID": "slides/lec-24.html#types-of-clustering",
    "href": "slides/lec-24.html#types-of-clustering",
    "title": "CISC482 - Lecture24",
    "section": "Types of Clustering",
    "text": "Types of Clustering\n\nAgglomerative hierarchical clustering is _______\nDivisive hierarchical clustering is ____"
  },
  {
    "objectID": "slides/lec-24.html#measures-of-similarity",
    "href": "slides/lec-24.html#measures-of-similarity",
    "title": "CISC482 - Lecture24",
    "section": "Measures of Similarity",
    "text": "Measures of Similarity\n\nThe single linkage method calculates the distance between a pair of samples, one from each cluster, that are the _____.\nThe complete linkage method calculates the distance between a pair of samples, one from each cluster, that are the _____.\nThe centroid linkage method calculates the distance between the ________ of two clusters."
  },
  {
    "objectID": "slides/lec-24.html#questions---1",
    "href": "slides/lec-24.html#questions---1",
    "title": "CISC482 - Lecture24",
    "section": "Questions - 1",
    "text": "Questions - 1\n\n\nWhich two samples should be used to determine similarity using the Single linkage? Complete Linkage?"
  },
  {
    "objectID": "slides/lec-24.html#questions---1-1",
    "href": "slides/lec-24.html#questions---1-1",
    "title": "CISC482 - Lecture24",
    "section": "Questions - 1",
    "text": "Questions - 1\n\n\nWhich two samples should be used to determine similarity using the Single linkage? Complete Linkage?"
  },
  {
    "objectID": "slides/lec-24.html#terminology",
    "href": "slides/lec-24.html#terminology",
    "title": "CISC482 - Lecture24",
    "section": "Terminology",
    "text": "Terminology\n\nA _______ is a ________ that shows the order in which clusters are grouped together and the distances between clusters.\nRead it from BOTTOM up!"
  },
  {
    "objectID": "slides/lec-24.html#section",
    "href": "slides/lec-24.html#section",
    "title": "CISC482 - Lecture24",
    "section": "",
    "text": "A _____ is a branch of a dendogram/vertical line.\nA _____ is a horizontal line that connects two ______, height gives the distance between clusters.\nA _____ is the terminal end of each _______ in a dendrogram, which represents a single sample."
  },
  {
    "objectID": "slides/lec-24.html#question-threshold",
    "href": "slides/lec-24.html#question-threshold",
    "title": "CISC482 - Lecture24",
    "section": "Question Threshold",
    "text": "Question Threshold\n\n\n\n\nHow many total samples?\nHow many clusters would there be with dashed blue line?\nHow many clusters would there be with dashed red line?"
  },
  {
    "objectID": "slides/lec-24.html#terms",
    "href": "slides/lec-24.html#terms",
    "title": "CISC482 - Lecture24",
    "section": "Terms",
    "text": "Terms\n\nPrincipal component analysis, or PCA, is a dimensionality reduction technique.\nIt can be used to compress data.\nLets say we have \\(n=10\\) points that such that \\(x \\in\\mathbf{R}^2\\)\nPCA can reduce the data be \\(n=10\\) points inside \\(R^{1}\\)\nIt does this by finding the component, or feature vector that contains the largest variability\nLets looks at an example"
  },
  {
    "objectID": "slides/lec-24.html#example-1-start",
    "href": "slides/lec-24.html#example-1-start",
    "title": "CISC482 - Lecture24",
    "section": "Example 1 Start",
    "text": "Example 1 Start\n\nFind me a vector or axis that when you project the data to it maximizes the variance"
  },
  {
    "objectID": "slides/lec-24.html#projection",
    "href": "slides/lec-24.html#projection",
    "title": "CISC482 - Lecture24",
    "section": "Projection",
    "text": "Projection"
  },
  {
    "objectID": "slides/lec-24.html#example-1-animation",
    "href": "slides/lec-24.html#example-1-animation",
    "title": "CISC482 - Lecture24",
    "section": "Example 1 Animation",
    "text": "Example 1 Animation"
  },
  {
    "objectID": "slides/lec-24.html#example-2-start",
    "href": "slides/lec-24.html#example-2-start",
    "title": "CISC482 - Lecture24",
    "section": "Example 2 Start",
    "text": "Example 2 Start\n\n\n\n\nCode\nx = np.arange(0, 10) + np.random.randn(10) * 0.1\ny = x * 1 + np.random.randn(10) * 0.3\nsns.scatterplot(x=x,y=y)\n\n\n<Axes: >\n\n\n\n\n\n\nFind me a vector or axis that when you project the data to it maximizes the variance"
  },
  {
    "objectID": "slides/lec-24.html#example-2---first-principle-component",
    "href": "slides/lec-24.html#example-2---first-principle-component",
    "title": "CISC482 - Lecture24",
    "section": "Example 2 - First Principle Component",
    "text": "Example 2 - First Principle Component\n\n\n\n\nCode\nx = np.arange(0, 10) + np.random.randn(10) * 0.1\ny = x * 1 + np.random.randn(10) * 0.3\nax = sns.scatterplot(x=x,y=y)\nax.plot([0, 10], [0, 10], ls='dashed', c='r')\n\n\n\n\n\n\nDo you see how this vector will maximize the variance? The vector is \\([1, 1]\\) or \\([.707, .707]\\) when normalized."
  },
  {
    "objectID": "slides/lec-24.html#intuition",
    "href": "slides/lec-24.html#intuition",
    "title": "CISC482 - Lecture24",
    "section": "Intuition",
    "text": "Intuition"
  },
  {
    "objectID": "slides/lec-24.html#back-to-example-2",
    "href": "slides/lec-24.html#back-to-example-2",
    "title": "CISC482 - Lecture24",
    "section": "Back to Example 2",
    "text": "Back to Example 2\n\n\n\n\nCode\nX = np.column_stack([x, y])\ndf = pd.DataFrame(X, columns=['x', 'y'])\ndf\n\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -0.20\n      -0.28\n    \n    \n      1\n      0.83\n      0.66\n    \n    \n      2\n      2.03\n      2.07\n    \n    \n      3\n      3.24\n      3.10\n    \n    \n      4\n      4.11\n      4.50\n    \n    \n      5\n      5.17\n      5.23\n    \n    \n      6\n      6.01\n      6.13\n    \n    \n      7\n      7.14\n      7.04\n    \n    \n      8\n      7.97\n      8.35\n    \n    \n      9\n      9.06\n      8.84\n    \n  \n\n\n\n\n\n\n\n\nCode\npc1 = np.array([[.707], [.707]])\nprint(f\"PC1 = {pc1.flatten()}\")\npc1_vals = X @ pc1\ndf = pd.DataFrame(pc1_vals, columns=['PC1'])\ndf\n\n\nPC1 = [0.7 0.7]\n\n\n\n\n\n\n  \n    \n      \n      PC1\n    \n  \n  \n    \n      0\n      -0.34\n    \n    \n      1\n      1.05\n    \n    \n      2\n      2.89\n    \n    \n      3\n      4.48\n    \n    \n      4\n      6.09\n    \n    \n      5\n      7.35\n    \n    \n      6\n      8.58\n    \n    \n      7\n      10.02\n    \n    \n      8\n      11.54\n    \n    \n      9\n      12.66"
  },
  {
    "objectID": "slides/lec-24.html#compression-from-example-2",
    "href": "slides/lec-24.html#compression-from-example-2",
    "title": "CISC482 - Lecture24",
    "section": "Compression from Example 2",
    "text": "Compression from Example 2\n\n\n\n\nCode\nsns.scatterplot(x=x,y=y, label=\"Raw\");\n\n\n\n\n\n\n\n\nCode\npoints = pc1.flatten() * pc1_vals\nax = sns.scatterplot(x=x,y=y, label='Raw')\nax.scatter(x=points[:, 0],y=points[:, 1], label='Compressed')\nplt.legend();\n\n\n\n\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-07.html#schedule",
    "href": "slides/lec-07.html#schedule",
    "title": "CISC482 - Lecture07",
    "section": "Schedule",
    "text": "Schedule\n\nReading 3-2: Feb 8 @ 12PM, Wednesday\nReading 4-1: Feb 10 @ 12PM, Friday\nReading 4-2: Feb 15 @ 12PM, Wednesday\nHW3: Feb 15 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-07.html#cs-faculty-candidate",
    "href": "slides/lec-07.html#cs-faculty-candidate",
    "title": "CISC482 - Lecture07",
    "section": "CS Faculty Candidate",
    "text": "CS Faculty Candidate\n\nRazuan Hossain is here on Friday\nPlease attend a meet and greet at 3:15 in SBSC 112\nExtra Credit!"
  },
  {
    "objectID": "slides/lec-07.html#bar-chart",
    "href": "slides/lec-07.html#bar-chart",
    "title": "CISC482 - Lecture07",
    "section": "Bar Chart",
    "text": "Bar Chart\n\nA bar chart: groups on one axis, rectangles with heights that represent the number of samples."
  },
  {
    "objectID": "slides/lec-07.html#example-bar-chart",
    "href": "slides/lec-07.html#example-bar-chart",
    "title": "CISC482 - Lecture07",
    "section": "Example Bar Chart",
    "text": "Example Bar Chart\n\n\nCode\nspecies_count = df.groupby(['species'])['species'].count()\nax = sns.barplot(x=species_count.index.values, y=species_count.values)\nax.bar_label(ax.containers[0])\nax;"
  },
  {
    "objectID": "slides/lec-07.html#numerical-features",
    "href": "slides/lec-07.html#numerical-features",
    "title": "CISC482 - Lecture07",
    "section": "Numerical Features",
    "text": "Numerical Features\n\nSometimes we want visualize numerical features\nWe are interested in showing users the variation of this feature\nHistograms\nDensity Plots\nBox Plots"
  },
  {
    "objectID": "slides/lec-07.html#histogram-bar-chart",
    "href": "slides/lec-07.html#histogram-bar-chart",
    "title": "CISC482 - Lecture07",
    "section": "Histogram Bar Chart",
    "text": "Histogram Bar Chart"
  },
  {
    "objectID": "slides/lec-07.html#histogram-bar-chart-1",
    "href": "slides/lec-07.html#histogram-bar-chart-1",
    "title": "CISC482 - Lecture07",
    "section": "Histogram Bar Chart",
    "text": "Histogram Bar Chart\n\n\n\n\n\nDividing the numerical feature into small regions and then count the number of values in each region\nNotice - axis have labels!\nNotice - bar widths are small enough that you can see the distributions shape\n\n\n\n\nWhat do you notice about this distribution?\nWhat is (roughly) the most likely flipper length\n\n\n\n\n\n\nCode\nax = sns.histplot(data=df, x=\"flipper_length_mm\");"
  },
  {
    "objectID": "slides/lec-07.html#histogram-bar-horizontal-bar-chart",
    "href": "slides/lec-07.html#histogram-bar-horizontal-bar-chart",
    "title": "CISC482 - Lecture07",
    "section": "Histogram Bar Horizontal Bar Chart",
    "text": "Histogram Bar Horizontal Bar Chart\n\n\n\nSometimes its better to have the bar chart grow horizontal\n\n\n\n\nCode\nspecies_count = df.groupby(['species'])['species'].count()\nax = sns.barplot(data=df, y=\"island\", x=\"body_mass_g\", errorbar=None)\nax.bar_label(ax.containers[0]);"
  },
  {
    "objectID": "slides/lec-07.html#density-plot",
    "href": "slides/lec-07.html#density-plot",
    "title": "CISC482 - Lecture07",
    "section": "Density Plot",
    "text": "Density Plot\n\n\n\nA plot that approximates the density function of the distribution for the feature.\nDensity plots can be thought of as a smoothed histogram\n\n\n\n\nCode\nax = sns.kdeplot(data=df, x=\"body_mass_g\");"
  },
  {
    "objectID": "slides/lec-07.html#density-plot-with-histogram",
    "href": "slides/lec-07.html#density-plot-with-histogram",
    "title": "CISC482 - Lecture07",
    "section": "Density Plot with Histogram",
    "text": "Density Plot with Histogram\n\n\nCode\nax = sns.histplot(data=df, x=\"body_mass_g\", kde=True);"
  },
  {
    "objectID": "slides/lec-07.html#box-plot",
    "href": "slides/lec-07.html#box-plot",
    "title": "CISC482 - Lecture07",
    "section": "Box Plot",
    "text": "Box Plot\n\n\n\nA visual representation of the summary:\n\nminimum, maximum\nfirst quartile, median, third quartiles\noutliers\n\n\n\n\n\nCode\nax = sns.boxplot(data=df, x=\"body_mass_g\");"
  },
  {
    "objectID": "slides/lec-07.html#boxen-plot",
    "href": "slides/lec-07.html#boxen-plot",
    "title": "CISC482 - Lecture07",
    "section": "Boxen Plot",
    "text": "Boxen Plot\n\n\n\nPlots more quantiles\nProvides more information about the shape of the distribution, particularly in the tails.\n50%, 25%, 12.5%, 6.25%, 3.13%\n\n\n\n\nCode\nax = sns.boxenplot(data=df, x=\"body_mass_g\");"
  },
  {
    "objectID": "slides/lec-07.html#two-features",
    "href": "slides/lec-07.html#two-features",
    "title": "CISC482 - Lecture07",
    "section": "Two features",
    "text": "Two features\n\nWe visualized a single feature uses one axis to display the feature value and another axis to display the value’s frequency\nHowever, what if we want to communicate or investigate the relationship between two variables?\n\nScatterplot, line plots, etc!"
  },
  {
    "objectID": "slides/lec-07.html#example-scatter-plot-data",
    "href": "slides/lec-07.html#example-scatter-plot-data",
    "title": "CISC482 - Lecture07",
    "section": "Example Scatter Plot Data",
    "text": "Example Scatter Plot Data\n\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4"
  },
  {
    "objectID": "slides/lec-07.html#example-scatter-plot",
    "href": "slides/lec-07.html#example-scatter-plot",
    "title": "CISC482 - Lecture07",
    "section": "Example Scatter Plot",
    "text": "Example Scatter Plot\n\n\n\nEvery point is a data point\nPoint out the best tips\nPoint out the worst tips\n\n\n\n\nCode\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\", aspect=1.5);"
  },
  {
    "objectID": "slides/lec-07.html#example-line-plot",
    "href": "slides/lec-07.html#example-line-plot",
    "title": "CISC482 - Lecture07",
    "section": "Example Line Plot",
    "text": "Example Line Plot\n\n\n\nCode\ntips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\", aspect=1.5, kind='line');"
  },
  {
    "objectID": "slides/lec-07.html#combined-plot",
    "href": "slides/lec-07.html#combined-plot",
    "title": "CISC482 - Lecture07",
    "section": "Combined Plot",
    "text": "Combined Plot\n\nData points and linear regression model"
  },
  {
    "objectID": "slides/lec-07.html#looks-can-be-deceiving",
    "href": "slides/lec-07.html#looks-can-be-deceiving",
    "title": "CISC482 - Lecture07",
    "section": "Looks can be deceiving",
    "text": "Looks can be deceiving\n\nAlways show the points…"
  },
  {
    "objectID": "slides/lec-07.html#categorical-features-1",
    "href": "slides/lec-07.html#categorical-features-1",
    "title": "CISC482 - Lecture07",
    "section": "Categorical Features",
    "text": "Categorical Features\n\nSometimes we have a categorical feature and see the difference between two different sets\n\nCategory: Man, Woman. Feature: Height\nCategory: Friday, Saturday, Sunday. Feature: Avg. Tip\nCategory: Penguin Species. Feature: Body Mass"
  },
  {
    "objectID": "slides/lec-07.html#density-plot-with-histogram-species-as-hue",
    "href": "slides/lec-07.html#density-plot-with-histogram-species-as-hue",
    "title": "CISC482 - Lecture07",
    "section": "Density Plot with Histogram, Species as Hue",
    "text": "Density Plot with Histogram, Species as Hue\n\n\nCode\nax = sns.histplot(data=df, x=\"body_mass_g\", kde=True, hue='species');"
  },
  {
    "objectID": "slides/lec-07.html#bar-plot-sex-as-hue",
    "href": "slides/lec-07.html#bar-plot-sex-as-hue",
    "title": "CISC482 - Lecture07",
    "section": "Bar Plot, Sex as Hue",
    "text": "Bar Plot, Sex as Hue\n\n\nCode\n# Draw a nested barplot by species and sex\ng = sns.catplot(\n    data=df, kind=\"bar\", \n    x=\"species\", y=\"body_mass_g\", hue=\"sex\", \n    errorbar=None, alpha=0.6, dodge=False\n)"
  },
  {
    "objectID": "slides/lec-07.html#bar-plot-sex-as-hue-1",
    "href": "slides/lec-07.html#bar-plot-sex-as-hue-1",
    "title": "CISC482 - Lecture07",
    "section": "Bar Plot, Sex as Hue",
    "text": "Bar Plot, Sex as Hue\n\n\n\nWhat species and sex combination is the most prevalent?\nWhich species has the smallest numerical difference between sexes?\n\n\n\n\nCode\n# Draw a nested barplot by species and sex\ng = sns.catplot(\n    data=df, kind=\"bar\", \n    x=\"species\", y=\"body_mass_g\", hue=\"sex\", \n    errorbar=None, alpha=0.6\n)"
  },
  {
    "objectID": "slides/lec-07.html#strip-plot",
    "href": "slides/lec-07.html#strip-plot",
    "title": "CISC482 - Lecture07",
    "section": "Strip Plot",
    "text": "Strip Plot\n\n\nCode\ng = sns.catplot(\n    data=df, kind=\"strip\", \n    y=\"species\", x=\"bill_length_mm\", hue=\"sex\",  aspect=2, alpha=0.6\n)"
  },
  {
    "objectID": "slides/lec-07.html#swarm-plot",
    "href": "slides/lec-07.html#swarm-plot",
    "title": "CISC482 - Lecture07",
    "section": "Swarm Plot",
    "text": "Swarm Plot\n\nA swarm plot is a scatter plot with points jittered off the lines for the categorical feature so the points do not overlap.\nA swarm plot is useful for small datasets, but with an increasing number of points, the plots get too wide."
  },
  {
    "objectID": "slides/lec-07.html#swarm-plot-example",
    "href": "slides/lec-07.html#swarm-plot-example",
    "title": "CISC482 - Lecture07",
    "section": "Swarm Plot Example",
    "text": "Swarm Plot Example\n\n\nCode\ng = sns.swarmplot(\n    data=df, \n    y=\"species\", x=\"bill_length_mm\", hue='sex', alpha=0.6\n)"
  },
  {
    "objectID": "slides/lec-07.html#violin",
    "href": "slides/lec-07.html#violin",
    "title": "CISC482 - Lecture07",
    "section": "Violin",
    "text": "Violin\n\n\nCode\nsns.violinplot(data=df, x=\"bill_length_mm\", y=\"species\");"
  },
  {
    "objectID": "slides/lec-07.html#violin-explained",
    "href": "slides/lec-07.html#violin-explained",
    "title": "CISC482 - Lecture07",
    "section": "Violin Explained",
    "text": "Violin Explained"
  },
  {
    "objectID": "slides/lec-07.html#matplotlib",
    "href": "slides/lec-07.html#matplotlib",
    "title": "CISC482 - Lecture07",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\nThe most popular graphic library (2D and 3D)\nOut of the box, plots are functional but don’t look too nice"
  },
  {
    "objectID": "slides/lec-07.html#matplotlib-example",
    "href": "slides/lec-07.html#matplotlib-example",
    "title": "CISC482 - Lecture07",
    "section": "Matplotlib Example",
    "text": "Matplotlib Example"
  },
  {
    "objectID": "slides/lec-07.html#seaborn",
    "href": "slides/lec-07.html#seaborn",
    "title": "CISC482 - Lecture07",
    "section": "Seaborn",
    "text": "Seaborn\n\nSeaborn is a Python data visualization library based on matplotlib\nNice and simple api that integrates very nicely with pandas\nJust pass it a data frame and call functions like\n\nhistplot\nscatterplot\nrelplot\nboxplot"
  },
  {
    "objectID": "slides/lec-07.html#seaborn-examples",
    "href": "slides/lec-07.html#seaborn-examples",
    "title": "CISC482 - Lecture07",
    "section": "Seaborn Examples",
    "text": "Seaborn Examples\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-09.html#schedule",
    "href": "slides/lec-09.html#schedule",
    "title": "CISC482 - Lecture09",
    "section": "Schedule",
    "text": "Schedule\n\nReading 5-1: Feb 22 @ 12PM, Wednesday\nReading 5-2: Feb 24 @ 12PM, Friday\nTopic Ideas - Feb 22 @ Midnight"
  },
  {
    "objectID": "slides/lec-09.html#today",
    "href": "slides/lec-09.html#today",
    "title": "CISC482 - Lecture09",
    "section": "Today",
    "text": "Today\n\nReview Exam\nReview Topic Ideas\nMore Exploratory Data Analysis"
  },
  {
    "objectID": "slides/lec-09.html#most-missed-questions",
    "href": "slides/lec-09.html#most-missed-questions",
    "title": "CISC482 - Lecture09",
    "section": "Most Missed Questions",
    "text": "Most Missed Questions\n\nStudent spending at bookstore is normally distributed with a mean of $130 and a standard deviation of $30. Approximately what percentage of students spend less than $100 a month. Options: - 16, 20, 33, 50\nSpringfield College wants to give gift cards to the top 3% of spenders. What should be monthly spending cutoff to determine which students get gift cards?\n\n130, 160, 186, 220\n\nHypothesis Testing, p-value of 0.01, using 95% confidence when making a decison to reject or accept null hypothesis."
  },
  {
    "objectID": "slides/lec-09.html#normal-distribution",
    "href": "slides/lec-09.html#normal-distribution",
    "title": "CISC482 - Lecture09",
    "section": "Normal Distribution",
    "text": "Normal Distribution"
  },
  {
    "objectID": "slides/lec-09.html#normal-distribution-solution",
    "href": "slides/lec-09.html#normal-distribution-solution",
    "title": "CISC482 - Lecture09",
    "section": "Normal Distribution Solution",
    "text": "Normal Distribution Solution\n\n50% of students spend more than 130. 34% of students spend between 100 and 130. Thefore 50+34=84% students spend more than 100. Thefore 100-84=16% students spend less than 100.\n50% of students spend at least 130. Roughly 34+14=48% of students spend between 130 and 190 dollars. Thefore 50+48=98% students spend less than 190. Thefore the top 2% spends 190 or more. If we want the top 3% it will be must a little less. Looking at the options 186 is the best one."
  },
  {
    "objectID": "slides/lec-09.html#hypothesis-testing-solution",
    "href": "slides/lec-09.html#hypothesis-testing-solution",
    "title": "CISC482 - Lecture09",
    "section": "Hypothesis Testing Solution",
    "text": "Hypothesis Testing Solution\n\nReject null hypothesis when p-value is less than 0.05 and accept the alternative hypothesis."
  },
  {
    "objectID": "slides/lec-09.html#purpose",
    "href": "slides/lec-09.html#purpose",
    "title": "CISC482 - Lecture09",
    "section": "Purpose",
    "text": "Purpose\n\nFind 2-3 datasets that you are interested and are connected to your core signature assignments\nVerify the data is suitable for analysis\nPractice writing concisely and clearly about complex topics"
  },
  {
    "objectID": "slides/lec-09.html#requirements",
    "href": "slides/lec-09.html#requirements",
    "title": "CISC482 - Lecture09",
    "section": "Requirements",
    "text": "Requirements\n\nData Requirements\nPaper Requirements"
  },
  {
    "objectID": "slides/lec-09.html#data-requirements",
    "href": "slides/lec-09.html#data-requirements",
    "title": "CISC482 - Lecture09",
    "section": "Data Requirements",
    "text": "Data Requirements\n\nAt least 100 observations\nAt least 8 columns\nAt least 6 of the columns must be useful and unique predictor variables.\nAt least one variable that can be identified as a reasonable response variable\n\nThe response variable can be quantitative or categorical.\n\nObservations should reasonably meet the independence condition."
  },
  {
    "objectID": "slides/lec-09.html#paper-requirements",
    "href": "slides/lec-09.html#paper-requirements",
    "title": "CISC482 - Lecture09",
    "section": "Paper Requirements",
    "text": "Paper Requirements\n\nIntroduction\nResearch Question\nGlimpse of Data"
  },
  {
    "objectID": "slides/lec-09.html#introduction-section",
    "href": "slides/lec-09.html#introduction-section",
    "title": "CISC482 - Lecture09",
    "section": "Introduction Section",
    "text": "Introduction Section\n\nState source of data\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nDescribe the observations and the general characteristics being measured in the data.\nDescribe how the data set connect to your Core Studies."
  },
  {
    "objectID": "slides/lec-09.html#research-question",
    "href": "slides/lec-09.html#research-question",
    "title": "CISC482 - Lecture09",
    "section": "Research Question",
    "text": "Research Question\n\nDescribe a research question you’re interested in answering using this data\nCan you accurately predict whether a person will survive in the titanic data set? What features would be most important to make that prediction?"
  },
  {
    "objectID": "slides/lec-09.html#glimpse-of-data",
    "href": "slides/lec-09.html#glimpse-of-data",
    "title": "CISC482 - Lecture09",
    "section": "Glimpse of Data",
    "text": "Glimpse of Data\n\nPlease print out the results of the info() function of the dataframe\nAlso print out the first few rows of data head()"
  },
  {
    "objectID": "slides/lec-09.html#example-template",
    "href": "slides/lec-09.html#example-template",
    "title": "CISC482 - Lecture09",
    "section": "Example Template",
    "text": "Example Template\n\nBrightspace assignment\nLink to template to follow\nCopy and put in your shared google drive folder\nPrint PDF and submit to brightspace"
  },
  {
    "objectID": "slides/lec-09.html#steps",
    "href": "slides/lec-09.html#steps",
    "title": "CISC482 - Lecture09",
    "section": "Steps",
    "text": "Steps\n\nUnderstand the Data\n\nSize of the dataset (rows,cols), features (categorical, numerical).\n\nIdentify Relationships between features\n\nDirection and strength of correlation\n\nDescribe the shape of the data\n\nSymmetric, Skewed\n\nDetect outliers and missing values\n\nBox an Whisker!"
  },
  {
    "objectID": "slides/lec-09.html#understand-the-data",
    "href": "slides/lec-09.html#understand-the-data",
    "title": "CISC482 - Lecture09",
    "section": "Understand the Data",
    "text": "Understand the Data\n\nNumber of Rows, Columns?\nWhat features are categorical vs numeric?\nAny missing data?"
  },
  {
    "objectID": "slides/lec-09.html#example-data",
    "href": "slides/lec-09.html#example-data",
    "title": "CISC482 - Lecture09",
    "section": "Example Data",
    "text": "Example Data\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), object(3)\nmemory usage: 21.6+ KB\nNone\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.10\n      18.70\n      181.00\n      3,750.00\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.50\n      17.40\n      186.00\n      3,800.00\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.30\n      18.00\n      195.00\n      3,250.00\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.70\n      19.30\n      193.00\n      3,450.00\n      female\n      2007"
  },
  {
    "objectID": "slides/lec-09.html#relationship-1",
    "href": "slides/lec-09.html#relationship-1",
    "title": "CISC482 - Lecture09",
    "section": "Relationship 1",
    "text": "Relationship 1"
  },
  {
    "objectID": "slides/lec-09.html#relationship-2",
    "href": "slides/lec-09.html#relationship-2",
    "title": "CISC482 - Lecture09",
    "section": "Relationship 2",
    "text": "Relationship 2"
  },
  {
    "objectID": "slides/lec-09.html#relationship-3",
    "href": "slides/lec-09.html#relationship-3",
    "title": "CISC482 - Lecture09",
    "section": "Relationship 3",
    "text": "Relationship 3"
  },
  {
    "objectID": "slides/lec-09.html#quantifying-the-relationship",
    "href": "slides/lec-09.html#quantifying-the-relationship",
    "title": "CISC482 - Lecture09",
    "section": "Quantifying the Relationship",
    "text": "Quantifying the Relationship\n\nCovariance - measure of the joint variability of two random variables\nThe sign of the covariance, therefore, shows whether it is positive or negative relationship\n\\(Cov(X,Y)= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{n}\\)\nThe magnitude doesn’t tell you much….we need a way to normalize it….Correlation to the rescue!"
  },
  {
    "objectID": "slides/lec-09.html#correlation",
    "href": "slides/lec-09.html#correlation",
    "title": "CISC482 - Lecture09",
    "section": "Correlation",
    "text": "Correlation\n\nA metric between -1 and +1\n\\(r(X,Y)= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{x})^2 \\cdot \\sum_{i=1}^n(y_i - \\bar{y})}}\\)\n\\(r(X,Y)= \\frac{Cov(X,Y)}{\\sigma_x \\cdot \\sigma_y}\\)"
  },
  {
    "objectID": "slides/lec-09.html#example-data-1",
    "href": "slides/lec-09.html#example-data-1",
    "title": "CISC482 - Lecture09",
    "section": "Example Data",
    "text": "Example Data\n\n\ndata = np.array([\n  [1, 2,   -2,     3],\n  [2, 4,   -4,     3],\n  [3, 3.5, -3.5,   3],\n  [4, 7.5, -7.5,   3],\n  [5, 10.2, -10.2, 3],\n  [6, 12.1, -12.1, 3],\n])\ncov = np.cov(data, rowvar=False)\nprint('Cov:', np.array2string(cov, prefix='Cov: ', formatter={'float_kind':lambda x: f\"{x:5.1f}\"}))\nprint(np.var(data[:, 0], ddof=1))\nprint(np.var(data[:, 1], ddof=1))\n\n\nCov: [[  3.5   7.3  -7.3   0.0]\n      [  7.3  16.3 -16.3   0.0]\n      [ -7.3 -16.3  16.3   0.0]\n      [  0.0   0.0   0.0   0.0]]\n3.5\n16.307"
  },
  {
    "objectID": "slides/lec-09.html#example-correlation-graph",
    "href": "slides/lec-09.html#example-correlation-graph",
    "title": "CISC482 - Lecture09",
    "section": "Example Correlation Graph",
    "text": "Example Correlation Graph\n\n\ncorr = np.corrcoef(data, rowvar=False)\ndataplot = sns.heatmap(corr, cmap=\"vlag\", annot=True)"
  },
  {
    "objectID": "slides/lec-09.html#secret-weapon",
    "href": "slides/lec-09.html#secret-weapon",
    "title": "CISC482 - Lecture09",
    "section": "Secret Weapon",
    "text": "Secret Weapon\n\nNow its time to show you my secret weapon\nDont tell anyone : )\nWhat I am about to show you single handedly landed me an offer to work at a big company in California\nNow for the story"
  },
  {
    "objectID": "slides/lec-09.html#background",
    "href": "slides/lec-09.html#background",
    "title": "CISC482 - Lecture09",
    "section": "Background",
    "text": "Background\n\nMy job was to predict where on the airport runway the airplane was using cameras\nSpecifally your lateral position on the runway"
  },
  {
    "objectID": "slides/lec-09.html#the-story-continued",
    "href": "slides/lec-09.html#the-story-continued",
    "title": "CISC482 - Lecture09",
    "section": "The Story (Continued)",
    "text": "The Story (Continued)\n\n\nI created a model that predicted cross-track position.\n\n\\(\\hat{CT} = f(\\text{FlightState}) = f(\\theta, \\psi, \\phi, \\text{image, etc.})\\)\n\nWe had the true CT position (from GPS) to compare my model against. \\(Error = \\hat{CT} - CT\\)\nIt worked great most of the time!\nHowever, we kept getting really large errors sometimes\nNo obvious pattern?\nWHY!?!?!"
  },
  {
    "objectID": "slides/lec-09.html#data",
    "href": "slides/lec-09.html#data",
    "title": "CISC482 - Lecture09",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\n  \n    \n      \n      ct_error\n      roll\n      pitch\n      yaw\n      downtracks\n      crosstracks\n    \n  \n  \n    \n      0\n      1.63\n      7.99\n      2.55\n      3.91\n      0.00\n      0.00\n    \n    \n      1\n      3.33\n      2.93\n      -2.10\n      3.89\n      4.02\n      1.10\n    \n    \n      2\n      2.43\n      0.12\n      -0.72\n      -0.41\n      8.03\n      2.20\n    \n    \n      3\n      3.24\n      -1.19\n      -2.21\n      -15.68\n      12.05\n      3.30\n    \n    \n      4\n      3.16\n      4.12\n      -3.75\n      -5.25\n      16.06\n      4.39"
  },
  {
    "objectID": "slides/lec-09.html#pair-plot",
    "href": "slides/lec-09.html#pair-plot",
    "title": "CISC482 - Lecture09",
    "section": "Pair Plot",
    "text": "Pair Plot\nsns.pairplot(df_bad)"
  },
  {
    "objectID": "slides/lec-09.html#more-data",
    "href": "slides/lec-09.html#more-data",
    "title": "CISC482 - Lecture09",
    "section": "More Data!",
    "text": "More Data!\n\n\n\n\n\n\n  \n    \n      \n      ct_error\n      roll\n      pitch\n      yaw\n      downtracks\n      crosstracks\n      alt\n    \n  \n  \n    \n      0\n      1.63\n      7.99\n      2.55\n      3.91\n      0.00\n      0.00\n      203.27\n    \n    \n      1\n      3.33\n      2.93\n      -2.10\n      3.89\n      4.02\n      1.10\n      206.84\n    \n    \n      2\n      2.43\n      0.12\n      -0.72\n      -0.41\n      8.03\n      2.20\n      205.51\n    \n    \n      3\n      3.24\n      -1.19\n      -2.21\n      -15.68\n      12.05\n      3.30\n      207.36\n    \n    \n      4\n      3.16\n      4.12\n      -3.75\n      -5.25\n      16.06\n      4.39\n      207.70"
  },
  {
    "objectID": "slides/lec-09.html#the-error",
    "href": "slides/lec-09.html#the-error",
    "title": "CISC482 - Lecture09",
    "section": "The Error",
    "text": "The Error\n\nLong story short, the GPS altitude was broken!\nIt was reporting that the airpane was off the ground!\nMy algorithm took into account your height off the ground.\nAt first, my bosses would not beleive me! It was a $10,000 GPS!\nBut the correlation plots and all my subsequent research convinced them"
  },
  {
    "objectID": "slides/lec-09.html#the-true-error",
    "href": "slides/lec-09.html#the-true-error",
    "title": "CISC482 - Lecture09",
    "section": "The True Error",
    "text": "The True Error\n\nSomeone forgot to renew the subscription service for the High Precision GPS!"
  },
  {
    "objectID": "slides/lec-09.html#key-workflow-and-graphs",
    "href": "slides/lec-09.html#key-workflow-and-graphs",
    "title": "CISC482 - Lecture09",
    "section": "Key Workflow and Graphs",
    "text": "Key Workflow and Graphs\n\nDescriptive Statisitcs\n\nMeans, quartiles, etc. for each feature\n\nHistogram of any intresting features\nShape of Data\nMissing Data\nFind relationship (Correlation)"
  },
  {
    "objectID": "slides/lec-09.html#class-activity-1",
    "href": "slides/lec-09.html#class-activity-1",
    "title": "CISC482 - Lecture09",
    "section": "Class Activity",
    "text": "Class Activity\nWork on your Topic Idea!\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-15.html#schedule",
    "href": "slides/lec-15.html#schedule",
    "title": "CISC482 - Lecture15",
    "section": "Schedule",
    "text": "Schedule\n\nReading 6-2: Mar 10 @ 12PM, Friday\nReading 6-3: Mar 22 @ 12PM, Wednesday\nProposal: Mar 22, Wednesday\nPropsal Template!\nHW5 - Mar 29 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-15.html#today",
    "href": "slides/lec-15.html#today",
    "title": "CISC482 - Lecture15",
    "section": "Today",
    "text": "Today\n\nReview Overfit/Underfit\nReivew Regression and Classification Metrics\nTraining vs Testing Set"
  },
  {
    "objectID": "slides/lec-15.html#overfitunderfit",
    "href": "slides/lec-15.html#overfitunderfit",
    "title": "CISC482 - Lecture15",
    "section": "Overfit/Underfit",
    "text": "Overfit/Underfit\n\nOverfit - model is too complex to fit the data well.\n\nFitting the data too closely\nIncorporating too much noise (meaningless variation)\nMisses the general trend\n\nUnderfit - model is too simple to fit the data well.\n\nLarge systematic errror"
  },
  {
    "objectID": "slides/lec-15.html#question---underfit-or-overfit",
    "href": "slides/lec-15.html#question---underfit-or-overfit",
    "title": "CISC482 - Lecture15",
    "section": "Question - Underfit or Overfit?",
    "text": "Question - Underfit or Overfit?"
  },
  {
    "objectID": "slides/lec-15.html#regression-metrics",
    "href": "slides/lec-15.html#regression-metrics",
    "title": "CISC482 - Lecture15",
    "section": "Regression Metrics",
    "text": "Regression Metrics\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = \\frac{\\text{variation explained by regression}}{\\text{total variation in the data}} = \\frac{\\sum (\\hat{y}_i - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2}\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\nWhat is the range? Units?"
  },
  {
    "objectID": "slides/lec-15.html#binary-classification-metrics",
    "href": "slides/lec-15.html#binary-classification-metrics",
    "title": "CISC482 - Lecture15",
    "section": "Binary Classification Metrics",
    "text": "Binary Classification Metrics\n\nTrue Positive (TP) is an outcome that was correctly identified as positive\nTrue Negative (TN) is an outcome that was correctly identified as negative.\nFalse Positive (FP) is an outcome that was incorrectly identified as positive\nFalse Negative (TN) is an outcome that was incorrectly identified as negative"
  },
  {
    "objectID": "slides/lec-15.html#metrics",
    "href": "slides/lec-15.html#metrics",
    "title": "CISC482 - Lecture15",
    "section": "Metrics",
    "text": "Metrics\n\nAccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nPrecision: \\(\\frac{TP}{TP + FP}\\)\nRecall: \\(\\frac{TP}{TP + FN}\\)"
  },
  {
    "objectID": "slides/lec-15.html#purpose-of-model-evaluation",
    "href": "slides/lec-15.html#purpose-of-model-evaluation",
    "title": "CISC482 - Lecture15",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\), \\(recall\\), etc. tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/lec-15.html#splitting-data",
    "href": "slides/lec-15.html#splitting-data",
    "title": "CISC482 - Lecture15",
    "section": "Splitting data",
    "text": "Splitting data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/lec-15.html#the-split",
    "href": "slides/lec-15.html#the-split",
    "title": "CISC482 - Lecture15",
    "section": "The Split",
    "text": "The Split\n\nTraining data is used to fit a model.\nValidation data is used to evaluate model performance while adjusting hyperparameter estimates and conducting feature selection. We also use this choose between two models. This is not always needed!\nTest data is used to evaluate final model performance and compare different models.\nThe ratio for this split: 80/10/10 or 70/10/20"
  },
  {
    "objectID": "slides/lec-15.html#visualization",
    "href": "slides/lec-15.html#visualization",
    "title": "CISC482 - Lecture15",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "slides/lec-15.html#an-example",
    "href": "slides/lec-15.html#an-example",
    "title": "CISC482 - Lecture15",
    "section": "An Example!",
    "text": "An Example!\n\n\nCode\nfrom sklearn.metrics import r2_score\nlinear_model = LinearRegression()\nlinear_model.fit(X, y)\nprint(f\"Linear Model R^2 = {r2_score(y, linear_model.predict(X)):.2f}\")\n\ndegree = 5\nquadratic_model = np.poly1d(np.polyfit(x, y, degree)) # quadratic\nprint(f\"Polynomial Model R^2 = {r2_score(y, quadratic_model(x)):.2f}\")\n\nx_graph = np.linspace(0, 10, 100)\nax = sns.scatterplot(x=x, y=y, label=\"All Data\")\nax.plot(x_graph, linear_model.predict(x_graph[:, np.newaxis]), color='r', label='Linear Regression')\nax.plot(x_graph, quadratic_model(x_graph), color='m', label='Polynomial Regression')\nax.legend();"
  },
  {
    "objectID": "slides/lec-15.html#an-example-output",
    "href": "slides/lec-15.html#an-example-output",
    "title": "CISC482 - Lecture15",
    "section": "An Example!",
    "text": "An Example!\n\nLinear Model R^2 = 0.55\nPolynomial Model R^2 = 0.64"
  },
  {
    "objectID": "slides/lec-15.html#split-the-data",
    "href": "slides/lec-15.html#split-the-data",
    "title": "CISC482 - Lecture15",
    "section": "Split the Data!",
    "text": "Split the Data!\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=0)\n\nfig, ax = plt.subplots(nrows=1, ncols=2) # create 2 plots!\n# Plot training result\nsns.scatterplot(x=X_train[:,0], y=y_train, ax=ax[0], label=\"Training Data\")\nax[0].set_title(\"Training Set\")\nax[0].legend()\n# Plot testing result\nsns.scatterplot(x=X_test[:,0], y=y_test, ax=ax[1], label=\"Testing Data\")\nax[1].set_title(\"Testing Set\")\nax[1].legend();"
  },
  {
    "objectID": "slides/lec-15.html#traintest-split",
    "href": "slides/lec-15.html#traintest-split",
    "title": "CISC482 - Lecture15",
    "section": "Train/Test Split",
    "text": "Train/Test Split\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=0)\n\nlinear_model.fit(X_train, y_train)\nquadratic_model = np.poly1d(np.polyfit(X_train[:,0], y_train, degree)) # quadratic\n\nprint(f\"TRAIN SET - Linear Model R^2 = {r2_score(y_train, linear_model.predict(X_train)):.2f}; Polynomial Model R^2 = {r2_score(y_train, quadratic_model(X_train[:,0])):.2f}\")\nprint(f\"TEST SET - Linear Model R^2 = {r2_score(y_test, linear_model.predict(X_test)):.2f}; Polynomial Model R^2 = {r2_score(y_test, quadratic_model(X_test[:,0])):.2f}\")\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2) # create 2 plots!\n# Plot training result\nx_graph = np.linspace(0, 10, 100)\nsns.scatterplot(x=X_train[:,0], y=y_train, ax=ax[0], label=\"Training Data\")\nax[0].plot(x_graph, linear_model.predict(x_graph[:, np.newaxis]), color='r', label='Linear Regression')\nax[0].plot(x_graph, quadratic_model(x_graph), color='m', label='Polynomial Regression');\n# ax[0].scatter(np.sort(X_train[:, 0]), quadratic_model(np.sort(X_train[:, 0])), color='m');\nax[0].set_title(\"Training Set\")\nax[0].legend()\n# Plot testing result\nsns.scatterplot(x=X_test[:,0], y=y_test, ax=ax[1], label=\"Testing Data\")\nax[1].plot(x_graph, linear_model.predict(x_graph[:, np.newaxis]), color='r', label='Linear Regression')\nax[1].plot(x_graph, quadratic_model(x_graph), color='m', label='Polynomial Regression');\n# ax[1].scatter(np.sort(X_test[:, 0]), quadratic_model(np.sort(X_test[:, 0])), color='m');\nax[1].set_title(\"Testing Set\")\nax[1].legend();\n\n\nTRAIN SET - Linear Model R^2 = 0.44; Polynomial Model R^2 = 0.62\nTEST SET - Linear Model R^2 = 0.65; Polynomial Model R^2 = 0.41"
  },
  {
    "objectID": "slides/lec-15.html#cross-validation-1",
    "href": "slides/lec-15.html#cross-validation-1",
    "title": "CISC482 - Lecture15",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nIt seems a little strange to now judge our models just on the random selection of observations of validation set.\nIts like the choice of our model is biased towards this random selection of validation set.\nSolution: Do this process (fitting a model, validating model) multiple times using different subsets of data -> Cross Validation"
  },
  {
    "objectID": "slides/lec-15.html#k-folds",
    "href": "slides/lec-15.html#k-folds",
    "title": "CISC482 - Lecture15",
    "section": "K-Folds",
    "text": "K-Folds\n\nk-fold cross-validation is a popular method of evaluating model performance\nStep 1: Choose k, usually 10\nStep 2: ShuffleDivide our data, X, into X_fold and X_test.\nStep 3: Divide X_fold into k groups (folds)\nStep 4: Model is trained and validated repeatedly using these groups"
  },
  {
    "objectID": "slides/lec-15.html#k-folds-split-k10",
    "href": "slides/lec-15.html#k-folds-split-k10",
    "title": "CISC482 - Lecture15",
    "section": "K-Folds Split (k=10)",
    "text": "K-Folds Split (k=10)"
  },
  {
    "objectID": "slides/lec-15.html#k-folds-train-and-validate",
    "href": "slides/lec-15.html#k-folds-train-and-validate",
    "title": "CISC482 - Lecture15",
    "section": "K-Folds Train and Validate",
    "text": "K-Folds Train and Validate"
  },
  {
    "objectID": "slides/lec-15.html#choosing-k",
    "href": "slides/lec-15.html#choosing-k",
    "title": "CISC482 - Lecture15",
    "section": "Choosing k",
    "text": "Choosing k\n\nthe larger the k, the more models need to be trained\nthe larger the k, larger training folds\nthis vastly increases computational requirements.\nFor large models, you need to have a small \\(k\\). e.g. chat gpt!\nMost common for small/medium models is k=10"
  },
  {
    "objectID": "slides/lec-15.html#class-activity-1",
    "href": "slides/lec-15.html#class-activity-1",
    "title": "CISC482 - Lecture15",
    "section": "Class Activity",
    "text": "Class Activity\nModel Evaluation\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-12.html#schedule",
    "href": "slides/lec-12.html#schedule",
    "title": "CISC482 - Lecture12",
    "section": "Schedule",
    "text": "Schedule\n\nReading 6-1: Mar 08 @ 12PM, Wednesday\nTopic Ideas - Should be turned in! Will grade soon!\nHW4 - Mar 08 @ Midnight\nProposal: Mar 22, Wednesday"
  },
  {
    "objectID": "slides/lec-12.html#today",
    "href": "slides/lec-12.html#today",
    "title": "CISC482 - Lecture12",
    "section": "Today",
    "text": "Today\n\nReview Linear Regression\nLogistic Regression"
  },
  {
    "objectID": "slides/lec-12.html#what-we-learned",
    "href": "slides/lec-12.html#what-we-learned",
    "title": "CISC482 - Lecture12",
    "section": "What we learned",
    "text": "What we learned\n\n\nSimple Linear Regression\n\n\\(\\hat{y} = \\beta_0 + x + \\beta_1\\)\n\nSimple Polynomial Linear Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x + ... + \\beta_k x^k\\)\n\nMultiple Linear Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\\)\n\nMultiple (Variable) Polynomial Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1 x_2 + \\beta_4 x_2 + \\beta_5 x_2^2\\)"
  },
  {
    "objectID": "slides/lec-12.html#regression-diagram",
    "href": "slides/lec-12.html#regression-diagram",
    "title": "CISC482 - Lecture12",
    "section": "Regression Diagram",
    "text": "Regression Diagram\n\n\n\n\nflowchart LR\n  A[Features] --> B{Single Feature}\n  B -- Yes --> C{Curvature}\n  C -- No --> E[Simple Regression]\n  C -- Yes --> D[Simple Polynomial <br> Regression]\n  B -- No --> F{Non-linear Correlation <br> Between Features}\n  F -- No --> G[Multiple Linear <br> Regression]\n  F -- Yes --> H[Polynomial Regression]"
  },
  {
    "objectID": "slides/lec-12.html#class-activity-1",
    "href": "slides/lec-12.html#class-activity-1",
    "title": "CISC482 - Lecture12",
    "section": "Class Activity",
    "text": "Class Activity\nDemo!"
  },
  {
    "objectID": "slides/lec-12.html#classification",
    "href": "slides/lec-12.html#classification",
    "title": "CISC482 - Lecture12",
    "section": "Classification",
    "text": "Classification\n\nWe are now going to be focusing on problems where we are interested in predicting the group or the class of something\nGive \\(X = {x_1, x_2, ..., x_k}\\) what is the class of this object.\n\nGiven the bill_length is 10 and bill_depth is 20 what is the species of this penguin?\nA person arrives at the emergency room with a set of symptoms. Which of the three medical conditions does the individual have?\n\nLets first focus where there are only 2 classes. Lets just call them (+) positive and (-) negative."
  },
  {
    "objectID": "slides/lec-12.html#example-classification",
    "href": "slides/lec-12.html#example-classification",
    "title": "CISC482 - Lecture12",
    "section": "Example Classification",
    "text": "Example Classification"
  },
  {
    "objectID": "slides/lec-12.html#linear-regression-for-classification",
    "href": "slides/lec-12.html#linear-regression-for-classification",
    "title": "CISC482 - Lecture12",
    "section": "Linear Regression for Classification",
    "text": "Linear Regression for Classification\n\nLinear Regression is generally not suitable for classification purposes\nHowever, you can use it to draw a line that whose value predicts the class\nE.g. if \\(\\hat{y} > 0.5 -> Positive\\)"
  },
  {
    "objectID": "slides/lec-12.html#lr-classification-okay",
    "href": "slides/lec-12.html#lr-classification-okay",
    "title": "CISC482 - Lecture12",
    "section": "LR Classification (Okay)",
    "text": "LR Classification (Okay)"
  },
  {
    "objectID": "slides/lec-12.html#lr-classification-bad",
    "href": "slides/lec-12.html#lr-classification-bad",
    "title": "CISC482 - Lecture12",
    "section": "LR Classification (BAD)",
    "text": "LR Classification (BAD)"
  },
  {
    "objectID": "slides/lec-12.html#lr-classification-really-bad",
    "href": "slides/lec-12.html#lr-classification-really-bad",
    "title": "CISC482 - Lecture12",
    "section": "LR Classification (Really BAD)",
    "text": "LR Classification (Really BAD)"
  },
  {
    "objectID": "slides/lec-12.html#logistic-regression",
    "href": "slides/lec-12.html#logistic-regression",
    "title": "CISC482 - Lecture12",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\n\nUse a specific exponential function\n\\(\\hat{y} = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\)\nWhats the range of this function?\n\\(\\hat{p} = \\sigma(z)\\), where\n\n\\(z = \\beta_0 + \\beta_1 x\\)\n\\(\\sigma(z) = \\frac{e^{z}}{1 + e^{z}}\\)\n\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)"
  },
  {
    "objectID": "slides/lec-12.html#example",
    "href": "slides/lec-12.html#example",
    "title": "CISC482 - Lecture12",
    "section": "Example",
    "text": "Example\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\nlinear_model = LinearRegression()\nlogistic_model = LogisticRegression(C=1e5)\n\nlinear_model.fit(X, y)\nlogistic_model.fit(X, y)\n\nprint(f\"LINEAR MODEL: {linear_model.coef_[0]:.1f} * x + {linear_model.intercept_:.1f}\")\nprint(f\"LOGISTIC MODEL: sigmoid({logistic_model.coef_[0][0]:.1f} * x + {logistic_model.intercept_[0]:.1f})\");\n\nLINEAR MODEL: 0.1 * x + 0.4\nLOGISTIC MODEL: sigmoid(6.9 * x + -1.6)"
  },
  {
    "objectID": "slides/lec-12.html#visual",
    "href": "slides/lec-12.html#visual",
    "title": "CISC482 - Lecture12",
    "section": "Visual",
    "text": "Visual\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nWhat do you notice?"
  },
  {
    "objectID": "slides/lec-12.html#multivariable-classification",
    "href": "slides/lec-12.html#multivariable-classification",
    "title": "CISC482 - Lecture12",
    "section": "Multivariable Classification",
    "text": "Multivariable Classification"
  },
  {
    "objectID": "slides/lec-12.html#in-3d",
    "href": "slides/lec-12.html#in-3d",
    "title": "CISC482 - Lecture12",
    "section": "In 3D",
    "text": "In 3D"
  },
  {
    "objectID": "slides/lec-12.html#multivariable-logistic-regression",
    "href": "slides/lec-12.html#multivariable-logistic-regression",
    "title": "CISC482 - Lecture12",
    "section": "Multivariable Logistic Regression",
    "text": "Multivariable Logistic Regression\n\n\nCode\nlogistic_model = LogisticRegression(C=1e5)\nlogistic_model.fit(X, y)\n\nx_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)\ny_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)\nXX, YY = np.meshgrid(x_range, y_range)\nP = expit(XX* logistic_model.coef_[0][0] + YY * logistic_model.coef_[0][0] + logistic_model.intercept_)\n\n\nwith sns.plotting_context(\"talk\"):\n  fig = plt.figure(figsize = (10, 7))\n  ax = plt.axes(projection =\"3d\")\n  # ZZ = np.ones_like(XX) * 0.5 \n  # ax.plot_surface(XX, YY, ZZ,  **kwargs, zorder=1)\n  ax.scatter(X[:,0], X[:,1], y, c=y, cmap='jet', zorder=10, edgecolor='k')\n  ax.plot_surface(XX, YY, P, alpha=1.0, cmap='viridis', edgecolor='k');"
  },
  {
    "objectID": "slides/lec-12.html#multivariable-logistic-regression-1",
    "href": "slides/lec-12.html#multivariable-logistic-regression-1",
    "title": "CISC482 - Lecture12",
    "section": "Multivariable Logistic Regression",
    "text": "Multivariable Logistic Regression\n\n\nCode\nwith sns.plotting_context(\"talk\"):\n  fig = plt.figure(figsize = (10, 7))\n  ax = plt.axes(projection =\"3d\")\n  ZZ = np.ones_like(XX) * 0.5 \n  ax.plot_surface(XX, YY, ZZ,  zorder=1, edgecolor='k')\n  ax.plot_surface(XX, YY, P, alpha=1.0, cmap='viridis', edgecolor='k');"
  },
  {
    "objectID": "slides/lec-12.html#view-decision-boundary",
    "href": "slides/lec-12.html#view-decision-boundary",
    "title": "CISC482 - Lecture12",
    "section": "View Decision Boundary",
    "text": "View Decision Boundary\n\n\nCode\nfrom sklearn.inspection import DecisionBoundaryDisplay\ndisp = DecisionBoundaryDisplay.from_estimator(\n    logistic_model, X, response_method=\"predict\", cmap=plt.cm.viridis, alpha=0.5\n)\ndisp.ax_.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='k');\ndisp.ax_.set_xlabel(\"X\")\ndisp.ax_.set_ylabel(\"Y\")\n\n\nText(0, 0.5, 'Y')"
  },
  {
    "objectID": "slides/lec-12.html#loss-function",
    "href": "slides/lec-12.html#loss-function",
    "title": "CISC482 - Lecture12",
    "section": "Loss Function",
    "text": "Loss Function\n\n\nLets create a likelihood function \\(L\\) that is a function of our parameters \\(\\beta\\). We want to find parameters \\(\\beta\\) that make \\(L\\) as big as possible.\n\\(L(\\beta) = \\sum_{i=1}^n \\underbrace{y_i \\cdot \\sigma(\\beta \\; x_i)}_{y = 1} + \\underbrace{(1-y_i) \\cdot (1 - \\sigma(\\beta \\; x_i))}_{y = 0}\\)\nConvert to a loss function \\(J\\). We also take the logarithm. This ensures our cost function is convex.\n\\(J(\\beta) = -\\sum_{i=1}^n y_i \\cdot log(\\sigma(\\beta \\; x_i)) + (1-y_i) \\cdot log(\\sigma(\\beta \\; x_i))\\)\nThis does not have a closed form solution. AKA you can not just take derivative and solve the parameters \\(\\beta\\).\nWe have to use a numerical solver. For example, gradient descent."
  },
  {
    "objectID": "slides/lec-12.html#convex-vs-non-convex",
    "href": "slides/lec-12.html#convex-vs-non-convex",
    "title": "CISC482 - Lecture12",
    "section": "Convex vs Non Convex",
    "text": "Convex vs Non Convex"
  },
  {
    "objectID": "slides/lec-12.html#convex-function-soccer-ball",
    "href": "slides/lec-12.html#convex-function-soccer-ball",
    "title": "CISC482 - Lecture12",
    "section": "Convex Function, Soccer Ball",
    "text": "Convex Function, Soccer Ball"
  },
  {
    "objectID": "slides/lec-12.html#gradient-descent",
    "href": "slides/lec-12.html#gradient-descent",
    "title": "CISC482 - Lecture12",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-05.html#schedule",
    "href": "slides/lec-05.html#schedule",
    "title": "CISC482 - Lecture05",
    "section": "Schedule",
    "text": "Schedule\n\nHW2: Feb 1 @ Midnight, Wednesday\nReading 3-1: Feb 3 @ 12PM, Friday\nHW3: Feb 15 @ Midnight, Wednesday\nExam 1: Feb 15 in Class"
  },
  {
    "objectID": "slides/lec-05.html#cs-faculty-candidate",
    "href": "slides/lec-05.html#cs-faculty-candidate",
    "title": "CISC482 - Lecture05",
    "section": "CS Faculty Candidate",
    "text": "CS Faculty Candidate\n\nBenz Tran is here Friday!\nPlease attend a meet and greet at 3:15 in SBSC 112\nExtra Credit!"
  },
  {
    "objectID": "slides/lec-05.html#master-degree",
    "href": "slides/lec-05.html#master-degree",
    "title": "CISC482 - Lecture05",
    "section": "Master Degree",
    "text": "Master Degree\n\nNortheastern University’s Roux Institute\nTuesday, February 21, 2023, noon to 2 p.m., in the Campus Union\nData Science, Computer Science, Project Management, etc.\n$25,000 scholarships for students who begin their Northeastern University master’s program at The Roux Institute in Fall 2023"
  },
  {
    "objectID": "slides/lec-05.html#main-ideas",
    "href": "slides/lec-05.html#main-ideas",
    "title": "CISC482 - Lecture05",
    "section": "Main Ideas",
    "text": "Main Ideas\n\nInferential statistics are methods that result in conclusions and estimates about the population based on a sample\nSeek to estimate the parameters that describe a populations distribution\n\nmean, std, etc.\n\nRemember, we collect data from a population, this creates a sample\nUsing that sample, we make inferences about the underlying population\nThe sample proportion of voters who plan to vote by mail is a statistic and estimates the parameter, or the population proportion, of all voters who plan to vote by mail.\n\n\n\n\n\n\n\nWarning\n\n\nWe will not go too deeply into this subject. We will only focus on the most important points."
  },
  {
    "objectID": "slides/lec-05.html#sampling",
    "href": "slides/lec-05.html#sampling",
    "title": "CISC482 - Lecture05",
    "section": "Sampling",
    "text": "Sampling\n\nBecause calculated statistics will vary from sample to sample due to natural sampling variability, statistics do not estimate parameters with 100% accuracy.\nThe overall behavior of a statistic from repeated sampling is described by a sampling distribution.\nThe sampling distribution of a statistic describes the statistic’s possible values and a measure of how likely the values are to occur."
  },
  {
    "objectID": "slides/lec-05.html#sampling-video",
    "href": "slides/lec-05.html#sampling-video",
    "title": "CISC482 - Lecture05",
    "section": "Sampling Video",
    "text": "Sampling Video\n\n\nMean of many means"
  },
  {
    "objectID": "slides/lec-05.html#central-limit-theorem",
    "href": "slides/lec-05.html#central-limit-theorem",
    "title": "CISC482 - Lecture05",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\n\n\n\nTip\n\n\nThe Central Limit Theorem (CLT) states that if random samples of size are drawn from a large population and is large enough, then the sampling distribution of the sample mean will follow approximately a normal distribution."
  },
  {
    "objectID": "slides/lec-05.html#hypothesis-test",
    "href": "slides/lec-05.html#hypothesis-test",
    "title": "CISC482 - Lecture05",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\nHypothesis Test - method for examining a claim, or hypothesis, about a population parameter.\nnull hypothesis, \\(H_0\\), by-chance, no-effect explanation\n\nmedicine has no effect\n\nalternative hypothesis, \\(H_a\\), explanation of an effect\n\nmedicine has a (positive) effect\n\np-value - or likelihood, of obtaining a statistic at least as extreme when \\(H_0\\) is true.\n\n\n\n\n\n\n\nDanger\n\n\nStatistics is widely and notoriously misunderstood. Even by experts. Including me."
  },
  {
    "objectID": "slides/lec-05.html#visualization",
    "href": "slides/lec-05.html#visualization",
    "title": "CISC482 - Lecture05",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "slides/lec-05.html#questions",
    "href": "slides/lec-05.html#questions",
    "title": "CISC482 - Lecture05",
    "section": "Questions",
    "text": "Questions\n\nA graph of the sampling distribution of the sample proportion of correct detections out of 37 trials when the null hypothesis is true is given below. Based on this graph, observing a sample proportion of 0.78 or greater is\nCan the dog detect cancer?\n\n\n\n\n\n\n\n\nDanger\n\n\nThis does not mean the dog detects cancer! This just means the dog is doing better than guessing! Maybe cancer patients eat PB sandwiches and thats what the dog is smelling!"
  },
  {
    "objectID": "slides/lec-05.html#type-1-and-type-2-errors",
    "href": "slides/lec-05.html#type-1-and-type-2-errors",
    "title": "CISC482 - Lecture05",
    "section": "Type 1 and Type 2 Errors",
    "text": "Type 1 and Type 2 Errors\n\nType 1 Error - is rejecting the null hypothesis in favor of the alternative when in reality the null hypothesis is true.\n\nFalse Positive\n\nType 2 Error - is failing to reject (accepting) the null hypothesis when in reality the alternative hypothesis is true.\n\nFalse Negative\n\n\n\n\n\n\n\n\nTip\n\n\nBinary Outcomes, True or False, 1 or 0\nFalse-Positive = Incorrectly Predict a Positive Outcome\n\\[\n\\underbrace{\\textbf{False}}_{\\textbf{Correct or Incorrect}}-\\underbrace{\\textbf{Positive}}_{\\textbf{Your Prediction}}\n\\]"
  },
  {
    "objectID": "slides/lec-05.html#table",
    "href": "slides/lec-05.html#table",
    "title": "CISC482 - Lecture05",
    "section": "Table",
    "text": "Table\n\n\n\n\nActual Positive\nActual Negative\n\n\n\n\nPredict Positive\nTP\nFP, Type I\n\n\nPredict Negative\nFN, Type II\nTN"
  },
  {
    "objectID": "slides/lec-05.html#confidence-intervals",
    "href": "slides/lec-05.html#confidence-intervals",
    "title": "CISC482 - Lecture05",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSometimes you want to estimate the value of a population parameter (e.g., the mean)\nConfidence Interval provides an interval of possible values for the parameter being estimated\n\nestimate +- margin of error\n\nMargin of error includes\n\nthe standard error, or measure of sampling variability\nConfidence level, or measure of interval reliability. Usually set at 95%."
  },
  {
    "objectID": "slides/lec-05.html#example",
    "href": "slides/lec-05.html#example",
    "title": "CISC482 - Lecture05",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lec-05.html#standard-deviation-of-sampling-distribution",
    "href": "slides/lec-05.html#standard-deviation-of-sampling-distribution",
    "title": "CISC482 - Lecture05",
    "section": "Standard Deviation of Sampling Distribution",
    "text": "Standard Deviation of Sampling Distribution\n\nWe don’t usually have the standard deviation of sampling distribution.\nEstimate with the Standard Error\n\\(SD_{\\hat{p}} = SD_{\\bar{x}} = \\sigma_{\\bar{x}} = SEM\\)\n\\(SD_{\\bar{p}} = \\frac{\\sigma_x}{\\sqrt{N}}\\)\n\n\\(\\sigma_x\\) - This is the standard deviation of your samples\n\\(N\\) - How many samples you took\n\n\n\n\n\n\n\n\nTip\n\n\nUse standard error when estimating means."
  },
  {
    "objectID": "slides/lec-05.html#confirming-reported-statistic",
    "href": "slides/lec-05.html#confirming-reported-statistic",
    "title": "CISC482 - Lecture05",
    "section": "Confirming reported statistic",
    "text": "Confirming reported statistic\n\n\nGiven a statistic about a population, \\(\\hat{\\pi}\\). Mean radon levels of houses in Iowa.\nYou gather samples that are IID. Independent and identically distributed\nCompute the mean and standard deviation from your sample: \\(\\hat{p}, \\hat{\\sigma}\\)\n\\(H_0\\): The true population mean is \\(\\hat{\\pi}\\). \\(\\pi = \\hat{\\pi}\\)\n\\(H_a\\): The true population mean is not \\(\\hat{\\pi}\\). \\(\\pi \\neq \\hat{\\pi}\\)"
  },
  {
    "objectID": "slides/lec-05.html#procedure",
    "href": "slides/lec-05.html#procedure",
    "title": "CISC482 - Lecture05",
    "section": "Procedure",
    "text": "Procedure"
  },
  {
    "objectID": "slides/lec-05.html#z-table",
    "href": "slides/lec-05.html#z-table",
    "title": "CISC482 - Lecture05",
    "section": "Z-table",
    "text": "Z-table\nZ Table\n\n\n\n\n\n\n\nDanger\n\n\nWhat if we didn’t gather that many samples, e.g., \\(< 30\\)? Then our sampling distribution follows a t-distribution."
  },
  {
    "objectID": "slides/lec-05.html#t-vs-normal-distribution",
    "href": "slides/lec-05.html#t-vs-normal-distribution",
    "title": "CISC482 - Lecture05",
    "section": "T vs Normal Distribution",
    "text": "T vs Normal Distribution"
  },
  {
    "objectID": "slides/lec-05.html#t-table",
    "href": "slides/lec-05.html#t-table",
    "title": "CISC482 - Lecture05",
    "section": "T-table",
    "text": "T-table\nLink"
  },
  {
    "objectID": "slides/lec-05.html#highlights",
    "href": "slides/lec-05.html#highlights",
    "title": "CISC482 - Lecture05",
    "section": "Highlights",
    "text": "Highlights\n\nInferential statistics are methods that result in conclusions and estimates about the population based on a sample\nHypothesis Testing - create a null hypothesis, gather data, compute sample statistics, compute z-score and generate p-value.\nConfidence Interval - sample data, compute sample statistics, use confidence multiplier (z-value) and standard deviation of samples to compute margin of error"
  },
  {
    "objectID": "slides/lec-05.html#whats-next",
    "href": "slides/lec-05.html#whats-next",
    "title": "CISC482 - Lecture05",
    "section": "Whats next?",
    "text": "Whats next?\n\nStart playing more with data!\nWill be using the software pandas to help analyze the data\nCleaning data\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-02.html#subscibre-to-zybook",
    "href": "slides/lec-02.html#subscibre-to-zybook",
    "title": "CISC482 - Lecture02",
    "section": "Subscibre to Zybook",
    "text": "Subscibre to Zybook\n\n\nBrightspace integration seems to be working now"
  },
  {
    "objectID": "slides/lec-02.html#due-dates",
    "href": "slides/lec-02.html#due-dates",
    "title": "CISC482 - Lecture02",
    "section": "Due Dates",
    "text": "Due Dates\n\nReading 1-1: Jan 22, Sunday\nHW1: Jan 25 @ Midnight, Wed\nReading 2-1: Jan 25 @ 12PM, Wed\nReading 2-2: Jan 27 @ 12PM, Friday\nHW2: Feb 1 @ Midnight, Wed"
  },
  {
    "objectID": "slides/lec-02.html#data-science",
    "href": "slides/lec-02.html#data-science",
    "title": "CISC482 - Lecture02",
    "section": "Data Science",
    "text": "Data Science\n\n\nData science is an interdisciplinary field focused on discovering patterns and describing relationships using data.\n\n\nData science uses techniques from computer science and statistics.\nData scientists use computers to write code and store, modify, and visualize large datasets.\nData scientists also build, test, and interpret models that describe real-life situations, then use models to make predictions for new data."
  },
  {
    "objectID": "slides/lec-02.html#ds-early-history",
    "href": "slides/lec-02.html#ds-early-history",
    "title": "CISC482 - Lecture02",
    "section": "DS early history",
    "text": "DS early history\n\n\nPeter Naur won the Turing Award\nAstronomer -> Computer Science -> Professor\nInventor of Algol Programming Language (PL)"
  },
  {
    "objectID": "slides/lec-02.html#ds-20th-century",
    "href": "slides/lec-02.html#ds-20th-century",
    "title": "CISC482 - Lecture02",
    "section": "DS 20th Century",
    "text": "DS 20th Century"
  },
  {
    "objectID": "slides/lec-02.html#statistics-and-computer-science",
    "href": "slides/lec-02.html#statistics-and-computer-science",
    "title": "CISC482 - Lecture02",
    "section": "Statistics and Computer Science",
    "text": "Statistics and Computer Science"
  },
  {
    "objectID": "slides/lec-02.html#focus-of-task",
    "href": "slides/lec-02.html#focus-of-task",
    "title": "CISC482 - Lecture02",
    "section": "Focus of Task",
    "text": "Focus of Task"
  },
  {
    "objectID": "slides/lec-02.html#statistics-and-machine-learning",
    "href": "slides/lec-02.html#statistics-and-machine-learning",
    "title": "CISC482 - Lecture02",
    "section": "Statistics and Machine Learning",
    "text": "Statistics and Machine Learning"
  },
  {
    "objectID": "slides/lec-02.html#where-are-we",
    "href": "slides/lec-02.html#where-are-we",
    "title": "CISC482 - Lecture02",
    "section": "Where are we?",
    "text": "Where are we?\n\n\nEqual Exploratory and Predictive\nLittle bit of statistics"
  },
  {
    "objectID": "slides/lec-02.html#learning-goals",
    "href": "slides/lec-02.html#learning-goals",
    "title": "CISC482 - Lecture02",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nIdentify features and instances in a dataset\nThree V’s- volume, velocity, and variety"
  },
  {
    "objectID": "slides/lec-02.html#data-sets",
    "href": "slides/lec-02.html#data-sets",
    "title": "CISC482 - Lecture02",
    "section": "Data sets",
    "text": "Data sets\n\nA dataset is a collection of information. Consists of features and instances\nA feature, or variable, is a characteristic that can be measured or observed on an observational unit.\nAn instance, or observational units, is a tuple of features. Often called data points or observations."
  },
  {
    "objectID": "slides/lec-02.html#example-data-set",
    "href": "slides/lec-02.html#example-data-set",
    "title": "CISC482 - Lecture02",
    "section": "Example Data Set",
    "text": "Example Data Set\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n  \n\n\n\n\n\nROW is an instance.\nCOLUMN represents a different feature.\n\n\n\nResearchers at the Palmer Archipelago in the Antarctic collected data on three local penguin species: Adelie, Chinstrap, and Gentoo."
  },
  {
    "objectID": "slides/lec-02.html#volume-variety-velocity",
    "href": "slides/lec-02.html#volume-variety-velocity",
    "title": "CISC482 - Lecture02",
    "section": "Volume, Variety, Velocity",
    "text": "Volume, Variety, Velocity\n\nVolume - The amount of data being collected digitally is exponentially increasing.\nBillions of people are being digitally indexed and catalogued\nEstimate that 2.5 quintillion bytes of data is created each day\nWe now have the processing power where large data sets can now be processed\n\n\n\nBig data is really, really big. In 1986, the total estimated data in the world was 2.6 exabytes (EB).\nOne exabyte (EB) equals bytes, or 1 million TB. Most laptops come with 1 TB storage at most.\nBy 1993, the total estimated data in the world had grown to 15.8 EB.\nIn 2000, the total estimated data in the world had reached 54.5 EB.\nIn 2007, the total estimated data in the world was 295 EB.\nBy 2020, the total estimated data had increased to 6800 EB, or 6.8 PB - the equivalent of over 7 trillion laptop computers."
  },
  {
    "objectID": "slides/lec-02.html#volume-variety-velocity-1",
    "href": "slides/lec-02.html#volume-variety-velocity-1",
    "title": "CISC482 - Lecture02",
    "section": "Volume, Variety, Velocity",
    "text": "Volume, Variety, Velocity\n\nVariety - Data from a variety of difference sources are being collected and combined\nIts not just your name and your playlist.\n\nIts how long you listened\nHow long it took you swipe next\nYour search history on the subject\nYour bloody e-mails\nYour phones resolution, battery information, browser, time zone"
  },
  {
    "objectID": "slides/lec-02.html#volume-variety-velocity-2",
    "href": "slides/lec-02.html#volume-variety-velocity-2",
    "title": "CISC482 - Lecture02",
    "section": "Volume, Variety, Velocity",
    "text": "Volume, Variety, Velocity\n\nVelocity - Data is being created and ingested at a faster time\nThe search you just made was fed into an algorithm to give you an add within 10 seconds\nSensor Data - Your recorded heart rate was just noted and used to indicate the statistical likelihood of sending you an add now or later\nWith more up to date information, data scientist can make better predictions"
  },
  {
    "objectID": "slides/lec-02.html#reproducibility",
    "href": "slides/lec-02.html#reproducibility",
    "title": "CISC482 - Lecture02",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nTwo data scientists are building models to classify brain tumors as benign or malignant.\nOne data scientist uses a programming language, such as Python, to write code to fit the model.\nAnother data scientist uses software instead of coding to fit the model.\nNew brain scans arrive. The data scientist who used coding re-runs the analysis -> a few minutes.\nSince the software uses a point and click interface, re-analyzing could take several hours."
  },
  {
    "objectID": "slides/lec-02.html#job-titles",
    "href": "slides/lec-02.html#job-titles",
    "title": "CISC482 - Lecture02",
    "section": "Job Titles",
    "text": "Job Titles\n\n\n\n\n\n\n\n\nTitle\nDescription\n\n\n\n\nData engineers\nData engineers specialize in data gathering and storage for production. Data engineers extract, transform, and load datasets for later analysis.\n\n\nData scientists\nData scientists gather data, transform data, and use models and algorithms to extract meaningful insights from datasets. Development\n\n\nData analysts\nData analysts work with industry experts to analyze datasets and create visualizations. Data analysts use some data science models, but tend to use data visualization and summary more than modeling.\n\n\nBusiness intelligence analysts\nBusiness intelligence analysts specialize in data related to financial and market transactions. Data analysts and business intelligence analysts are similar roles, but the term business intelligence is more common in business and finance.\n\n\nMachine learning engineers\nMachine learning engineers specialize in machine learning models instead of statistical models. Machine learning engineers often focus on the implementation and development of a model rather than selection and interpretation. Production"
  },
  {
    "objectID": "slides/lec-02.html#your-new-life",
    "href": "slides/lec-02.html#your-new-life",
    "title": "CISC482 - Lecture02",
    "section": "Your new life",
    "text": "Your new life\n\n\n\n\n\n\n\n\nStep\nDescription\n\n\n\n\nStep 1: Gathering data\nIdentify available and relevant data; gather new data if needed.\n\n\nStep 2: Cleaning data\nReformat datasets, create new features, and address missing values.\n\n\nStep 3: Exploring data\nCreate data visualizations and calculate summary statistics to explore potential relationships in the dataset.\n\n\nStep 4: Modeling data\nUse modeling skills and content knowledge to fit and evaluate models, measure relationships, and make predictions.\n\n\nStep 5: Interpreting data\nDescribe and interpret conclusions from data through written reports and presentations."
  },
  {
    "objectID": "slides/lec-02.html#visualizing-the-5-steps",
    "href": "slides/lec-02.html#visualizing-the-5-steps",
    "title": "CISC482 - Lecture02",
    "section": "Visualizing the 5 Steps",
    "text": "Visualizing the 5 Steps"
  },
  {
    "objectID": "slides/lec-02.html#gathering-data",
    "href": "slides/lec-02.html#gathering-data",
    "title": "CISC482 - Lecture02",
    "section": "Gathering Data",
    "text": "Gathering Data\nGetting a dataset is easier then ever\n\nKaggle Datasets\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "slides/lec-02.html#example-dataset---palmer-penguins",
    "href": "slides/lec-02.html#example-dataset---palmer-penguins",
    "title": "CISC482 - Lecture02",
    "section": "Example Dataset - Palmer Penguins",
    "text": "Example Dataset - Palmer Penguins\n\nThe Palmer penguins dataset by Allison Horst, Alison Hill, and Kristen Gorman"
  },
  {
    "objectID": "slides/lec-02.html#the-researchers",
    "href": "slides/lec-02.html#the-researchers",
    "title": "CISC482 - Lecture02",
    "section": "The Researchers",
    "text": "The Researchers"
  },
  {
    "objectID": "slides/lec-02.html#cleaning-data---before",
    "href": "slides/lec-02.html#cleaning-data---before",
    "title": "CISC482 - Lecture02",
    "section": "Cleaning Data - Before",
    "text": "Cleaning Data - Before\n\n\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      2007-11-11\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      2007-11-11\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      2007-11-16\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      2007-11-16\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      2007-11-16\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN"
  },
  {
    "objectID": "slides/lec-02.html#important-features",
    "href": "slides/lec-02.html#important-features",
    "title": "CISC482 - Lecture02",
    "section": "Important Features",
    "text": "Important Features"
  },
  {
    "objectID": "slides/lec-02.html#cleaning-data---after",
    "href": "slides/lec-02.html#cleaning-data---after",
    "title": "CISC482 - Lecture02",
    "section": "Cleaning Data - After",
    "text": "Cleaning Data - After\n\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007"
  },
  {
    "objectID": "slides/lec-02.html#exploring-data",
    "href": "slides/lec-02.html#exploring-data",
    "title": "CISC482 - Lecture02",
    "section": "Exploring Data",
    "text": "Exploring Data"
  },
  {
    "objectID": "slides/lec-02.html#modelling-data",
    "href": "slides/lec-02.html#modelling-data",
    "title": "CISC482 - Lecture02",
    "section": "Modelling Data",
    "text": "Modelling Data"
  },
  {
    "objectID": "slides/lec-02.html#interpreting-data",
    "href": "slides/lec-02.html#interpreting-data",
    "title": "CISC482 - Lecture02",
    "section": "Interpreting Data",
    "text": "Interpreting Data\n\nThere is a linear relationship between flipper length and body length\nGentoo penguins have significantly higher body length and body mass then Adelie or Chinstrap.\netc."
  },
  {
    "objectID": "slides/lec-02.html#big-takeaways",
    "href": "slides/lec-02.html#big-takeaways",
    "title": "CISC482 - Lecture02",
    "section": "Big Takeaways",
    "text": "Big Takeaways\n\nWhat is data science relationship with Statistics? Computer Science?\nThree V’s\nFive steps of data science"
  },
  {
    "objectID": "slides/lec-02.html#this-weeks-tasks",
    "href": "slides/lec-02.html#this-weeks-tasks",
    "title": "CISC482 - Lecture02",
    "section": "This week’s tasks",
    "text": "This week’s tasks\n\nSign up for the Zybook\nRead the syllabus\nHW1 - Markdown"
  },
  {
    "objectID": "slides/lec-02.html#practice",
    "href": "slides/lec-02.html#practice",
    "title": "CISC482 - Lecture02",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 01 - Practice Markdown"
  },
  {
    "objectID": "slides/lec-02.html#hw1",
    "href": "slides/lec-02.html#hw1",
    "title": "CISC482 - Lecture02",
    "section": "HW1",
    "text": "HW1\nHW Link\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-17.html#schedule",
    "href": "slides/lec-17.html#schedule",
    "title": "CISC482 - Lecture17",
    "section": "Schedule",
    "text": "Schedule\n\nReading 7-1: Mar 29 @ 12PM, Wednesday\nHW5 - Mar 29 @ Midnight, Wednesday\nReading 7-2: Mar 31 @ 12PM, Friday\nHW6 - Working on it… April 12 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-17.html#today",
    "href": "slides/lec-17.html#today",
    "title": "CISC482 - Lecture17",
    "section": "Today",
    "text": "Today\n\nReview Exam Results\nIntroduction to Supervised Learning\nKNN"
  },
  {
    "objectID": "slides/lec-17.html#exam-results",
    "href": "slides/lec-17.html#exam-results",
    "title": "CISC482 - Lecture17",
    "section": "Exam Results",
    "text": "Exam Results\n\n\nCurve: +3 Bonus Points (3 questions, 6% bump)\nAverage: 85%"
  },
  {
    "objectID": "slides/lec-17.html#top-difficult-questions---1",
    "href": "slides/lec-17.html#top-difficult-questions---1",
    "title": "CISC482 - Lecture17",
    "section": "Top Difficult Questions - #1",
    "text": "Top Difficult Questions - #1\n\n\nLinear model has higher precision\nLinear model has worse recall\nLinear model has more FP\nLinear model has more FN"
  },
  {
    "objectID": "slides/lec-17.html#fn-tp-tn-fp",
    "href": "slides/lec-17.html#fn-tp-tn-fp",
    "title": "CISC482 - Lecture17",
    "section": "FN, TP, TN, FP",
    "text": "FN, TP, TN, FP"
  },
  {
    "objectID": "slides/lec-17.html#top-difficult-questions---2",
    "href": "slides/lec-17.html#top-difficult-questions---2",
    "title": "CISC482 - Lecture17",
    "section": "Top Difficult Questions - #2",
    "text": "Top Difficult Questions - #2\nSelect all non-linear functions with respect to model parameters \\(\\mathbf{\\beta}\\)\n\n\\(\\beta_0 sin(x)\\)\n\\(\\beta_0 x^2\\)\n\\(\\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\)\n\\(e^{\\beta_0 + \\beta_1 x}\\)\n\\(sin(\\beta_0 x)\\)"
  },
  {
    "objectID": "slides/lec-17.html#top-difficult-questions---3",
    "href": "slides/lec-17.html#top-difficult-questions---3",
    "title": "CISC482 - Lecture17",
    "section": "Top Difficult Questions - #3",
    "text": "Top Difficult Questions - #3\n\n\nPlease explain if the assumptions of Linear Regression are met or not.\nYou must discuss at least 3 of the assumptions and clearly explain if each assumption is met from looking at the graph."
  },
  {
    "objectID": "slides/lec-17.html#top-difficult-questions---4",
    "href": "slides/lec-17.html#top-difficult-questions---4",
    "title": "CISC482 - Lecture17",
    "section": "Top Difficult Questions - #4",
    "text": "Top Difficult Questions - #4\n\n\nWhich day had the largest tip?"
  },
  {
    "objectID": "slides/lec-17.html#first-14",
    "href": "slides/lec-17.html#first-14",
    "title": "CISC482 - Lecture17",
    "section": "First 1/4",
    "text": "First 1/4\n\n\n\n\nflowchart LR\n    A[Stats and Prob] --> B[Descriptive Statistics]\n    A --> C[Distribtuions]\n    A --> D[Population Inference]\n\n    E[Data Exploration] --> F[Plotting]\n    E --> G[Dataframes]\n    E --> H[Exploratory Data Analysis]"
  },
  {
    "objectID": "slides/lec-17.html#first-12",
    "href": "slides/lec-17.html#first-12",
    "title": "CISC482 - Lecture17",
    "section": "First 1/2",
    "text": "First 1/2\n\n\n\n\nflowchart LR\n    A[Regression] --> B[Linear Regression]\n    A --> C[Mutliple Linear Regression]\n    A --> D[Polynomial Regression]\n    A --> Z[Logistic Regression]\n\n    E[Model Evaluation] --> F[RMSE, R^2]\n    E --> G[Precision, Recall]\n    E --> H[Train,Valid,Test ]"
  },
  {
    "objectID": "slides/lec-17.html#whats-next",
    "href": "slides/lec-17.html#whats-next",
    "title": "CISC482 - Lecture17",
    "section": "Whats next",
    "text": "Whats next\n\nMore advanced models!\nWe will still be using what we learned! Especially model evaluation!\nFully define concept of supervised learning"
  },
  {
    "objectID": "slides/lec-17.html#terms",
    "href": "slides/lec-17.html#terms",
    "title": "CISC482 - Lecture17",
    "section": "Terms",
    "text": "Terms\n\nAn instance is labeled if the outcome feature’s value is known for that instance.\nSupervised learning is training a model to predict a labeled outcome feature based on input features.\nThe outcome feature is also known as the target or dependent variable.\nThe input features are also known as features or independent variables"
  },
  {
    "objectID": "slides/lec-17.html#example-regression-problem",
    "href": "slides/lec-17.html#example-regression-problem",
    "title": "CISC482 - Lecture17",
    "section": "Example Regression Problem",
    "text": "Example Regression Problem"
  },
  {
    "objectID": "slides/lec-17.html#example-classification-problem",
    "href": "slides/lec-17.html#example-classification-problem",
    "title": "CISC482 - Lecture17",
    "section": "Example Classification Problem",
    "text": "Example Classification Problem"
  },
  {
    "objectID": "slides/lec-17.html#regression-vs-classification",
    "href": "slides/lec-17.html#regression-vs-classification",
    "title": "CISC482 - Lecture17",
    "section": "Regression vs Classification",
    "text": "Regression vs Classification\n\nRegression - Predicting a continuous numeric outcome\nClassification - Predicting the class (group) of an instance"
  },
  {
    "objectID": "slides/lec-17.html#goals-of-supervised-learning",
    "href": "slides/lec-17.html#goals-of-supervised-learning",
    "title": "CISC482 - Lecture17",
    "section": "Goals of Supervised Learning",
    "text": "Goals of Supervised Learning\n\nPredict the values that unlabeled data will have and to explain how the inputs lead to the predicted outputs.\nA model is interpretable if the relationship between input and output features in the model are easy to explain.\nA model is predictive if the outcomes produced by the model match the actual outcomes with new data."
  },
  {
    "objectID": "slides/lec-17.html#what-is-more-important",
    "href": "slides/lec-17.html#what-is-more-important",
    "title": "CISC482 - Lecture17",
    "section": "What is more important",
    "text": "What is more important\n\nA model can be both predictive and interpretable, but different clients and disciplines value one more than the other.\nEx: Scientists value interpretability more to explain the effect of experiments; image recognition models value predictiveness more.\nA large business may really only care about results. Being predictive may be the only thing that matters"
  },
  {
    "objectID": "slides/lec-17.html#introduction",
    "href": "slides/lec-17.html#introduction",
    "title": "CISC482 - Lecture17",
    "section": "Introduction",
    "text": "Introduction\n\nk-nearest neighbors (kNN) is a supervised learning algorithm that predicts the output feature of a new instance using other instances that are close for certain input features.\nA value of k is selected and the k instances with the closest input features are found. The output features of those k instances are used to make the prediction.\n\n\n\n\n\n\n\nTip\n\n\nSometimes, the phrase “birds of a feather flock together” is used to describe the k-nearest algorithm, meaning the algorithm assumes that instances with similar inputs will have similar outputs."
  },
  {
    "objectID": "slides/lec-17.html#motivation",
    "href": "slides/lec-17.html#motivation",
    "title": "CISC482 - Lecture17",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "slides/lec-17.html#example",
    "href": "slides/lec-17.html#example",
    "title": "CISC482 - Lecture17",
    "section": "Example",
    "text": "Example\n\n\n(4, 2.5)\n(0, 4)\n(2, 1)"
  },
  {
    "objectID": "slides/lec-17.html#finding-your-nearest-neighbors",
    "href": "slides/lec-17.html#finding-your-nearest-neighbors",
    "title": "CISC482 - Lecture17",
    "section": "Finding your nearest neighbors",
    "text": "Finding your nearest neighbors\n\n\nHow do determine how close a point is form the another (metric)\ndistance(x,y) = \\(\\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_p - y_p)^2 }\\)"
  },
  {
    "objectID": "slides/lec-17.html#using-knn-for-classification",
    "href": "slides/lec-17.html#using-knn-for-classification",
    "title": "CISC482 - Lecture17",
    "section": "Using KNN for Classification",
    "text": "Using KNN for Classification"
  },
  {
    "objectID": "slides/lec-17.html#regression-with-knn",
    "href": "slides/lec-17.html#regression-with-knn",
    "title": "CISC482 - Lecture17",
    "section": "Regression with KNN",
    "text": "Regression with KNN\n\nSelect k, the number of neighbors to consider.\nCalculate the distance between all labeled instances and the instance to predict.\nThe k instances with the shortest distances are the nearest neighbors.\nPredict by averaging the k nearest neighbors’ output values."
  },
  {
    "objectID": "slides/lec-17.html#scikit-learn",
    "href": "slides/lec-17.html#scikit-learn",
    "title": "CISC482 - Lecture17",
    "section": "scikit-learn",
    "text": "scikit-learn\n\nKNeighborsClassifier\nKNeighborsClassifier\nVery easy to use! Lets do an example"
  },
  {
    "objectID": "slides/lec-17.html#example---data",
    "href": "slides/lec-17.html#example---data",
    "title": "CISC482 - Lecture17",
    "section": "Example - Data",
    "text": "Example - Data\n\n\nCode\nX, y = make_blobs(n_samples=100, centers=2, cluster_std=1.5, n_features=2,\n                  random_state=0)\nc_names = [\"Class 0\", \"Class 1\"]\nfig, ax = plt.subplots(nrows=1, ncols=1)\nscatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis');\nax.legend(handles=scatter.legend_elements()[0], \n           labels=c_names,\n           title=\"Class\")\n\n\n<matplotlib.legend.Legend at 0x7f13489e12a0>"
  },
  {
    "objectID": "slides/lec-17.html#split-into-train-and-test",
    "href": "slides/lec-17.html#split-into-train-and-test",
    "title": "CISC482 - Lecture17",
    "section": "Split into Train and Test",
    "text": "Split into Train and Test\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\n# Plot data\nfig, ax = plt.subplots(nrows=1, ncols=2)\n\ndata = [(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]\nfor (X, y, title), ax_ in zip(data, ax):\n  scatter = ax_.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n  ax_.set_title(title)\n  ax_.set_xlabel(\"X0\")\n  ax_.set_ylabel(\"X1\")\n  ax_.legend(handles=scatter.legend_elements()[0], \n           labels=c_names,\n           title=\"Class\")"
  },
  {
    "objectID": "slides/lec-17.html#train-model-sanity-check",
    "href": "slides/lec-17.html#train-model-sanity-check",
    "title": "CISC482 - Lecture17",
    "section": "Train Model, Sanity Check",
    "text": "Train Model, Sanity Check\n\n\nCode\nfrom sklearn.neighbors import KNeighborsClassifier\n# Create and train model\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\n\n# Single point sanity check\ntest_point = [2, 1]\npredicted_group = model.predict([test_point])\nprint(f\"We predict that {test_point} will belong to class: {predicted_group}\")\n\n\nWe predict that [2, 1] will belong to class: [1]"
  },
  {
    "objectID": "slides/lec-17.html#test-model",
    "href": "slides/lec-17.html#test-model",
    "title": "CISC482 - Lecture17",
    "section": "Test Model",
    "text": "Test Model\n\n\nCode\n# Make predictions\npredictions = model.predict(X_test)\n\n# Plot data\nfig, ax = plt.subplots(nrows=1, ncols=2)\ndata = [(X_test, y_test, 'Test Set (Ground Truth)'), (X_test, predictions, 'Test Set Predicted')]\nfor (X, y, title), ax_ in zip(data, ax):\n  scatter = ax_.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n  ax_.set_title(title)\n  ax_.set_xlabel(\"X0\")\n  ax_.set_ylabel(\"X1\")\n  ax_.legend(handles=scatter.legend_elements()[0], \n           labels=c_names,\n           title=\"Class\")"
  },
  {
    "objectID": "slides/lec-17.html#confusion-matrix-plot",
    "href": "slides/lec-17.html#confusion-matrix-plot",
    "title": "CISC482 - Lecture17",
    "section": "Confusion Matrix, Plot",
    "text": "Confusion Matrix, Plot\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_test, predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=['Negative', 'Positive'])\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\nConfusion Matrix\n[[12  4]\n [ 1 13]]"
  },
  {
    "objectID": "slides/lec-17.html#classication-metrics",
    "href": "slides/lec-17.html#classication-metrics",
    "title": "CISC482 - Lecture17",
    "section": "Classication Metrics",
    "text": "Classication Metrics\n\nAccuracy = \\(\\frac{TP+TN}{TP+TN+FP+FN}\\)\nPrecision = \\(\\frac{TP}{TP+FP}\\)\nRecall = \\(\\frac{TP}{TP+FN}\\)\nF1 = \\(\\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}\\)"
  },
  {
    "objectID": "slides/lec-17.html#classification-report",
    "href": "slides/lec-17.html#classification-report",
    "title": "CISC482 - Lecture17",
    "section": "Classification Report",
    "text": "Classification Report\n\n\nCode\nfrom sklearn.metrics import classification_report\nreport = classification_report(y_test, predictions, target_names=[\"Negative\", \"Postive\"])\nprint(report)\n\n\n              precision    recall  f1-score   support\n\n    Negative       0.92      0.75      0.83        16\n     Postive       0.76      0.93      0.84        14\n\n    accuracy                           0.83        30\n   macro avg       0.84      0.84      0.83        30\nweighted avg       0.85      0.83      0.83        30\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-14.html#schedule",
    "href": "slides/lec-14.html#schedule",
    "title": "CISC482 - Lecture14",
    "section": "Schedule",
    "text": "Schedule\n\nReading 6-1: Mar 08 @ 12PM, Wednesday\nReading 6-2: Mar 10 @ 12PM, Friday\nProposal: Mar 22, Wednesday\nHW5 - Mar 29 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-14.html#today",
    "href": "slides/lec-14.html#today",
    "title": "CISC482 - Lecture14",
    "section": "Today",
    "text": "Today\n\nOverfit/Underfit\nBias/Variance Trade off\nRegression Metric\nBinary Classification Metrics"
  },
  {
    "objectID": "slides/lec-14.html#modelling",
    "href": "slides/lec-14.html#modelling",
    "title": "CISC482 - Lecture14",
    "section": "Modelling",
    "text": "Modelling\n\nWe approximate an output feature \\(y\\), using input features \\(X\\) with function \\(f\\) such that \\(\\hat{y} = f(X)\\)\n\nExample: Predicting penguin body mass by bill length\n\\(\\text{body mass} = \\hat{y} = mx + b\\)\n\nWe have to choose \\(f\\) and \\(X\\): simple linear model, polynomial model, multiple linear regression, logistic regression, etc.\nExample is \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) or is \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)"
  },
  {
    "objectID": "slides/lec-14.html#underfit",
    "href": "slides/lec-14.html#underfit",
    "title": "CISC482 - Lecture14",
    "section": "Underfit",
    "text": "Underfit\n\n\nUnderfit - model is too simple to fit the data well."
  },
  {
    "objectID": "slides/lec-14.html#underfit-problems",
    "href": "slides/lec-14.html#underfit-problems",
    "title": "CISC482 - Lecture14",
    "section": "Underfit Problems",
    "text": "Underfit Problems\n\nAn underfit model will miss the underlying trend\nWill score poorly in metrics"
  },
  {
    "objectID": "slides/lec-14.html#overfit",
    "href": "slides/lec-14.html#overfit",
    "title": "CISC482 - Lecture14",
    "section": "Overfit",
    "text": "Overfit\n\n\nOverfit - model is too complex to fit the data well."
  },
  {
    "objectID": "slides/lec-14.html#overfit-problems",
    "href": "slides/lec-14.html#overfit-problems",
    "title": "CISC482 - Lecture14",
    "section": "Overfit Problems",
    "text": "Overfit Problems\n\nFitting the data too closely\nIncorporating too much noise (meaningless variation)\nMisses the general trend of the data despite scoring well in some metrics\nIn fact, what is the error for this model?"
  },
  {
    "objectID": "slides/lec-14.html#optimal",
    "href": "slides/lec-14.html#optimal",
    "title": "CISC482 - Lecture14",
    "section": "Optimal",
    "text": "Optimal\n\nThis model would be best fit with a quadratic model"
  },
  {
    "objectID": "slides/lec-14.html#note",
    "href": "slides/lec-14.html#note",
    "title": "CISC482 - Lecture14",
    "section": "Note",
    "text": "Note\n\n\n\n\n\n\nImportant\n\n\nA model that is overfit or underfit is a bad predictor of outcomes outside of the data set and should not be used. In the field of data science, models tend to be overfit, so model selection techniques focus on choosing the least complex model that captures the general trend."
  },
  {
    "objectID": "slides/lec-14.html#find-most-underfit-and-most-overfit",
    "href": "slides/lec-14.html#find-most-underfit-and-most-overfit",
    "title": "CISC482 - Lecture14",
    "section": "Find Most Underfit and Most Overfit",
    "text": "Find Most Underfit and Most Overfit"
  },
  {
    "objectID": "slides/lec-14.html#breaking-down-error",
    "href": "slides/lec-14.html#breaking-down-error",
    "title": "CISC482 - Lecture14",
    "section": "Breaking down Error",
    "text": "Breaking down Error\n\nThe total error of a model is how much the observed values differ from predicted values. Total error is broken down into three pieces:\n\nBias - model’s prediction differs from the observed values due to the assumptions built into the model.\nVariance - spread/variance of predictions\nIrreducible error - error inherent to the data (noise)"
  },
  {
    "objectID": "slides/lec-14.html#visual-explanation",
    "href": "slides/lec-14.html#visual-explanation",
    "title": "CISC482 - Lecture14",
    "section": "Visual Explanation",
    "text": "Visual Explanation"
  },
  {
    "objectID": "slides/lec-14.html#bias-variance-tradeoff",
    "href": "slides/lec-14.html#bias-variance-tradeoff",
    "title": "CISC482 - Lecture14",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\n\nChoosing a more complex model (more features, a more complicated mathematical expression, etc.) means the model’s predictions are closer to the observed sample values, which decreases the bias.\nHowever, doing so makes the model’s predictions more spread out to meet the observed values, increasing the variance.\nAn optimal model should be just complex enough to capture the general trend of the data (low bias) without incorporating too much of the noise from the sample (low variance)."
  },
  {
    "objectID": "slides/lec-14.html#visual-example",
    "href": "slides/lec-14.html#visual-example",
    "title": "CISC482 - Lecture14",
    "section": "Visual Example",
    "text": "Visual Example"
  },
  {
    "objectID": "slides/lec-14.html#problem-1",
    "href": "slides/lec-14.html#problem-1",
    "title": "CISC482 - Lecture14",
    "section": "Problem 1",
    "text": "Problem 1"
  },
  {
    "objectID": "slides/lec-14.html#problem-2",
    "href": "slides/lec-14.html#problem-2",
    "title": "CISC482 - Lecture14",
    "section": "Problem 2",
    "text": "Problem 2"
  },
  {
    "objectID": "slides/lec-14.html#dataset",
    "href": "slides/lec-14.html#dataset",
    "title": "CISC482 - Lecture14",
    "section": "Dataset",
    "text": "Dataset\n\nWe will be using the penguin dataset\n\n\n\nCode\nX = df['bill_length_mm'].values[:, np.newaxis]\ny = df['body_mass_g'].values\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.10\n      18.70\n      181.00\n      3,750.00\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.50\n      17.40\n      186.00\n      3,800.00\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.30\n      18.00\n      195.00\n      3,250.00\n      female\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.70\n      19.30\n      193.00\n      3,450.00\n      female\n      2007\n    \n    \n      5\n      Adelie\n      Torgersen\n      39.30\n      20.60\n      190.00\n      3,650.00\n      male\n      2007"
  },
  {
    "objectID": "slides/lec-14.html#two-statistics",
    "href": "slides/lec-14.html#two-statistics",
    "title": "CISC482 - Lecture14",
    "section": "Two statistics",
    "text": "Two statistics\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = \\frac{\\text{variation explained by regression}}{\\text{total variation in the data}} = \\frac{\\sum (\\hat{y}_i - \\bar{y})^2}{\\sum (y_i - \\bar{y})^2} \\\\\n  R^2 = 1 - \\frac{\\sum (\\hat{y}_i - y_i)}{\\sum (y_i - \\bar{y})^2}\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/lec-14.html#r-squared",
    "href": "slides/lec-14.html#r-squared",
    "title": "CISC482 - Lecture14",
    "section": "R-squared",
    "text": "R-squared\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor), Unitless\nCalculate with model.score(X, y):\n\n\nmodel = LinearRegression()\nmodel.fit(X,y)\nr_squared = model.score(X,y)\nprint(f\"R^2 = {r_squared:.2f}\")\n\nR^2 = 0.35\n\n\n\nx = X[:, 0]\nregressed_fn = np.poly1d(np.polyfit(x, y, 1))\ny_hat = regressed_fn(x)\nr_squared = np.sum(np.square(y_hat - y.mean())) / np.sum(np.square(y.mean() - y))\nprint(f\"R^2 = {r_squared:.2f}\")\n\nR^2 = 0.35"
  },
  {
    "objectID": "slides/lec-14.html#more-examples",
    "href": "slides/lec-14.html#more-examples",
    "title": "CISC482 - Lecture14",
    "section": "More Examples",
    "text": "More Examples\n\nr = np.corrcoef(y, y_hat)[0, 1] # matrix, get first row second column\nr_squared = r ** 2\nprint(f\"R^2 = {r_squared:.2f}\")\n\nR^2 = 0.35"
  },
  {
    "objectID": "slides/lec-14.html#graph",
    "href": "slides/lec-14.html#graph",
    "title": "CISC482 - Lecture14",
    "section": "Graph",
    "text": "Graph\n\nax = sns.scatterplot(x=X[:,0], y=y, color=\"black\")\nax.plot(X, model.predict(X), color=\"red\", linewidth=3)"
  },
  {
    "objectID": "slides/lec-14.html#r2-example",
    "href": "slides/lec-14.html#r2-example",
    "title": "CISC482 - Lecture14",
    "section": "\\(R^2\\) Example",
    "text": "\\(R^2\\) Example"
  },
  {
    "objectID": "slides/lec-14.html#interpreting-r-squared",
    "href": "slides/lec-14.html#interpreting-r-squared",
    "title": "CISC482 - Lecture14",
    "section": "Interpreting R-squared",
    "text": "Interpreting R-squared\n\nThe \\(R^2\\) of the model for predicting penguin mass from bill length is 25%. Which of the following is the correct interpretation of this value?\n\n\nBill Length correctly predicts 25% of penguin mass.\n25% of the variability in penguin mass can be explained by bill length.\n25% of the time penugin mass can be predicted by bill length."
  },
  {
    "objectID": "slides/lec-14.html#rmse",
    "href": "slides/lec-14.html#rmse",
    "title": "CISC482 - Lecture14",
    "section": "RMSE",
    "text": "RMSE\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the outcome variable\nCalculate with means_squared_error(y_true, y_pred):\n\nfrom sklearn import metrics\nrmse = metrics.mean_squared_error(y, model.predict(X))\nprint(f\"RMSE: {rmse:.2f}\")\n\nRMSE: 421823.22\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models.\nComparing a model that uses bill length for a predictor or using flipper length"
  },
  {
    "objectID": "slides/lec-14.html#rmse-example",
    "href": "slides/lec-14.html#rmse-example",
    "title": "CISC482 - Lecture14",
    "section": "RMSE Example",
    "text": "RMSE Example"
  },
  {
    "objectID": "slides/lec-14.html#truefalse-positivenegative",
    "href": "slides/lec-14.html#truefalse-positivenegative",
    "title": "CISC482 - Lecture14",
    "section": "True/False Positive/Negative",
    "text": "True/False Positive/Negative\n\nTrue Positive (TP) is an outcome that was correctly identified as positive\nTrue Negative (TN) is an outcome that was correctly identified as negative.\nFalse Positive (FP) is an outcome that was incorrectly identified as positive\nFalse Negative (TN) is an outcome that was incorrectly identified as negative"
  },
  {
    "objectID": "slides/lec-14.html#confustion-matrix",
    "href": "slides/lec-14.html#confustion-matrix",
    "title": "CISC482 - Lecture14",
    "section": "Confustion Matrix",
    "text": "Confustion Matrix\n\n\n\n\nPositive (predicted)\nNegative (predicted)\n\n\n\n\nPositive (actual)\n170\n21\n\n\nNegative (actual)\n1\n377"
  },
  {
    "objectID": "slides/lec-14.html#metrics",
    "href": "slides/lec-14.html#metrics",
    "title": "CISC482 - Lecture14",
    "section": "Metrics",
    "text": "Metrics\n\nAccuracy - useful\nPrecison - very useful\nRecall - very useful"
  },
  {
    "objectID": "slides/lec-14.html#accuracy",
    "href": "slides/lec-14.html#accuracy",
    "title": "CISC482 - Lecture14",
    "section": "Accuracy",
    "text": "Accuracy\n\n\nAccuracy: \\(\\frac{\\text{# Correctly Predicted}}{\\text{Total}}\\)\n\\(\\frac{TP + TN}{TP + TN + FP + FN}\\)"
  },
  {
    "objectID": "slides/lec-14.html#precision",
    "href": "slides/lec-14.html#precision",
    "title": "CISC482 - Lecture14",
    "section": "Precision",
    "text": "Precision\n\n\nTell how precise your prediction is\n\\(\\frac{TP}{TP + FP}\\)\nThe higher this number, the less False Positives you have\nMy research - Identifying an emegency landing location nearby. A precison gives confidence that it truly is safe to land at the location the model predicts."
  },
  {
    "objectID": "slides/lec-14.html#recall",
    "href": "slides/lec-14.html#recall",
    "title": "CISC482 - Lecture14",
    "section": "Recall",
    "text": "Recall\n\n\nthe proportion of positives that were correctly predicted\n\\(\\frac{TP}{TP + FN}\\)\nThe higher this number, the less False Negative you have.\nMy Research - A high recall means I found nearly all the rooftops in the city that you could land on."
  },
  {
    "objectID": "slides/lec-14.html#example-question",
    "href": "slides/lec-14.html#example-question",
    "title": "CISC482 - Lecture14",
    "section": "Example Question",
    "text": "Example Question\n\n\n\n\nPositive (predicted)\nNegative (predicted)\n\n\n\n\nPositive (actual)\n170\n21\n\n\nNegative (actual)\n1\n377\n\n\n\n\nWhat is the Accuracy, Precision, and Recall?"
  },
  {
    "objectID": "slides/lec-14.html#tradeoff",
    "href": "slides/lec-14.html#tradeoff",
    "title": "CISC482 - Lecture14",
    "section": "Tradeoff",
    "text": "Tradeoff\n\nIn logisitc regression you specify a threshold to use a prediciton. By deafult we use 0.5 or 50%. But that is arbitary and you move that threshold"
  },
  {
    "objectID": "slides/lec-14.html#low-threshold",
    "href": "slides/lec-14.html#low-threshold",
    "title": "CISC482 - Lecture14",
    "section": "Low Threshold",
    "text": "Low Threshold"
  },
  {
    "objectID": "slides/lec-14.html#high-threshold",
    "href": "slides/lec-14.html#high-threshold",
    "title": "CISC482 - Lecture14",
    "section": "High Threshold",
    "text": "High Threshold\n\n\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-06.html#schedule",
    "href": "slides/lec-06.html#schedule",
    "title": "CISC482 - Lecture06",
    "section": "Schedule",
    "text": "Schedule\n\nReading 3-1: Feb 3 @ 12PM, Today!\nReading 3-2: Feb 8 @ 12PM, Wednesday\nReading 4-1: Feb 10 @ 12PM, Friday\nHW3: Feb 15 @ Midnight, Wednesday\nExam 1: Feb 15 in Class"
  },
  {
    "objectID": "slides/lec-06.html#cs-faculty-candidate",
    "href": "slides/lec-06.html#cs-faculty-candidate",
    "title": "CISC482 - Lecture06",
    "section": "CS Faculty Candidate",
    "text": "CS Faculty Candidate\n\nBenz Tran is here today!\nPlease attend a meet and greet at 3:15 in SBSC 112\nExtra Credit!"
  },
  {
    "objectID": "slides/lec-06.html#steps",
    "href": "slides/lec-06.html#steps",
    "title": "CISC482 - Lecture06",
    "section": "Steps",
    "text": "Steps\n\nDiscovery - Familiarize with source data\nStructuring - Transforms features to uniform formats, units, and scales.\nCleaning - Removes or replaces missing and outlier data.\nEnriching - Derives new features from existing features and appends new data from external sources.\nValidating - Verifies that the dataset is internally consistent and accurate\nPublishing - Makes the dataset available to others"
  },
  {
    "objectID": "slides/lec-06.html#example",
    "href": "slides/lec-06.html#example",
    "title": "CISC482 - Lecture06",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lec-06.html#extract-transform-load",
    "href": "slides/lec-06.html#extract-transform-load",
    "title": "CISC482 - Lecture06",
    "section": "Extract, Transform, Load",
    "text": "Extract, Transform, Load\n\nExtract, Transform, Load (ETL) is a process that extracts data from databases, transforms the data, and loads the data into an analytic database.\nSimilar to data wrangling: process data, clean, enrich, etc.\nDifference is that data wrangling is more informal, where ETL is a usually a business process or service."
  },
  {
    "objectID": "slides/lec-06.html#what-is-pandas",
    "href": "slides/lec-06.html#what-is-pandas",
    "title": "CISC482 - Lecture06",
    "section": "What is Pandas",
    "text": "What is Pandas\n\n\n\nPandas is an open source Python package\nWidely used for data science/data analysis\nKey idea is organizing data into a dataframe\nTabular data, effecient queries, uses numpy if possible"
  },
  {
    "objectID": "slides/lec-06.html#loading-pandas-from-python-lists",
    "href": "slides/lec-06.html#loading-pandas-from-python-lists",
    "title": "CISC482 - Lecture06",
    "section": "Loading Pandas from python lists",
    "text": "Loading Pandas from python lists\n\n\nimport pandas as pd\nd = {'a': [1, 2, 3], \n    'b': [4.0, 5.0, 6.0]\n    }\ndf = pd.DataFrame(data=d)\ndf\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      4.0\n    \n    \n      1\n      2\n      5.0\n    \n    \n      2\n      3\n      6.0\n    \n  \n\n\n\n\n\n\nPandas creates an index automatically (unnamed column)"
  },
  {
    "objectID": "slides/lec-06.html#loading-pandas-from-numpy",
    "href": "slides/lec-06.html#loading-pandas-from-numpy",
    "title": "CISC482 - Lecture06",
    "section": "Loading Pandas from NumPy",
    "text": "Loading Pandas from NumPy\n\n\nd = {'a': np.array([1, 2, 3]), \n    'b': np.array([4.0, 5.0, 6.0])\n    }\ndf = pd.DataFrame(data=d)\ndf\n\n\n\n\n\n\n  \n    \n      \n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      4.0\n    \n    \n      1\n      2\n      5.0\n    \n    \n      2\n      3\n      6.0\n    \n  \n\n\n\n\n\n\nYou can also use NumPy arrays"
  },
  {
    "objectID": "slides/lec-06.html#csv-files",
    "href": "slides/lec-06.html#csv-files",
    "title": "CISC482 - Lecture06",
    "section": "CSV Files",
    "text": "CSV Files\nLink"
  },
  {
    "objectID": "slides/lec-06.html#loading-pandas-from-a-csv",
    "href": "slides/lec-06.html#loading-pandas-from-a-csv",
    "title": "CISC482 - Lecture06",
    "section": "Loading Pandas from a CSV",
    "text": "Loading Pandas from a CSV\n\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins-raw.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      2007-11-11\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      2007-11-11\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      2007-11-16\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      2007-11-16\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      2007-11-16\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN"
  },
  {
    "objectID": "slides/lec-06.html#pandas-methods---info",
    "href": "slides/lec-06.html#pandas-methods---info",
    "title": "CISC482 - Lecture06",
    "section": "Pandas Methods - Info",
    "text": "Pandas Methods - Info\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            344 non-null    object \n 1   Sample Number        344 non-null    int64  \n 2   Species              344 non-null    object \n 3   Region               344 non-null    object \n 4   Island               344 non-null    object \n 5   Stage                344 non-null    object \n 6   Individual ID        344 non-null    object \n 7   Clutch Completion    344 non-null    object \n 8   Date Egg             344 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  333 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             54 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 45.8+ KB"
  },
  {
    "objectID": "slides/lec-06.html#select-only-interesting-columns",
    "href": "slides/lec-06.html#select-only-interesting-columns",
    "title": "CISC482 - Lecture06",
    "section": "Select Only Interesting Columns",
    "text": "Select Only Interesting Columns\n\n\n\ninteresting_columns = ['Species', 'Island', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex', 'Date Egg']\ndf = df[interesting_columns]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Species\n      Island\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Date Egg\n    \n  \n  \n    \n      0\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      2007-11-11\n    \n    \n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      2007-11-11\n    \n    \n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      2007-11-16\n    \n    \n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007-11-16\n    \n    \n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      2007-11-16"
  },
  {
    "objectID": "slides/lec-06.html#rename-columns",
    "href": "slides/lec-06.html#rename-columns",
    "title": "CISC482 - Lecture06",
    "section": "Rename Columns",
    "text": "Rename Columns\n\n\n\nnew_names = ['species', 'island', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'year']\nmapping = dict(zip(interesting_columns, new_names))\ndf = df.rename(columns=mapping)\ndf.head()\n\n\n\n\n{'Body Mass (g)': 'body_mass_g',\n 'Culmen Depth (mm)': 'bill_depth_mm',\n 'Culmen Length (mm)': 'bill_length_mm',\n 'Date Egg': 'year',\n 'Flipper Length (mm)': 'flipper_length_mm',\n 'Island': 'island',\n 'Sex': 'sex',\n 'Species': 'species'}"
  },
  {
    "objectID": "slides/lec-06.html#rename-columns-output",
    "href": "slides/lec-06.html#rename-columns-output",
    "title": "CISC482 - Lecture06",
    "section": "Rename Columns",
    "text": "Rename Columns\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      2007-11-11\n    \n    \n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      2007-11-11\n    \n    \n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      2007-11-16\n    \n    \n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007-11-16\n    \n    \n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      2007-11-16"
  },
  {
    "objectID": "slides/lec-06.html#remove-nan",
    "href": "slides/lec-06.html#remove-nan",
    "title": "CISC482 - Lecture06",
    "section": "Remove Nan",
    "text": "Remove Nan\n\n\n\ndf = df.dropna()\ndf = df.reset_index(drop=True) # used to be 343, now 333\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    float64\n 5   body_mass_g        333 non-null    float64\n 6   sex                333 non-null    object \n 7   year               333 non-null    object \ndtypes: float64(4), object(4)\nmemory usage: 20.9+ KB"
  },
  {
    "objectID": "slides/lec-06.html#recap",
    "href": "slides/lec-06.html#recap",
    "title": "CISC482 - Lecture06",
    "section": "Recap",
    "text": "Recap\n\nGot raw data by using pd.read_csv(..)\nLook at data\nSelected columns of interest\nRename columns\nTODO\n\nExtract year as integer from string year column\nlower case sex column\nsimplify species name"
  },
  {
    "objectID": "slides/lec-06.html#selecting-data",
    "href": "slides/lec-06.html#selecting-data",
    "title": "CISC482 - Lecture06",
    "section": "Selecting Data",
    "text": "Selecting Data\n\nYou can select a column by using the [] notation\nOne column wil return a series, more than one column will return a new data frame is returned\n\n\n\n\n\ncolumn = df['year']\ncolumn\n\n\n0      2007-11-11\n1      2007-11-11\n2      2007-11-16\n3      2007-11-16\n4      2007-11-16\n          ...    \n328    2009-11-19\n329    2009-11-21\n330    2009-11-21\n331    2009-11-21\n332    2009-11-21\nName: year, Length: 333, dtype: object"
  },
  {
    "objectID": "slides/lec-06.html#selecting-data-view",
    "href": "slides/lec-06.html#selecting-data-view",
    "title": "CISC482 - Lecture06",
    "section": "Selecting Data, View",
    "text": "Selecting Data, View\n\n\n\n\ndf2 = df[['year', 'sex']]\ndf2.head()\n\n\n\n\n\n\n  \n    \n      \n      year\n      sex\n    \n  \n  \n    \n      0\n      2007-11-11\n      MALE\n    \n    \n      1\n      2007-11-11\n      FEMALE\n    \n    \n      2\n      2007-11-16\n      FEMALE\n    \n    \n      3\n      2007-11-16\n      FEMALE\n    \n    \n      4\n      2007-11-16\n      MALE"
  },
  {
    "objectID": "slides/lec-06.html#create-a-new-column",
    "href": "slides/lec-06.html#create-a-new-column",
    "title": "CISC482 - Lecture06",
    "section": "Create a new column",
    "text": "Create a new column\n\n\n\n\ndf2['test'] = 'CISC482'\ndf2.head()\n\n\n\n\n\n\n  \n    \n      \n      year\n      sex\n      test\n    \n  \n  \n    \n      0\n      2007-11-11\n      MALE\n      CISC482\n    \n    \n      1\n      2007-11-11\n      FEMALE\n      CISC482\n    \n    \n      2\n      2007-11-16\n      FEMALE\n      CISC482\n    \n    \n      3\n      2007-11-16\n      FEMALE\n      CISC482\n    \n    \n      4\n      2007-11-16\n      MALE\n      CISC482"
  },
  {
    "objectID": "slides/lec-06.html#changing-column",
    "href": "slides/lec-06.html#changing-column",
    "title": "CISC482 - Lecture06",
    "section": "Changing Column",
    "text": "Changing Column\n\n\n\n\ndf2.loc[:, 'test'] = 'DataScience'\n# df2['test'] = \"Data Science\" # don't do it!\ndf2.head()\n\n\n\n\n\n\n  \n    \n      \n      year\n      sex\n      test\n    \n  \n  \n    \n      0\n      2007-11-11\n      MALE\n      DataScience\n    \n    \n      1\n      2007-11-11\n      FEMALE\n      DataScience\n    \n    \n      2\n      2007-11-16\n      FEMALE\n      DataScience\n    \n    \n      3\n      2007-11-16\n      FEMALE\n      DataScience\n    \n    \n      4\n      2007-11-16\n      MALE\n      DataScience"
  },
  {
    "objectID": "slides/lec-06.html#selecting-some-rows",
    "href": "slides/lec-06.html#selecting-some-rows",
    "title": "CISC482 - Lecture06",
    "section": "Selecting Some Rows",
    "text": "Selecting Some Rows\n\n\n\n\ndf_big_bills = df[df['bill_length_mm'] > 56]\ndf_big_bills\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      178\n      Gentoo penguin (Pygoscelis papua)\n      Biscoe\n      59.6\n      17.0\n      230.0\n      6050.0\n      MALE\n      2007-12-03\n    \n    \n      282\n      Chinstrap penguin (Pygoscelis antarctica)\n      Dream\n      58.0\n      17.8\n      181.0\n      3700.0\n      FEMALE\n      2007-11-30\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nHow would I just select the male penguins\n\n\n\n\n\ndf_male = df[df['sex'] == \"MALE\"]"
  },
  {
    "objectID": "slides/lec-06.html#fixing-the-year",
    "href": "slides/lec-06.html#fixing-the-year",
    "title": "CISC482 - Lecture06",
    "section": "Fixing the Year",
    "text": "Fixing the Year\n\n\n\ndf.loc[:, 'year'] = df['year'].str[:4]\n# df.loc[:, 'year'] = df['year'].apply(lambda x: str(x).split('-')[0])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      2007\n    \n    \n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      2007\n    \n    \n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      2007\n    \n    \n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      2007\n    \n    \n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.3\n      20.6\n      190.0\n      3650.0\n      MALE\n      2007"
  },
  {
    "objectID": "slides/lec-06.html#fixing-the-species",
    "href": "slides/lec-06.html#fixing-the-species",
    "title": "CISC482 - Lecture06",
    "section": "Fixing the Species",
    "text": "Fixing the Species\n\n\n\nmapping = {\n  \"Adelie Penguin (Pygoscelis adeliae)\": \"Adelie\", \n  \"Gentoo penguin (Pygoscelis papua)\": \"Gentoo\",\n  \"Chinstrap penguin (Pygoscelis antarctica)\": \"Chinstrap\"\n}\ndf.loc[:, 'species'] = df['species'].map(mapping)\n# df.loc[:, 'species'] = df['species'].apply(lambda x: str(x).split('-')[0])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      39.3\n      20.6\n      190.0\n      3650.0\n      MALE\n      2007"
  },
  {
    "objectID": "slides/lec-06.html#fixing-the-sex",
    "href": "slides/lec-06.html#fixing-the-sex",
    "title": "CISC482 - Lecture06",
    "section": "Fixing the sex",
    "text": "Fixing the sex\n\n\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      female\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      39.3\n      20.6\n      190.0\n      3650.0\n      male\n      2007\n    \n  \n\n\n\n\n\n\n\ndf.loc[:, 'sex'] = df['sex'].str.lower()"
  },
  {
    "objectID": "slides/lec-06.html#checking-data-types",
    "href": "slides/lec-06.html#checking-data-types",
    "title": "CISC482 - Lecture06",
    "section": "Checking Data Types",
    "text": "Checking Data Types\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    float64\n 5   body_mass_g        333 non-null    float64\n 6   sex                333 non-null    object \n 7   year               333 non-null    object \ndtypes: float64(4), object(4)\nmemory usage: 20.9+ KB"
  },
  {
    "objectID": "slides/lec-06.html#checking-data-types-1",
    "href": "slides/lec-06.html#checking-data-types-1",
    "title": "CISC482 - Lecture06",
    "section": "Checking Data Types",
    "text": "Checking Data Types\n\n\n\ndf.loc[:, 'year'] = pd.to_numeric(df['year'])\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 333 entries, 0 to 332\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            333 non-null    object \n 1   island             333 non-null    object \n 2   bill_length_mm     333 non-null    float64\n 3   bill_depth_mm      333 non-null    float64\n 4   flipper_length_mm  333 non-null    float64\n 5   body_mass_g        333 non-null    float64\n 6   sex                333 non-null    object \n 7   year               333 non-null    object \ndtypes: float64(4), object(4)\nmemory usage: 20.9+ KB"
  },
  {
    "objectID": "slides/lec-06.html#practice-pandas",
    "href": "slides/lec-06.html#practice-pandas",
    "title": "CISC482 - Lecture06",
    "section": "Practice Pandas",
    "text": "Practice Pandas\nCleaning Data and Graphing\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-10.html#schedule",
    "href": "slides/lec-10.html#schedule",
    "title": "CISC482 - Lecture10",
    "section": "Schedule",
    "text": "Schedule\n\nTopic Ideas - Feb 22 @ Midnight\nReading 5-1: Feb 22 @ 12PM, Wednesday\nReading 5-2: Feb 24 @ 12PM, Friday\nReading 5-3: Mar 01 @ 12PM, Wednesday"
  },
  {
    "objectID": "slides/lec-10.html#today",
    "href": "slides/lec-10.html#today",
    "title": "CISC482 - Lecture10",
    "section": "Today",
    "text": "Today\n\nIntroduction to Regression\nReview Topic Ideas\nMore Exploratory Data Analysis"
  },
  {
    "objectID": "slides/lec-10.html#input-and-output",
    "href": "slides/lec-10.html#input-and-output",
    "title": "CISC482 - Lecture10",
    "section": "Input and Output",
    "text": "Input and Output\n\nAn input feature takes values without being impacted by any other features.\nAn output feature has values that vary in response to variation in some other feature(s).\n\nWe often called this the response variable\n\nIn an experiment, input features are often controlled by researchters, and output features are observed\nWe often visualize these with scatter plots"
  },
  {
    "objectID": "slides/lec-10.html#example-data",
    "href": "slides/lec-10.html#example-data",
    "title": "CISC482 - Lecture10",
    "section": "Example Data",
    "text": "Example Data\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.10\n      18.70\n      181.00\n      3,750.00\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.50\n      17.40\n      186.00\n      3,800.00\n      female\n      2007\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.30\n      18.00\n      195.00\n      3,250.00\n      female\n      2007\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.70\n      19.30\n      193.00\n      3,450.00\n      female\n      2007"
  },
  {
    "objectID": "slides/lec-10.html#scatter-plot",
    "href": "slides/lec-10.html#scatter-plot",
    "title": "CISC482 - Lecture10",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\n\nCorrelation: 0.60"
  },
  {
    "objectID": "slides/lec-10.html#describing",
    "href": "slides/lec-10.html#describing",
    "title": "CISC482 - Lecture10",
    "section": "Describing",
    "text": "Describing\n\nThe direction: positive if larger values of one feature correspond to larger values of the other feature.\nThe form: linear pattern or a nonlinear pattern. Sometimes two features may not have an obvious form.\nThe strength: how closely the observations in a scatter plot follow the form’s pattern."
  },
  {
    "objectID": "slides/lec-10.html#questions",
    "href": "slides/lec-10.html#questions",
    "title": "CISC482 - Lecture10",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "slides/lec-10.html#correlation",
    "href": "slides/lec-10.html#correlation",
    "title": "CISC482 - Lecture10",
    "section": "Correlation",
    "text": "Correlation\n\nCorrelation is a statistical measure that expresses the extent to which two variables are linearly related\nThey change together at a constant rate\nIt’s a common tool for describing simple relationships without making a statement about cause and effect.\nRange from -1 to 0 to +1"
  },
  {
    "objectID": "slides/lec-10.html#regression-model",
    "href": "slides/lec-10.html#regression-model",
    "title": "CISC482 - Lecture10",
    "section": "Regression Model",
    "text": "Regression Model\n\nA model for an output feature \\(y\\) using input feature(s) \\(X\\) is a function \\(f(X)\\) that predicts an expected value \\(\\hat{y}\\) for a given value of \\(X\\).and\n\\(\\hat{y} = f(X)\\)\nA regression model is one that has a numerical output feature and input features."
  },
  {
    "objectID": "slides/lec-10.html#regression-example",
    "href": "slides/lec-10.html#regression-example",
    "title": "CISC482 - Lecture10",
    "section": "Regression Example",
    "text": "Regression Example\n\nWhat is the error of our model (Residual)"
  },
  {
    "objectID": "slides/lec-10.html#main-equation",
    "href": "slides/lec-10.html#main-equation",
    "title": "CISC482 - Lecture10",
    "section": "Main Equation",
    "text": "Main Equation\n\n\nSet of data: \\(\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\)\n\nx: input, y: output\n\nModel: \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\epsilon\\)\n\n\\(\\hat{y}\\) = prediction\n\\(\\beta_0\\) = y-intercept\n\\(\\beta_1\\) = slope\n\\(\\epsilon = error\\)"
  },
  {
    "objectID": "slides/lec-10.html#visual-explanation",
    "href": "slides/lec-10.html#visual-explanation",
    "title": "CISC482 - Lecture10",
    "section": "Visual Explanation",
    "text": "Visual Explanation\n\n\n\n\n\n\n\nTip\n\n\nHow can we formalize what we are optimizing for?"
  },
  {
    "objectID": "slides/lec-10.html#formulation",
    "href": "slides/lec-10.html#formulation",
    "title": "CISC482 - Lecture10",
    "section": "Formulation",
    "text": "Formulation\n\n\n\nWe want to find the \\(\\beta_0\\) and \\(\\beta_1\\) parameters that reduce the combined error.\nA loss function that takes as input our parameters and whose output is our model error.\n\\(f(\\beta_0, \\beta_1) = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = \\sum\\limits_{i=1}^{n}\\epsilon_i^2\\)\nSum of the Squared Residuals (SSR/SSE). Big or small?\n\n\n\n\n\\[\nSSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\\\\n= [y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2\n\\]"
  },
  {
    "objectID": "slides/lec-10.html#derivation",
    "href": "slides/lec-10.html#derivation",
    "title": "CISC482 - Lecture10",
    "section": "Derivation",
    "text": "Derivation\n\nWill not be going through the derivation\nBasic idea is take the partial derivaitve of the funciton with respect to the parameters and set equal to 0.\nDo a bunch of algebra and you arrive at some nice equations\nFull derivation is here and here"
  },
  {
    "objectID": "slides/lec-10.html#classic-stats-result",
    "href": "slides/lec-10.html#classic-stats-result",
    "title": "CISC482 - Lecture10",
    "section": "Classic Stats Result",
    "text": "Classic Stats Result\n\n\\(\\beta_1 (slope) = \\frac{\\sum\\limits_{i=1}^{n}[(x_i-\\bar{x})(y_i- \\bar{y})]}{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2}\\)\n\\(\\beta_0\\) (intercept) - \\(\\bar{y} - \\beta_1 \\bar{x}\\)\n\n\ndf_r = df[['bill_length_mm', 'body_mass_g']].dropna()\nx = df_r.bill_length_mm\ny = df_r.body_mass_g\nx_bar = x.mean()\ny_bar = y.mean()\nprint(f\"Xbar: {x_bar:.1f}\")\nprint(f\"Ybar: {y_bar:.1f}\")\n\nXbar: 43.9\nYbar: 4201.8"
  },
  {
    "objectID": "slides/lec-10.html#computation",
    "href": "slides/lec-10.html#computation",
    "title": "CISC482 - Lecture10",
    "section": "Computation",
    "text": "Computation\n\nnumerator = np.sum((x - x_bar) * (y - y_bar))\ndenominator = np.sum((x - x_bar)**2)\nslope = numerator / denominator\nintercept = y_bar - slope * x_bar\nprint(f\"Slope: {slope:.1f}; \\nIntercept: {intercept:.1f}\");\n\nSlope: 87.4; \nIntercept: 362.3"
  },
  {
    "objectID": "slides/lec-10.html#another-classic-stats-result",
    "href": "slides/lec-10.html#another-classic-stats-result",
    "title": "CISC482 - Lecture10",
    "section": "Another Classic Stats Result",
    "text": "Another Classic Stats Result\n\\[\n\\begin{aligned}\\hat{\\beta}_1 &= \\frac{\\text{Cov}(x,y)}{s_x^2}\\end{aligned}\n\\]\nThe correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into above, we have\n\\[\n\\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x}\n\\]\n\\(\\beta_0\\) (intercept) - \\(\\bar{y} - \\beta_1 \\bar{x}\\)"
  },
  {
    "objectID": "slides/lec-10.html#computation-1",
    "href": "slides/lec-10.html#computation-1",
    "title": "CISC482 - Lecture10",
    "section": "Computation",
    "text": "Computation\n\nr = df_r.bill_length_mm.corr(df_r.body_mass_g)\nslope = r * (df_r.body_mass_g.std() / df_r.bill_length_mm.std())\nintercept = y_bar - slope * x_bar\nprint(f\"Slope: {slope:.1f}; \\nIntercept: {intercept:.1f}\")\n\nSlope: 87.4; \nIntercept: 362.3"
  },
  {
    "objectID": "slides/lec-10.html#another-full-example",
    "href": "slides/lec-10.html#another-full-example",
    "title": "CISC482 - Lecture10",
    "section": "Another Full Example",
    "text": "Another Full Example"
  },
  {
    "objectID": "slides/lec-10.html#matrix-math-result",
    "href": "slides/lec-10.html#matrix-math-result",
    "title": "CISC482 - Lecture10",
    "section": "Matrix Math Result",
    "text": "Matrix Math Result\n\\[\n\\begin{align*}\nA = \\begin{bmatrix}\nx_1 & 1\\\\\nx_2 & 1 \\\\\n... & ... \\\\\nx_n & 1\n\\end{bmatrix}\n\\qquad\ny = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n\\end{align*}\n\\qquad\n\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix}\n\\]\n\\[\n\\hat{y_1} = x_1 \\cdot \\beta_0 + 1 \\cdot \\beta_1 \\\\\n... \\\\\n\\hat{y} = A \\beta\n\\]\n\n\\[\n\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} = (A^TA)^{-1} A^Ty\n\\]"
  },
  {
    "objectID": "slides/lec-10.html#computation-of-matrix-result",
    "href": "slides/lec-10.html#computation-of-matrix-result",
    "title": "CISC482 - Lecture10",
    "section": "Computation of Matrix Result",
    "text": "Computation of Matrix Result\n\nn = len(df_r)\nprint(x.shape, type(x))\nx = x.values\nprint(x.shape,type(x))\nx = np.expand_dims(x, axis=1) # convert to matrix\nprint(x.shape)\nA = np.append(x, np.ones(shape=(n, 1)), axis=1) # add one column\nprint(A.shape)\ny = y.values\nprint(x[:5, :]) # sample of matrix\n# intercept = y_bar - slope * x_bar\n# print(f\"Slope: {slope:.1f}; \\nIntercept: {intercept:.1f}\")\n\n(342,) <class 'pandas.core.series.Series'>\n(342,) <class 'numpy.ndarray'>\n(342, 1)\n(342, 2)\n[[39.1]\n [39.5]\n [40.3]\n [36.7]\n [39.3]]"
  },
  {
    "objectID": "slides/lec-10.html#computation-of-matrix-result-1",
    "href": "slides/lec-10.html#computation-of-matrix-result-1",
    "title": "CISC482 - Lecture10",
    "section": "Computation of Matrix Result",
    "text": "Computation of Matrix Result\n\nbeta = np.linalg.inv(A.T @ A) @ A.T @ y\nprint(f\"Slope: {beta[0]:.1f}; \\nIntercept: {beta[1]:.1f}\")\n\nSlope: 87.4; \nIntercept: 362.3"
  },
  {
    "objectID": "slides/lec-10.html#class-activity-1",
    "href": "slides/lec-10.html#class-activity-1",
    "title": "CISC482 - Lecture10",
    "section": "Class Activity",
    "text": "Class Activity\nPractice Regression\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-11.html#schedule",
    "href": "slides/lec-11.html#schedule",
    "title": "CISC482 - Lecture11",
    "section": "Schedule",
    "text": "Schedule\n\nTopic Ideas - Should be turned in! Will grade soon!\nReading 5-3: Mar 01 @ 12PM, Wednesday\nHW4 - Mar 08 @ Midnight\nProposal: Mar 22, Wednesday"
  },
  {
    "objectID": "slides/lec-11.html#today",
    "href": "slides/lec-11.html#today",
    "title": "CISC482 - Lecture11",
    "section": "Today",
    "text": "Today\n\nAssumptions of Linear Regression\nMultiple Linear Regression"
  },
  {
    "objectID": "slides/lec-11.html#review-simple-linear-regression",
    "href": "slides/lec-11.html#review-simple-linear-regression",
    "title": "CISC482 - Lecture11",
    "section": "Review Simple Linear Regression",
    "text": "Review Simple Linear Regression\n\n\nWhat is the model for linear regresion: \\(\\hat{y} = ?\\)\nWhat is residual?\nWhat is the loss function to solve for the parameters\n\n\\(L(\\beta_0, \\beta_1) = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2\\)\n\\(L(\\beta_0, \\beta_1) = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2\\)"
  },
  {
    "objectID": "slides/lec-11.html#formulas",
    "href": "slides/lec-11.html#formulas",
    "title": "CISC482 - Lecture11",
    "section": "Formulas?",
    "text": "Formulas?\n\\(\\beta_1 (slope) = \\frac{\\sum\\limits_{i=1}^{n}[(x_i-\\bar{x})(y_i- \\bar{y})]}{\\sum\\limits_{i=1}^{n} (x_i - \\bar{x})^2}\\)\n\\(\\beta_0\\) (intercept) = \\(\\bar{y} - \\beta_1 \\bar{x}\\) \n\n\n\\(\\hat{\\beta}_1 (slope) = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_y}{s_x}\\)\n\\(\\beta_0\\) (intercept) = \\(\\bar{y} - \\beta_1 \\bar{x}\\)"
  },
  {
    "objectID": "slides/lec-11.html#assumptions-of-linear-regression",
    "href": "slides/lec-11.html#assumptions-of-linear-regression",
    "title": "CISC482 - Lecture11",
    "section": "Assumptions of Linear Regression",
    "text": "Assumptions of Linear Regression\n\n\\(x\\) and \\(y\\) have a linear relationship.\nThe residuals of the observations are independent.\nThe mean of the residuals is 0 and the variance of the residuals is constant.\nThe residuals are approximately normally distributed."
  },
  {
    "objectID": "slides/lec-11.html#linear-relationship",
    "href": "slides/lec-11.html#linear-relationship",
    "title": "CISC482 - Lecture11",
    "section": "Linear Relationship?",
    "text": "Linear Relationship?"
  },
  {
    "objectID": "slides/lec-11.html#residual-plot",
    "href": "slides/lec-11.html#residual-plot",
    "title": "CISC482 - Lecture11",
    "section": "Residual Plot",
    "text": "Residual Plot\n\n\nCode\nax = sns.residplot(x=x,y=y)\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Residuals\");"
  },
  {
    "objectID": "slides/lec-11.html#independence-of-error",
    "href": "slides/lec-11.html#independence-of-error",
    "title": "CISC482 - Lecture11",
    "section": "Independence of Error",
    "text": "Independence of Error\n\nThe distances from the regression line to the points (residuals) should generally be random.\nYou do not want to see patterns\nWas the previous plot of residual showing patterns, or was it more or less random?"
  },
  {
    "objectID": "slides/lec-11.html#residual-are-independent",
    "href": "slides/lec-11.html#residual-are-independent",
    "title": "CISC482 - Lecture11",
    "section": "Residual are independent?",
    "text": "Residual are independent?"
  },
  {
    "objectID": "slides/lec-11.html#examples-of-dependence",
    "href": "slides/lec-11.html#examples-of-dependence",
    "title": "CISC482 - Lecture11",
    "section": "Examples of Dependence",
    "text": "Examples of Dependence\n\nTime dependence can often be assessed by analyzing the scatter plot of residuals over time.\nSpatial dependence can often be assessed by analyzing a map of where the data was collected along with further inspection of the residuals for spatial patterns.\nDependencies between observational units must be assessed in context of the study."
  },
  {
    "objectID": "slides/lec-11.html#example---time",
    "href": "slides/lec-11.html#example---time",
    "title": "CISC482 - Lecture11",
    "section": "Example - Time",
    "text": "Example - Time"
  },
  {
    "objectID": "slides/lec-11.html#example---space",
    "href": "slides/lec-11.html#example---space",
    "title": "CISC482 - Lecture11",
    "section": "Example - Space",
    "text": "Example - Space"
  },
  {
    "objectID": "slides/lec-11.html#discussion",
    "href": "slides/lec-11.html#discussion",
    "title": "CISC482 - Lecture11",
    "section": "Discussion",
    "text": "Discussion\n\nIn these graphs we saw time and spatial dependencies in our data set.\nWe saw this by plotting residual plots and looking for any patterns.\nRecall the model was: \\(MPG = m \\cdot Weight + b\\)\nDoes the independence of error assumption hold in this model?\nDoes that mean we should never use this model?"
  },
  {
    "objectID": "slides/lec-11.html#keeping-error-low-and-consistent",
    "href": "slides/lec-11.html#keeping-error-low-and-consistent",
    "title": "CISC482 - Lecture11",
    "section": "Keeping error low and consistent",
    "text": "Keeping error low and consistent\n\nThe residuals of a fitted linear model has a mean of 0. Always.\nA mean of 0 means that on average the predicted value is equal to to the observed value."
  },
  {
    "objectID": "slides/lec-11.html#example-residual-0",
    "href": "slides/lec-11.html#example-residual-0",
    "title": "CISC482 - Lecture11",
    "section": "Example, Residual 0",
    "text": "Example, Residual 0"
  },
  {
    "objectID": "slides/lec-11.html#example-residual-1",
    "href": "slides/lec-11.html#example-residual-1",
    "title": "CISC482 - Lecture11",
    "section": "Example, Residual 1",
    "text": "Example, Residual 1\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nLook at the y-axis scale!"
  },
  {
    "objectID": "slides/lec-11.html#consistent-variance",
    "href": "slides/lec-11.html#consistent-variance",
    "title": "CISC482 - Lecture11",
    "section": "Consistent Variance",
    "text": "Consistent Variance\n\nA linear regression should have a constant variance for all levels of the input.\nThat means the spread of the error should be the same at all levels of input\nLevel of input = x axis changing"
  },
  {
    "objectID": "slides/lec-11.html#example-residual-3",
    "href": "slides/lec-11.html#example-residual-3",
    "title": "CISC482 - Lecture11",
    "section": "Example, Residual 3",
    "text": "Example, Residual 3"
  },
  {
    "objectID": "slides/lec-11.html#example-residual-4",
    "href": "slides/lec-11.html#example-residual-4",
    "title": "CISC482 - Lecture11",
    "section": "Example, Residual 4",
    "text": "Example, Residual 4\n\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\nClearly see a huge jump in variance around 15"
  },
  {
    "objectID": "slides/lec-11.html#example-mpg-prediction",
    "href": "slides/lec-11.html#example-mpg-prediction",
    "title": "CISC482 - Lecture11",
    "section": "Example, MPG Prediction",
    "text": "Example, MPG Prediction"
  },
  {
    "objectID": "slides/lec-11.html#normality-of-errors",
    "href": "slides/lec-11.html#normality-of-errors",
    "title": "CISC482 - Lecture11",
    "section": "Normality of Errors",
    "text": "Normality of Errors\n\n\nAs long as the previous assumptions hold you are going to get a good simple linear regression model\n\n\\(x\\) and \\(y\\) have a linear relationship\nErrors are independent (no spatial, time dependence, or other feature dependence)\nResidual mean of 0, variance constant\n\nHowever, if the errors are also normally distributed, more awesomeness can be done: Interval Estimates"
  },
  {
    "objectID": "slides/lec-11.html#interval-estimates",
    "href": "slides/lec-11.html#interval-estimates",
    "title": "CISC482 - Lecture11",
    "section": "Interval Estimates",
    "text": "Interval Estimates"
  },
  {
    "objectID": "slides/lec-11.html#full-example",
    "href": "slides/lec-11.html#full-example",
    "title": "CISC482 - Lecture11",
    "section": "Full Example",
    "text": "Full Example\n\nUse library numpy and scikit-learn to fit our models\nscikit-learn is library made for machine learning. Its awesome!"
  },
  {
    "objectID": "slides/lec-11.html#creating-data",
    "href": "slides/lec-11.html#creating-data",
    "title": "CISC482 - Lecture11",
    "section": "Creating data",
    "text": "Creating data\n\nn = 250\nx = np.arange(n)\nnoise = np.random.normal(loc=0, scale=5, size=n)\ny = (1.5 * x + 4) + noise\nax = sns.scatterplot(x=x,y=y);\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\");"
  },
  {
    "objectID": "slides/lec-11.html#creating-data-output",
    "href": "slides/lec-11.html#creating-data-output",
    "title": "CISC482 - Lecture11",
    "section": "Creating data",
    "text": "Creating data"
  },
  {
    "objectID": "slides/lec-11.html#fitting-the-data",
    "href": "slides/lec-11.html#fitting-the-data",
    "title": "CISC482 - Lecture11",
    "section": "Fitting the data",
    "text": "Fitting the data\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\nX = x[:, np.newaxis] # n X 1 matrix\nreg = model.fit(X, y) # X needs to be a matrix\n\nslope, intercept  = reg.coef_[0], reg.intercept_\nprint(f\"Slope: {slope:.1f}; \\nIntercept: {intercept:.1f}\")\n\nax = sns.scatterplot(x=x, y=y)\nax.axline((0, intercept), slope=slope, color='r', label='Regressed Line');\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\");"
  },
  {
    "objectID": "slides/lec-11.html#fitting-the-data-output",
    "href": "slides/lec-11.html#fitting-the-data-output",
    "title": "CISC482 - Lecture11",
    "section": "Fitting the data",
    "text": "Fitting the data\n\nSlope: 1.5; \nIntercept: 3.9"
  },
  {
    "objectID": "slides/lec-11.html#residual-plot-1",
    "href": "slides/lec-11.html#residual-plot-1",
    "title": "CISC482 - Lecture11",
    "section": "Residual Plot",
    "text": "Residual Plot\n\nfrom yellowbrick.regressor import ResidualsPlot\nmodel = LinearRegression()\nvisualizer = ResidualsPlot(model)\n\nvisualizer.fit(X, y) \nvisualizer.show();"
  },
  {
    "objectID": "slides/lec-11.html#residual-plot-1-output",
    "href": "slides/lec-11.html#residual-plot-1-output",
    "title": "CISC482 - Lecture11",
    "section": "Residual Plot",
    "text": "Residual Plot"
  },
  {
    "objectID": "slides/lec-11.html#definition",
    "href": "slides/lec-11.html#definition",
    "title": "CISC482 - Lecture11",
    "section": "Definition",
    "text": "Definition\n\nDataset has multiple input features\nIncorporate more than one input feature into a single regression equation -> multiple linear regression\n\\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\\)\n\\(x_1, ..., x_k\\) = input features\n\\(\\hat{y}\\) = predicted feature\n\\(\\beta_0\\) = y-intercept. \\(\\beta_1, ..., \\beta_k\\) = slopes"
  },
  {
    "objectID": "slides/lec-11.html#example",
    "href": "slides/lec-11.html#example",
    "title": "CISC482 - Lecture11",
    "section": "Example",
    "text": "Example\n\npp = sns.pairplot(data=df,\n                  height=7,\n                  y_vars=['body_mass_g'],\n                  x_vars=['bill_length_mm', 'flipper_length_mm'])"
  },
  {
    "objectID": "slides/lec-11.html#example-output",
    "href": "slides/lec-11.html#example-output",
    "title": "CISC482 - Lecture11",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lec-11.html#d-plot",
    "href": "slides/lec-11.html#d-plot",
    "title": "CISC482 - Lecture11",
    "section": "3D plot",
    "text": "3D plot\n\nX = df[['bill_length_mm', 'flipper_length_mm']].values\ny = df['body_mass_g'].values\ndef plot_plane(x_coef, y_coef, intercept, ax):\n  x = np.linspace(X[:, 0].min(), X[:, 0].max(),  n)\n  y = np.linspace(X[:, 1].min(), X[:, 1].max(), n)\n  x, y = np.meshgrid(x, y)\n  eq = x_coef * x + y_coef * y + intercept\n  surface = ax.plot_surface(x, y, eq, color='red', alpha=0.5)\n\nfig = plt.figure(figsize = (10, 7))\nax = plt.axes(projection =\"3d\")\nax.scatter(X[:, 0], X[:, 1], y, label='penguins')\nax.set_xlabel(\"Bill Length\")\nax.set_ylabel(\"Flipper Length\")\nax.set_zlabel(\"Body Mass\")\nax.legend();"
  },
  {
    "objectID": "slides/lec-11.html#d-plot-output",
    "href": "slides/lec-11.html#d-plot-output",
    "title": "CISC482 - Lecture11",
    "section": "3D plot",
    "text": "3D plot"
  },
  {
    "objectID": "slides/lec-11.html#performing-regression",
    "href": "slides/lec-11.html#performing-regression",
    "title": "CISC482 - Lecture11",
    "section": "Performing Regression",
    "text": "Performing Regression\n\nmodel = LinearRegression()\nreg = model.fit(X, y) # X needs to be a matrix\n\nslope_bill, slope_flipper, intercept  = reg.coef_[0], reg.coef_[1], reg.intercept_\nprint(f\"Bill Slope: {slope:.1f}; Flipper Slope: {slope_flipper:.1f}; \\nIntercept: {intercept:.1f}\");\n\nBill Slope: 1.5; Flipper Slope: 48.9; \nIntercept: -5836.3"
  },
  {
    "objectID": "slides/lec-11.html#visualize-plane",
    "href": "slides/lec-11.html#visualize-plane",
    "title": "CISC482 - Lecture11",
    "section": "Visualize Plane",
    "text": "Visualize Plane\n\nfig = plt.figure(figsize = (10, 7))\nax = plt.axes(projection =\"3d\")\nax.scatter(X[:, 0], X[:, 1], y, label='penguins')\nax.set_xlabel(\"Bill Length\")\nax.set_ylabel(\"Flipper Length\")\nax.set_zlabel(\"Body Mass\")\nplot_plane(slope_bill, slope_flipper, intercept, ax=ax)\nax.legend();"
  },
  {
    "objectID": "slides/lec-11.html#visualize-plane-output",
    "href": "slides/lec-11.html#visualize-plane-output",
    "title": "CISC482 - Lecture11",
    "section": "Visualize Plane",
    "text": "Visualize Plane"
  },
  {
    "objectID": "slides/lec-11.html#simple-polynomial-regression",
    "href": "slides/lec-11.html#simple-polynomial-regression",
    "title": "CISC482 - Lecture11",
    "section": "Simple Polynomial Regression",
    "text": "Simple Polynomial Regression\n\nSpecial case of multiple linear regression\nInclude powers of a single features as inputs in the regression equation\nSimple polynomial linear regression\n\\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 .. \\beta_k x^k\\)\nQuadratic: \\(\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)"
  },
  {
    "objectID": "slides/lec-11.html#example-quadratic",
    "href": "slides/lec-11.html#example-quadratic",
    "title": "CISC482 - Lecture11",
    "section": "Example Quadratic",
    "text": "Example Quadratic\n\n\nCode\nn = 30\nx = np.arange(n)\nnoise = np.random.normal(loc=0, scale=5, size=n)\ny = (0.1 * x**2  + 0.3 * x + 4) + noise\nsns.scatterplot(x=x,y=y);"
  },
  {
    "objectID": "slides/lec-11.html#fitting-a-quadratic-numpy",
    "href": "slides/lec-11.html#fitting-a-quadratic-numpy",
    "title": "CISC482 - Lecture11",
    "section": "Fitting a Quadratic (numpy)",
    "text": "Fitting a Quadratic (numpy)\n\ncoeffs = np.polyfit(x, y, deg=2)\nprint(coeffs); # beta_2, beta_1, beta_0\n\n[0.1 0.3 3.0]"
  },
  {
    "objectID": "slides/lec-11.html#plotting-your-regressed-polynomial-function",
    "href": "slides/lec-11.html#plotting-your-regressed-polynomial-function",
    "title": "CISC482 - Lecture11",
    "section": "Plotting your regressed polynomial function",
    "text": "Plotting your regressed polynomial function\n\n\nCode\nregressed_fn = np.poly1d(coeffs)\ny_hat = regressed_fn(x)\nax = sns.scatterplot(x=x,y=y)\nax.plot(x, y_hat, color='red', linestyle='--');"
  },
  {
    "objectID": "slides/lec-11.html#fitting-a-quadratic-scikit-learn",
    "href": "slides/lec-11.html#fitting-a-quadratic-scikit-learn",
    "title": "CISC482 - Lecture11",
    "section": "Fitting a Quadratic (SciKit Learn)",
    "text": "Fitting a Quadratic (SciKit Learn)\n\nScikit-learn can also fit polynomials\nYou must first create the additional columns manually\nIn other words create a second column that is the first column squared!\nHelpful class called PolynomialFeatures"
  },
  {
    "objectID": "slides/lec-11.html#fitting-a-quadratic-scikit-learn-1",
    "href": "slides/lec-11.html#fitting-a-quadratic-scikit-learn-1",
    "title": "CISC482 - Lecture11",
    "section": "Fitting a Quadratic (SciKit Learn)",
    "text": "Fitting a Quadratic (SciKit Learn)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree=2,include_bias=False)\nX = x[:, np.newaxis] # n X 1 matrix\nx_features = pf.fit_transform(X)\nprint(\"Transformed Features:\\n\", x_features[:5, :])\nmodel = LinearRegression()\nreg = model.fit(x_features, y) # X needs to be a matrix\nprint(\"Coefficients\")\nprint(reg.coef_, reg.intercept_); # beta_2, beta_1, beta_0\n\nTransformed Features:\n [[0.0 0.0]\n [1.0 1.0]\n [2.0 4.0]\n [3.0 9.0]\n [4.0 16.0]]\nCoefficients\n[0.3 0.1] 2.965347007117529"
  },
  {
    "objectID": "slides/lec-11.html#plotting-your-regressed-polynomial-function-1",
    "href": "slides/lec-11.html#plotting-your-regressed-polynomial-function-1",
    "title": "CISC482 - Lecture11",
    "section": "Plotting your regressed polynomial function",
    "text": "Plotting your regressed polynomial function\n\n\nCode\nregressed_fn = np.poly1d(coeffs)\ny_hat = reg.predict(x_features)\nax = sns.scatterplot(x=x,y=y)\nax.plot(x, y_hat, color='red', linestyle='--');"
  },
  {
    "objectID": "slides/lec-11.html#class-activity-1",
    "href": "slides/lec-11.html#class-activity-1",
    "title": "CISC482 - Lecture11",
    "section": "Class Activity",
    "text": "Class Activity\nPractice Multiple Linear Regression\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-03.html#schedule",
    "href": "slides/lec-03.html#schedule",
    "title": "CISC482 - Lecture03",
    "section": "Schedule",
    "text": "Schedule\n\nReading 2-1: Jan 25 @ 12PM, Wed\nReading 2-2: Jan 27 @ 12PM, Friday\nReading 2-3: Feb 1 @ 12PM, Wed\nHW2: Feb 1 @ Midnight, Wed"
  },
  {
    "objectID": "slides/lec-03.html#stats-for-data-science",
    "href": "slides/lec-03.html#stats-for-data-science",
    "title": "CISC482 - Lecture03",
    "section": "Stats for data science",
    "text": "Stats for data science\n\nData science relies on statistics to make data driven decisions\nSampling methods are used to efeciently collect data and reduce bias\n\n\n\n\n\n\n\nNote\n\n\nBias - anything that leads to a systematic difference between the true parameters of a population and the statistics used to estimate those parameters. E.g., sampling only men."
  },
  {
    "objectID": "slides/lec-03.html#its-hard",
    "href": "slides/lec-03.html#its-hard",
    "title": "CISC482 - Lecture03",
    "section": "It’s hard…",
    "text": "It’s hard…"
  },
  {
    "objectID": "slides/lec-03.html#sampling-errors",
    "href": "slides/lec-03.html#sampling-errors",
    "title": "CISC482 - Lecture03",
    "section": "Sampling Errors",
    "text": "Sampling Errors\n\nA small, systematic polling error made a big difference\nAll polls were off in the same direction (5 pts) in swing states\nCorrelatated sampling errors in the midwest\nFailure to appreciate uncertainty"
  },
  {
    "objectID": "slides/lec-03.html#stats-for-data-science-1",
    "href": "slides/lec-03.html#stats-for-data-science-1",
    "title": "CISC482 - Lecture03",
    "section": "Stats for data science",
    "text": "Stats for data science\n\n\n\nDescriptive statistics -> explore visualizations\nInferential statistics -> modeling and estimation\nStatisitcs is foundational to ensure results are interpreted correctly\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "slides/lec-03.html#sampling",
    "href": "slides/lec-03.html#sampling",
    "title": "CISC482 - Lecture03",
    "section": "Sampling",
    "text": "Sampling\n\nPopulation - Entire set of individuals, items, or events of interest\nObservational Unit - individual item or event\nSample - subset of observational units from the population"
  },
  {
    "objectID": "slides/lec-03.html#types-of-sampling",
    "href": "slides/lec-03.html#types-of-sampling",
    "title": "CISC482 - Lecture03",
    "section": "Types of Sampling",
    "text": "Types of Sampling\n\n\nRandom Sampling\n\nOU are selected at random\n\nStratified Sampling\n\nPopulation is divided into groups (primary feature). Each group is sampled.\n\nCluster Sampling\n\nPopulation divided into groups (not a primary feature, geography)\n\nSystematic Sampling\n\nEvery k’th observational unit is sampled\n\nConvenience Sampling\n\nOU are selected that are easier"
  },
  {
    "objectID": "slides/lec-03.html#observational-vs-experiement",
    "href": "slides/lec-03.html#observational-vs-experiement",
    "title": "CISC482 - Lecture03",
    "section": "Observational vs Experiement",
    "text": "Observational vs Experiement\n\nObservational\n\nObserving or collecting data\nNot trying to control or influence an outcome\n\nExperimental\n\nYou are controlling a varaible\nYou manipulate that variable to get a different response"
  },
  {
    "objectID": "slides/lec-03.html#descriptive-statistics",
    "href": "slides/lec-03.html#descriptive-statistics",
    "title": "CISC482 - Lecture03",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "slides/lec-03.html#oveview",
    "href": "slides/lec-03.html#oveview",
    "title": "CISC482 - Lecture03",
    "section": "Oveview",
    "text": "Oveview\n\nTerminology\nMeasure of center\nMeasure of spread\nMeasure of position\nMeasure of shape"
  },
  {
    "objectID": "slides/lec-03.html#terminology",
    "href": "slides/lec-03.html#terminology",
    "title": "CISC482 - Lecture03",
    "section": "Terminology",
    "text": "Terminology\n\nDescriptive Statistics - summarize and describe a features important characteristics\ndistribution - the possible values the feature can take\ncluster - a distinct group of neighboring values in a distribution\ntails - the end values of a distributution"
  },
  {
    "objectID": "slides/lec-03.html#terminology---visualized",
    "href": "slides/lec-03.html#terminology---visualized",
    "title": "CISC482 - Lecture03",
    "section": "Terminology - Visualized",
    "text": "Terminology - Visualized"
  },
  {
    "objectID": "slides/lec-03.html#measure-of-center",
    "href": "slides/lec-03.html#measure-of-center",
    "title": "CISC482 - Lecture03",
    "section": "Measure of Center",
    "text": "Measure of Center\n\nMean - average, sum of all values divided by the total number of values, n \\[\n\\frac{1}{n} \\sum_{i=i}^{n} x_{i}\n\\]\nMedian - the middle value of the ordered data"
  },
  {
    "objectID": "slides/lec-03.html#code-example",
    "href": "slides/lec-03.html#code-example",
    "title": "CISC482 - Lecture03",
    "section": "Code Example!",
    "text": "Code Example!\n\nheights = [5.5, 5.7, 5.8, 5.9]\navg_height = sum(heights) / len(heights)\nprint(f\"Average Height: {avg_height:.2f}\")\n\nAverage Height: 5.72\n\n\n\n\nUsing Numpy\n\nimport numpy as np\nheights = np.array([5.5, 5.7, 5.8, 5.9, 6.2])\navg_height = np.average(heights)\nprint(f\"Average Height: {avg_height:.2f}\")\n\nAverage Height: 5.82"
  },
  {
    "objectID": "slides/lec-03.html#what-is-numpy",
    "href": "slides/lec-03.html#what-is-numpy",
    "title": "CISC482 - Lecture03",
    "section": "What is NumPy",
    "text": "What is NumPy\n\nNumPy is the fundamental package for scientific computing in Python.\nPython library that provides a multidimensional array object.\nStores data very effeciently very fast\n\n\na = np.array([1, 2])\n\n\n2D array - rows and columns\nb = np.array([\n  [1, 2], # first row\n  [3, 4]  # second row\n])"
  },
  {
    "objectID": "slides/lec-03.html#question",
    "href": "slides/lec-03.html#question",
    "title": "CISC482 - Lecture03",
    "section": "Question",
    "text": "Question\nnp.average(...) - finds the average\n\n\n\n\n\n\nTip\n\n\nWhat function to get the median?\n\n\n\n\n\nheights = np.array([5.5, 5.7, 5.8, 5.9, 6.2])\nmedian_height = np.median(heights)\nprint(f\"Median Height: {median_height:.2f}\")\n\nMedian Height: 5.80"
  },
  {
    "objectID": "slides/lec-03.html#spread",
    "href": "slides/lec-03.html#spread",
    "title": "CISC482 - Lecture03",
    "section": "Spread",
    "text": "Spread"
  },
  {
    "objectID": "slides/lec-03.html#spread-terminology",
    "href": "slides/lec-03.html#spread-terminology",
    "title": "CISC482 - Lecture03",
    "section": "Spread Terminology",
    "text": "Spread Terminology\n\nrange - distance between the min and max\ninterquartile range (IQR) - range of the middle 50%\nvariance - the average squared distance between a feature and its discribution mean\n\n\\[\n\\sigma^2 = Var(x) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x} )^2}{n-1}\n\\]\n\nStandard deviation - \\(\\sigma = sqrt(Var(X))\\)"
  },
  {
    "objectID": "slides/lec-03.html#postion-terminology-quantiles",
    "href": "slides/lec-03.html#postion-terminology-quantiles",
    "title": "CISC482 - Lecture03",
    "section": "Postion Terminology (Quantiles)",
    "text": "Postion Terminology (Quantiles)"
  },
  {
    "objectID": "slides/lec-03.html#example",
    "href": "slides/lec-03.html#example",
    "title": "CISC482 - Lecture03",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/lec-03.html#measures-of-shape",
    "href": "slides/lec-03.html#measures-of-shape",
    "title": "CISC482 - Lecture03",
    "section": "Measures of Shape",
    "text": "Measures of Shape\n\n\nSkewness - measure of the amount and direction of skew\nKurtosis - measure of tail heaviness"
  },
  {
    "objectID": "slides/lec-03.html#mean",
    "href": "slides/lec-03.html#mean",
    "title": "CISC482 - Lecture03",
    "section": "Mean",
    "text": "Mean\n\n                            # mean,  std,    n\nsamples = np.random.normal(loc=5.0, scale=1, size=11)\nsamples\n\narray([6.624, 4.388, 4.472, 3.927, 5.865, 2.698, 6.745, 4.239, 5.319,\n       4.751, 6.462])\n\n\nHow would I get the mean?\n\n\nprint(np.mean(samples))\n\n5.044609002634907"
  },
  {
    "objectID": "slides/lec-03.html#standard-deviation",
    "href": "slides/lec-03.html#standard-deviation",
    "title": "CISC482 - Lecture03",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nstd = np.std(samples)\nprint(f\"The standard deviation is: {std:.2f}\")\n\nThe standard deviation is: 1.22\n\n\n\n\nThe variance?\n\n\n\nvar = std * std\nprint(f\"The variance is: {var:.2f}\")\n\nThe variance is: 1.49"
  },
  {
    "objectID": "slides/lec-03.html#quantiles",
    "href": "slides/lec-03.html#quantiles",
    "title": "CISC482 - Lecture03",
    "section": "Quantiles",
    "text": "Quantiles\n\n\nsamples_sorted = np.sort(samples)\nprint(samples_sorted)\n\n[2.698 3.927 4.239 4.388 4.472 4.751 5.319 5.865 6.462 6.624 6.745]\n\n\n\n\nGetting the quantile\n\n\n\nfifty_percent_quantile = np.quantile(samples, 0.50)\nmedian = np.median(samples)\n\nprint(f\"{fifty_percent_quantile:.3f}\")\nprint(f\"{median:.3f}\")\n\n4.751\n4.751\n\n\n\n\n\nquantiles = np.quantile(samples, [0.25, 0.5, 0.75])\n\nprint(quantiles)\n\n[4.314 4.751 6.164]"
  },
  {
    "objectID": "slides/lec-03.html#shape",
    "href": "slides/lec-03.html#shape",
    "title": "CISC482 - Lecture03",
    "section": "Shape",
    "text": "Shape\nAdvanced statistics -> I recommend using the library scipy. This library is built on top of numpy but has more functionality.\n\n\nfrom scipy.stats import skewnorm\nsamples_skewed = skewnorm.rvs(3, loc=15, scale=2, size=1000)\nsamples = skewnorm.rvs(0, loc=5, scale=1, size=1000)\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fc718f8e260>"
  },
  {
    "objectID": "slides/lec-03.html#skewness-and-kurtosis",
    "href": "slides/lec-03.html#skewness-and-kurtosis",
    "title": "CISC482 - Lecture03",
    "section": "Skewness and Kurtosis",
    "text": "Skewness and Kurtosis\nCalculate the skewness\n\n\nimport scipy.stats as stats\nprint(f\"Skewed sample data set, skewnewss is: {stats.skew(samples_skewed):.3f}\")\nprint(f\"Normal sample data set, skewnewss is: {stats.skew(samples):.3f}\")\n\nSkewed sample data set, skewnewss is: 0.790\nNormal sample data set, skewnewss is: -0.003\n\n\n\n\nCalculate the kurtosis\n\nimport scipy.stats as stats\nprint(f\"Skewed sample data set, kurtosis is: {stats.kurtosis(samples_skewed):.3f}\")\nprint(f\"Normal sample data set, kurtosis is: {stats.kurtosis(samples):.3f}\")\n\nSkewed sample data set, kurtosis is: 0.938\nNormal sample data set, kurtosis is: -0.067\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThis skewed distribution is not very good at demonstrating kurtosis."
  },
  {
    "objectID": "slides/lec-03.html#all",
    "href": "slides/lec-03.html#all",
    "title": "CISC482 - Lecture03",
    "section": "All",
    "text": "All\n\nimport scipy.stats as stats\nstats.describe(samples)\n\nDescribeResult(nobs=1000, minmax=(1.7616568032476234, 7.787361447950662), mean=5.053665979218166, variance=1.0126662448100565, skewness=-0.0026145821525120016, kurtosis=-0.06654059199852824)\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-21.html#schedule",
    "href": "slides/lec-21.html#schedule",
    "title": "CISC482 - Lecture21",
    "section": "Schedule",
    "text": "Schedule\n\nReading 8-2: April 14 @ 12PM, Friday\nProject Draft Report: April 12 @ Midnight, Wednesday\nExample Report on Brightspace!!"
  },
  {
    "objectID": "slides/lec-21.html#today",
    "href": "slides/lec-21.html#today",
    "title": "CISC482 - Lecture21",
    "section": "Today",
    "text": "Today\n\nReview SVM\nUnsupervised Learning\nK-Means Clustering"
  },
  {
    "objectID": "slides/lec-21.html#terms",
    "href": "slides/lec-21.html#terms",
    "title": "CISC482 - Lecture21",
    "section": "Terms",
    "text": "Terms\n\nSupport Vector Machine (SVM) is a supervised learning algorithm that uses ________ to divide data into different classes.\n\n\n\nIn a two-dimensional feature space, a hyperplane is a _____\nIn a three-dimensional feature space, a hyperplane is a ____"
  },
  {
    "objectID": "slides/lec-21.html#hyperplane-1",
    "href": "slides/lec-21.html#hyperplane-1",
    "title": "CISC482 - Lecture21",
    "section": "Hyperplane 1?",
    "text": "Hyperplane 1?"
  },
  {
    "objectID": "slides/lec-21.html#hyperplane-2",
    "href": "slides/lec-21.html#hyperplane-2",
    "title": "CISC482 - Lecture21",
    "section": "Hyperplane 2?",
    "text": "Hyperplane 2?"
  },
  {
    "objectID": "slides/lec-21.html#separating-classes-with-planes-optimal",
    "href": "slides/lec-21.html#separating-classes-with-planes-optimal",
    "title": "CISC482 - Lecture21",
    "section": "Separating Classes with Planes (Optimal)",
    "text": "Separating Classes with Planes (Optimal)"
  },
  {
    "objectID": "slides/lec-21.html#visual-example-of-terms",
    "href": "slides/lec-21.html#visual-example-of-terms",
    "title": "CISC482 - Lecture21",
    "section": "Visual Example of Terms",
    "text": "Visual Example of Terms\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=[[-2, 5], [0, 0]], cluster_std=1)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10.)\nmodel.fit(X, Y)\n\nplot_svm(X, Y, model, ax=ax, plot_df=False);\n# ax.legend([\"Class 0\", \"Class 1\"])"
  },
  {
    "objectID": "slides/lec-21.html#multiclass",
    "href": "slides/lec-21.html#multiclass",
    "title": "CISC482 - Lecture21",
    "section": "Multiclass",
    "text": "Multiclass\n\n\nWhat do you do if you have a multiple classes. For example, A, B, and C classes?\n\nA vs (B,C)\nB vs (A,C)\nC vs (A,B)"
  },
  {
    "objectID": "slides/lec-21.html#one-vs-rest-ovr",
    "href": "slides/lec-21.html#one-vs-rest-ovr",
    "title": "CISC482 - Lecture21",
    "section": "One Vs Rest (OVR)",
    "text": "One Vs Rest (OVR)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=False, plot_sv=False, plot_hp=False, plot_margin=False, classes=['A', 'B', 'C']);"
  },
  {
    "objectID": "slides/lec-21.html#one-vs-rest-ovr-1",
    "href": "slides/lec-21.html#one-vs-rest-ovr-1",
    "title": "CISC482 - Lecture21",
    "section": "One Vs Rest (OVR)",
    "text": "One Vs Rest (OVR)\n\n\nCode\nfrom sklearn import svm\nX, Y = make_blobs(random_state=42, n_samples=100, n_features=2, centers=3)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=False, plot_margin=False, classes=['A', 'B', 'C']);"
  },
  {
    "objectID": "slides/lec-21.html#kernels",
    "href": "slides/lec-21.html#kernels",
    "title": "CISC482 - Lecture21",
    "section": "Kernels",
    "text": "Kernels\n\n\nWhat is the input space\n\n\\(X\\)\n\nWhat is your feature space\n\n\\(\\phi(X)\\)\n\nWhat do we have feature spaces?"
  },
  {
    "objectID": "slides/lec-21.html#kernel-trick",
    "href": "slides/lec-21.html#kernel-trick",
    "title": "CISC482 - Lecture21",
    "section": "Kernel Trick",
    "text": "Kernel Trick\n\n\nWhat mathematical operation do we use judge similarity between points?\n\nInner Product! (Dot Product)\n\nWhat is the kernel trick?\n\nAllows you to compute the product of points in a higher dimensional feature space, while actually still remaining in the input space."
  },
  {
    "objectID": "slides/lec-21.html#svm-kernel-example",
    "href": "slides/lec-21.html#svm-kernel-example",
    "title": "CISC482 - Lecture21",
    "section": "SVM Kernel Example",
    "text": "SVM Kernel Example"
  },
  {
    "objectID": "slides/lec-21.html#svm-with-linear-kernel",
    "href": "slides/lec-21.html#svm-with-linear-kernel",
    "title": "CISC482 - Lecture21",
    "section": "SVM With Linear Kernel",
    "text": "SVM With Linear Kernel\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"linear\", C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=True, plot_margin=True, classes=['A', 'B']);"
  },
  {
    "objectID": "slides/lec-21.html#svm-with-polynomial-kernel",
    "href": "slides/lec-21.html#svm-with-polynomial-kernel",
    "title": "CISC482 - Lecture21",
    "section": "SVM With Polynomial Kernel",
    "text": "SVM With Polynomial Kernel\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=1)\nmodel = svm.SVC(kernel=\"poly\", degree=2, C=10)\nmodel.fit(X, Y)\nplot_svm(X, Y, model, ax=ax, plot_df=True, plot_sv=True, plot_hp=False, plot_margin=False, classes=['A', 'B']);"
  },
  {
    "objectID": "slides/lec-21.html#terms-1",
    "href": "slides/lec-21.html#terms-1",
    "title": "CISC482 - Lecture21",
    "section": "Terms",
    "text": "Terms\n\nUnsupervised learning uses machine learning techniques to identify patterns in data without any prior knowledge about the data.\nThis means we have NO labels. Its like we have a bunch of penguin data, but it is not labeled!"
  },
  {
    "objectID": "slides/lec-21.html#visual",
    "href": "slides/lec-21.html#visual",
    "title": "CISC482 - Lecture21",
    "section": "Visual",
    "text": "Visual"
  },
  {
    "objectID": "slides/lec-21.html#examples-algs",
    "href": "slides/lec-21.html#examples-algs",
    "title": "CISC482 - Lecture21",
    "section": "Examples Algs",
    "text": "Examples Algs\n\n\n\n\n\n\n\nLearning algorithm\nExample\n\n\n\n\nA clustering algorithm groups observations with similar features.\nHierarchical clustering, k-means clustering\n\n\nAn outlier detection algorithm identifies deviations within data.\nDBSCAN, local outlier factor\n\n\nA latent variable model relates observable variables to a set of latent or unobservable variables.\nExpectation maximization, principal component analysis"
  },
  {
    "objectID": "slides/lec-21.html#all-clustering-algs",
    "href": "slides/lec-21.html#all-clustering-algs",
    "title": "CISC482 - Lecture21",
    "section": "All clustering algs",
    "text": "All clustering algs"
  },
  {
    "objectID": "slides/lec-21.html#terms-2",
    "href": "slides/lec-21.html#terms-2",
    "title": "CISC482 - Lecture21",
    "section": "Terms",
    "text": "Terms\n\nCluster is a set of samples with similar characteristics.\nGrouping samples into classes with similar characteristics is called clustering.\nA natural way of quantifying similarity is by picking a centroid for each cluster and finding the distance between the sample and the centroid.\nCentroid is a point that represents the center of each cluster. Samples that are closer to a cluster’s centroid are said to be similar to each other and considered part of the cluster."
  },
  {
    "objectID": "slides/lec-21.html#visual-example-1",
    "href": "slides/lec-21.html#visual-example-1",
    "title": "CISC482 - Lecture21",
    "section": "Visual Example 1",
    "text": "Visual Example 1"
  },
  {
    "objectID": "slides/lec-21.html#visual-example-2",
    "href": "slides/lec-21.html#visual-example-2",
    "title": "CISC482 - Lecture21",
    "section": "Visual Example 2",
    "text": "Visual Example 2"
  },
  {
    "objectID": "slides/lec-21.html#visual-example-3",
    "href": "slides/lec-21.html#visual-example-3",
    "title": "CISC482 - Lecture21",
    "section": "Visual Example 3",
    "text": "Visual Example 3"
  },
  {
    "objectID": "slides/lec-21.html#k-means-algorithm",
    "href": "slides/lec-21.html#k-means-algorithm",
    "title": "CISC482 - Lecture21",
    "section": "K-means Algorithm",
    "text": "K-means Algorithm\nStep 0: Select the number of clusters, \\(k\\).\nStep 1: Randomly initialize samples as cluster centroids.\nStep 2: For each sample, calculate the distance between that sample and each cluster’s centroid, and assign the sample to the cluster with the closest centroid.\nStep 3: For each cluster, calculate the mean of all samples in the cluster. This mean becomes the new centroid.\nStep 4: Repeat steps 2 and 3 until a stopping criterion is met. Such as reaching a certain number of iterations or the centroids staying the same."
  },
  {
    "objectID": "slides/lec-21.html#step-0",
    "href": "slides/lec-21.html#step-0",
    "title": "CISC482 - Lecture21",
    "section": "Step 0",
    "text": "Step 0\nSelect the number of clusters, \\(k\\). This is hard if you dont have any prior information! However we have some tricks we can discuss later."
  },
  {
    "objectID": "slides/lec-21.html#step-2",
    "href": "slides/lec-21.html#step-2",
    "title": "CISC482 - Lecture21",
    "section": "Step 2",
    "text": "Step 2\nFor each sample, calculate the distance between that sample and each cluster’s centroid, and assign the sample to the cluster with the closest centroid\n\\(d = \\sqrt {\\left( {x_1 - x_2 } \\right)^2 + \\left( {y_1 - y_2 } \\right)^2 }\\)\nIf n=100, how many distance calculation will we have?"
  },
  {
    "objectID": "slides/lec-21.html#step-3",
    "href": "slides/lec-21.html#step-3",
    "title": "CISC482 - Lecture21",
    "section": "Step 3",
    "text": "Step 3\nFor each cluster, calculate the mean of all samples in the cluster. This mean becomes the new centroid.\nThis replaces the random selection you had in the beginning!"
  },
  {
    "objectID": "slides/lec-21.html#step-4",
    "href": "slides/lec-21.html#step-4",
    "title": "CISC482 - Lecture21",
    "section": "Step 4",
    "text": "Step 4\nRepeat steps 2 and 3 until a stopping criterion is met. Such as reaching a certain number of iterations or the centroids staying the same."
  },
  {
    "objectID": "slides/lec-21.html#k-means-animation",
    "href": "slides/lec-21.html#k-means-animation",
    "title": "CISC482 - Lecture21",
    "section": "K-means Animation",
    "text": "K-means Animation"
  },
  {
    "objectID": "slides/lec-21.html#choosing-k",
    "href": "slides/lec-21.html#choosing-k",
    "title": "CISC482 - Lecture21",
    "section": "Choosing K",
    "text": "Choosing K\n\n\nThe number of clusters is not often obvious, especially if the data has more than two features.\nThe elbow method is the most common technique to determine the optimal number of clusters for the data.\nThe intuition is that good groups should be close together.\nHow can we measure how close things are together?\n\nThe sum of squared distanced between all samples and their centroid\nwithin-cluster sum of squares - WCSS"
  },
  {
    "objectID": "slides/lec-21.html#sweet-spot",
    "href": "slides/lec-21.html#sweet-spot",
    "title": "CISC482 - Lecture21",
    "section": "Sweet Spot",
    "text": "Sweet Spot\n\n\nThink of this example with \\(n\\) data points\nWhen you have 1 group things are very spread far apart!\n\nThats the the sum of squared distances between of all points to the center\n\nWhen you have n groups then things are really close together!\n\nEach sample is its own group and has NO distance from it center!\n\nYou want the sweet spot, where when you increase the number of groups (k) and the WCSS drops alot."
  },
  {
    "objectID": "slides/lec-21.html#elbow",
    "href": "slides/lec-21.html#elbow",
    "title": "CISC482 - Lecture21",
    "section": "Elbow",
    "text": "Elbow\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-16.html#schedule",
    "href": "slides/lec-16.html#schedule",
    "title": "CISC482 - Lecture16",
    "section": "Schedule",
    "text": "Schedule\n\nReading 6-3: Mar 22 @ 12PM, Wednesday\nProposal: Mar 22, Wednesday\nPropsal Template!\nHW5 - Mar 29 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-16.html#today",
    "href": "slides/lec-16.html#today",
    "title": "CISC482 - Lecture16",
    "section": "Today",
    "text": "Today\n\nReview Train/Valid/Test\nReview K-Folds"
  },
  {
    "objectID": "slides/lec-16.html#purpose-of-model-evaluation",
    "href": "slides/lec-16.html#purpose-of-model-evaluation",
    "title": "CISC482 - Lecture16",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\), \\(recall\\), etc. tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation,\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/lec-16.html#splitting-data",
    "href": "slides/lec-16.html#splitting-data",
    "title": "CISC482 - Lecture16",
    "section": "Splitting Data",
    "text": "Splitting Data\n\n\nTrain/Test/Valid\n\nTrain (70% of data) - Used to fit a model or multiple models\nValidation (15% of data) - Used to compare different models\nTest (15% of data) - Final model evaluation"
  },
  {
    "objectID": "slides/lec-16.html#trainvalid-split",
    "href": "slides/lec-16.html#trainvalid-split",
    "title": "CISC482 - Lecture16",
    "section": "Train/Valid Split",
    "text": "Train/Valid Split\n\n\nTRAIN SET - Linear Model R^2 = 0.44; Polynomial Model R^2 = 0.62\nVALID SET - Linear Model R^2 = 0.65; Polynomial Model R^2 = 0.41"
  },
  {
    "objectID": "slides/lec-16.html#k-folds-cross-validation",
    "href": "slides/lec-16.html#k-folds-cross-validation",
    "title": "CISC482 - Lecture16",
    "section": "K-folds Cross Validation",
    "text": "K-folds Cross Validation\n\n\nSplit data into Train(80%)/Test(20%)\nBreak the Train data into k groups.\nTrain/Validate across the goups"
  },
  {
    "objectID": "slides/lec-16.html#k-folds-split-k10",
    "href": "slides/lec-16.html#k-folds-split-k10",
    "title": "CISC482 - Lecture16",
    "section": "K-Folds Split (k=10)",
    "text": "K-Folds Split (k=10)"
  },
  {
    "objectID": "slides/lec-16.html#k-folds-train-and-validate",
    "href": "slides/lec-16.html#k-folds-train-and-validate",
    "title": "CISC482 - Lecture16",
    "section": "K-Folds Train and Validate",
    "text": "K-Folds Train and Validate"
  },
  {
    "objectID": "slides/lec-16.html#what-is-it",
    "href": "slides/lec-16.html#what-is-it",
    "title": "CISC482 - Lecture16",
    "section": "What is it?",
    "text": "What is it?\n\nBootstrapping is the process of generating simulated samples by repeatedly drawing with replacement from an existing sample.\nBootstrap samples are often used to evaluate a statistic’s ability to estimate a parameter.\nThis process can give you a distribution of estimates!"
  },
  {
    "objectID": "slides/lec-16.html#bootstrapping-starts-with-a-single-sample",
    "href": "slides/lec-16.html#bootstrapping-starts-with-a-single-sample",
    "title": "CISC482 - Lecture16",
    "section": "Bootstrapping starts with a single sample",
    "text": "Bootstrapping starts with a single sample\n\nPopulation may have an unknown n\nWe have one sample from the population. How many observations do we have in this sample?"
  },
  {
    "objectID": "slides/lec-16.html#repeated-sampling-with-replacement",
    "href": "slides/lec-16.html#repeated-sampling-with-replacement",
    "title": "CISC482 - Lecture16",
    "section": "Repeated sampling with replacement",
    "text": "Repeated sampling with replacement"
  },
  {
    "objectID": "slides/lec-16.html#example---flipper-length-vs-body-mass",
    "href": "slides/lec-16.html#example---flipper-length-vs-body-mass",
    "title": "CISC482 - Lecture16",
    "section": "Example - Flipper Length vs Body Mass",
    "text": "Example - Flipper Length vs Body Mass\n\n\n\n\nCode\ndf = df[['flipper_length_mm', 'body_mass_g']]\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      flipper_length_mm\n      body_mass_g\n    \n  \n  \n    \n      0\n      181.00\n      3,750.00\n    \n    \n      1\n      186.00\n      3,800.00\n    \n    \n      2\n      195.00\n      3,250.00\n    \n    \n      4\n      193.00\n      3,450.00\n    \n    \n      5\n      190.00\n      3,650.00\n    \n  \n\n\n\n\n\n\n\nCode\nsns.regplot(data=df, x='flipper_length_mm', y='body_mass_g', ci=None);"
  },
  {
    "objectID": "slides/lec-16.html#bootrapping-1",
    "href": "slides/lec-16.html#bootrapping-1",
    "title": "CISC482 - Lecture16",
    "section": "Bootrapping",
    "text": "Bootrapping\n\n\nCode\nn_boots = 100 # number of bootrap iterations\nn_points = int(len(df) * 0.50) # sample size with replacement\nboot_slopes = [] # store regressed line slopes\nboot_intercepts = [] # store regressed line intercepts\nplt.figure() # creates a figure that we can plot *multiple* times\nlinear_model = LinearRegression()\nfor _ in range(n_boots):\n # sample the rows, same size, with replacement\n sample_df = df.sample(n=n_points, replace=True)\n # fit a linear regression\n linear_model.fit(sample_df[['flipper_length_mm']], sample_df['body_mass_g'])\n # append regressed coefficients\n boot_intercepts.append(linear_model.intercept_)\n boot_slopes.append(linear_model.coef_[0])\n # plot a greyed out line of the prediction\n y_pred_temp = linear_model.predict(sample_df[['flipper_length_mm']])\n plt.plot(sample_df['flipper_length_mm'], y_pred_temp, color='grey', alpha=0.2)# add data points\n\nplt.scatter(sample_df['flipper_length_mm'], sample_df['body_mass_g'])\nplt.grid(True)\nplt.xlabel('Flipper Length')\nplt.ylabel('Body Mass')\nplt.title(f'Bootrapping; Bootstrap interations={n_boots}; Sample Size:{n_points}')\nplt.show();\n\n\n\n\n\n\nHow consistent are the predictions for different samples?"
  },
  {
    "objectID": "slides/lec-16.html#historgram-of-model-parameters",
    "href": "slides/lec-16.html#historgram-of-model-parameters",
    "title": "CISC482 - Lecture16",
    "section": "Historgram of Model Parameters!",
    "text": "Historgram of Model Parameters!\n\n\n\n\nCode\ndf_p = pd.DataFrame(dict(slope=boot_slopes, intercept=boot_intercepts))\nsns.displot(data=df_p, x='slope');\n\n\n\n\n\n\n\n\nCode\nsns.displot(data=df_p, x='intercept');"
  },
  {
    "objectID": "slides/lec-16.html#inferential-statisitcs",
    "href": "slides/lec-16.html#inferential-statisitcs",
    "title": "CISC482 - Lecture16",
    "section": "Inferential Statisitcs",
    "text": "Inferential Statisitcs\n\nSince we have a distribution of these popluation parameters we can calculate their mean and variance\n\nThe mean will be the best estimator\nThe variance can be used to calcluate our confidence in the result\n\nHow do we calculate confidence interval?\n\n\\(\\bar{x} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides/lec-16.html#one-standard-error-method",
    "href": "slides/lec-16.html#one-standard-error-method",
    "title": "CISC482 - Lecture16",
    "section": "One Standard Error Method",
    "text": "One Standard Error Method\n\n\nSelect a few different models you want to try out (linear regression, polynomial regressions, multivariable regression, etc.)\nPerform K-folds cross validation on each of these models.\nEach model will have a distribution for error, mean and variance.\nFind the model with the minimum mean score\nThen select the simplest model whose mean score falls within one standard deviation."
  },
  {
    "objectID": "slides/lec-16.html#example-one",
    "href": "slides/lec-16.html#example-one",
    "title": "CISC482 - Lecture16",
    "section": "Example One",
    "text": "Example One"
  },
  {
    "objectID": "slides/lec-16.html#example-two",
    "href": "slides/lec-16.html#example-two",
    "title": "CISC482 - Lecture16",
    "section": "Example Two",
    "text": "Example Two"
  },
  {
    "objectID": "slides/lec-16.html#ask-your-questions",
    "href": "slides/lec-16.html#ask-your-questions",
    "title": "CISC482 - Lecture16",
    "section": "Ask your Questions!",
    "text": "Ask your Questions!\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-23.html#schedule",
    "href": "slides/lec-23.html#schedule",
    "title": "CISC482 - Lecture23",
    "section": "Schedule",
    "text": "Schedule\n\nReturning Draft Reports\nFinal Report - April 28 @ Midnight\nFinal Presentation - May 5, Final"
  },
  {
    "objectID": "slides/lec-23.html#today",
    "href": "slides/lec-23.html#today",
    "title": "CISC482 - Lecture23",
    "section": "Today",
    "text": "Today\n\nRecap Peer Review\nHierarchal Clustering"
  },
  {
    "objectID": "slides/lec-23.html#groups",
    "href": "slides/lec-23.html#groups",
    "title": "CISC482 - Lecture23",
    "section": "Groups",
    "text": "Groups\nYou should have been e-mailed view only access to their drafts\n\nSupriya -> Review Jaydon and Carter\nAnmol -> Review Carter and Iana\nJaydon -> Review Iana and Chhandak\nCarter -> Review Prashant and Haven\nIana -> Review Haven and Chhandak\nHaven -> Review Prashant and Shivay\nChhandak -> Review Shivay and Supriya\nPrashant -> Review Supriya and Anmol\nShivay -> Review Anmol and Jaydon\n\n\n\n\n\n\n\nTip\n\n\nHow did it go?"
  },
  {
    "objectID": "slides/lec-23.html#questions-1",
    "href": "slides/lec-23.html#questions-1",
    "title": "CISC482 - Lecture23",
    "section": "Questions 1",
    "text": "Questions 1\n\n\nWhat did you learn from reviewing your peers’ work? Were there any insights or perspectives that you gained from seeing other students’ writing or ideas?\nHow did the peer review process impact your own writing or thinking about the assignment? Did you make any changes to your work based on the feedback you received from your peers?"
  },
  {
    "objectID": "slides/lec-23.html#questions-2",
    "href": "slides/lec-23.html#questions-2",
    "title": "CISC482 - Lecture23",
    "section": "Questions 2",
    "text": "Questions 2\n\n\nLooking forward, what advice would you give to someone who is new to peer review? What are some best practices or strategies that you would recommend for someone who wants to give or receive feedback effectively?\nWhat feedback did you receive from your peers? Was it helpful, and if so, why? Did any feedback surprise you, or challenge your assumptions about your own work?"
  },
  {
    "objectID": "slides/lec-23.html#what-is-it",
    "href": "slides/lec-23.html#what-is-it",
    "title": "CISC482 - Lecture23",
    "section": "What is it?",
    "text": "What is it?\n\n\nVideo"
  },
  {
    "objectID": "slides/lec-23.html#types-of-clustering",
    "href": "slides/lec-23.html#types-of-clustering",
    "title": "CISC482 - Lecture23",
    "section": "Types of Clustering",
    "text": "Types of Clustering\n\nAgglomerative hierarchical clustering is a clustering method where each sample is treated as an individual cluster\n\nIf we have 100 samples, we start with 100 clusters!\nTwo clusters are combined iteratively until all samples belong to a single cluster\n\nDivisive hierarchical clustering\n\nStart with one and split\n\n\n\n\n\n\n\n\nTip\n\n\nUse agglomerative! It allows us to observe local patterns first before creating groups. It gives really great results!"
  },
  {
    "objectID": "slides/lec-23.html#measures-of-similarity",
    "href": "slides/lec-23.html#measures-of-similarity",
    "title": "CISC482 - Lecture23",
    "section": "Measures of Similarity",
    "text": "Measures of Similarity\n\nThe single linkage method calculates the distance between a pair of samples, one from each cluster, that are the most similar.\nThe complete linkage method calculates the distance between a pair of samples, one from each cluster, that are the most different.\nThe centroid linkage method calculates the distance between the centroids of two clusters."
  },
  {
    "objectID": "slides/lec-23.html#single-linkage",
    "href": "slides/lec-23.html#single-linkage",
    "title": "CISC482 - Lecture23",
    "section": "Single Linkage",
    "text": "Single Linkage"
  },
  {
    "objectID": "slides/lec-23.html#complete-linkage",
    "href": "slides/lec-23.html#complete-linkage",
    "title": "CISC482 - Lecture23",
    "section": "Complete Linkage",
    "text": "Complete Linkage"
  },
  {
    "objectID": "slides/lec-23.html#centroid-linkage",
    "href": "slides/lec-23.html#centroid-linkage",
    "title": "CISC482 - Lecture23",
    "section": "Centroid Linkage",
    "text": "Centroid Linkage"
  },
  {
    "objectID": "slides/lec-23.html#questions---1",
    "href": "slides/lec-23.html#questions---1",
    "title": "CISC482 - Lecture23",
    "section": "Questions - 1",
    "text": "Questions - 1\n\n\nWhich two samples should be used to determine similarity using the Single linkage? Complete Linkage?"
  },
  {
    "objectID": "slides/lec-23.html#questions---1-1",
    "href": "slides/lec-23.html#questions---1-1",
    "title": "CISC482 - Lecture23",
    "section": "Questions - 1",
    "text": "Questions - 1\n\n\nWhich two samples should be used to determine similarity using the Single linkage? Complete Linkage?"
  },
  {
    "objectID": "slides/lec-23.html#questions---1-2",
    "href": "slides/lec-23.html#questions---1-2",
    "title": "CISC482 - Lecture23",
    "section": "Questions - 1",
    "text": "Questions - 1\n\n\n\n\nHow many total samples?\nHow many total clusters?\nWhat is the Euclidean distance between the clusters using complete linkage?"
  },
  {
    "objectID": "slides/lec-23.html#terminology",
    "href": "slides/lec-23.html#terminology",
    "title": "CISC482 - Lecture23",
    "section": "Terminology",
    "text": "Terminology\n\nThe output of a hierarchical clustering algorithm can be visualized using a dendrogram.\nA dendrogram is a tree that shows the order in which clusters are grouped together and the distances between clusters.\nRead it from BOTTOM up!"
  },
  {
    "objectID": "slides/lec-23.html#section",
    "href": "slides/lec-23.html#section",
    "title": "CISC482 - Lecture23",
    "section": "",
    "text": "A clade is a branch of a dendrogram/vertical line.\nA link is a horizontal line that connects two clades, height gives the distance between clusters.\nA leaf is the terminal end of each clade in a dendrogram, which represents a single sample.\n\n\n\n\n\n\nHow many total samples?\nHow many total clusters in the beggining?\nWhich clusters get grouped 1st? 2nd? 3rd? 4th?"
  },
  {
    "objectID": "slides/lec-23.html#visual-1",
    "href": "slides/lec-23.html#visual-1",
    "title": "CISC482 - Lecture23",
    "section": "Visual 1",
    "text": "Visual 1"
  },
  {
    "objectID": "slides/lec-23.html#visual-2",
    "href": "slides/lec-23.html#visual-2",
    "title": "CISC482 - Lecture23",
    "section": "Visual 2",
    "text": "Visual 2"
  },
  {
    "objectID": "slides/lec-23.html#visual-3",
    "href": "slides/lec-23.html#visual-3",
    "title": "CISC482 - Lecture23",
    "section": "Visual 3",
    "text": "Visual 3"
  },
  {
    "objectID": "slides/lec-23.html#visual-4",
    "href": "slides/lec-23.html#visual-4",
    "title": "CISC482 - Lecture23",
    "section": "Visual 4",
    "text": "Visual 4"
  },
  {
    "objectID": "slides/lec-23.html#visual-5",
    "href": "slides/lec-23.html#visual-5",
    "title": "CISC482 - Lecture23",
    "section": "Visual 5",
    "text": "Visual 5"
  },
  {
    "objectID": "slides/lec-23.html#visual-6",
    "href": "slides/lec-23.html#visual-6",
    "title": "CISC482 - Lecture23",
    "section": "Visual 6",
    "text": "Visual 6"
  },
  {
    "objectID": "slides/lec-23.html#visual-7",
    "href": "slides/lec-23.html#visual-7",
    "title": "CISC482 - Lecture23",
    "section": "Visual 7",
    "text": "Visual 7"
  },
  {
    "objectID": "slides/lec-23.html#thresholding",
    "href": "slides/lec-23.html#thresholding",
    "title": "CISC482 - Lecture23",
    "section": "Thresholding",
    "text": "Thresholding\n\nA dendrogram can be used as a starting point to find the optimal number of clusters.\nNo conclusions about the optimal number of clusters should be made without using more quantitative techniques like the elbow method.\nWe can specity a maximum distance between groups.\nAny clusters below that distance should be clustered\nAny clusters above that distance should not be clustered\nWe call this distance a threshold"
  },
  {
    "objectID": "slides/lec-23.html#threshold-1",
    "href": "slides/lec-23.html#threshold-1",
    "title": "CISC482 - Lecture23",
    "section": "Threshold 1",
    "text": "Threshold 1"
  },
  {
    "objectID": "slides/lec-23.html#threshold-2",
    "href": "slides/lec-23.html#threshold-2",
    "title": "CISC482 - Lecture23",
    "section": "Threshold 2",
    "text": "Threshold 2"
  },
  {
    "objectID": "slides/lec-23.html#question-threshold",
    "href": "slides/lec-23.html#question-threshold",
    "title": "CISC482 - Lecture23",
    "section": "Question Threshold",
    "text": "Question Threshold\n\n\n\n\nHow many total samples?\nHow many clusters would there be with dashed blue line?\nHow many clusters would there be with dashed red line?\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-22.html#schedule",
    "href": "slides/lec-22.html#schedule",
    "title": "CISC482 - Lecture22",
    "section": "Schedule",
    "text": "Schedule\n\nReading 8-3: April 19 @ 12PM, Wednesday\nExample Report on Brightspace!!\nPeer Review - April 19 @ 12PM, Wednesday\nFinal Report - April 28 @ Midnight\nFinal Presentation - May 5, Final"
  },
  {
    "objectID": "slides/lec-22.html#today",
    "href": "slides/lec-22.html#today",
    "title": "CISC482 - Lecture22",
    "section": "Today",
    "text": "Today\n\nReview Unsupervised Learning and K-Means\nPeer Review"
  },
  {
    "objectID": "slides/lec-22.html#ideas",
    "href": "slides/lec-22.html#ideas",
    "title": "CISC482 - Lecture22",
    "section": "Ideas",
    "text": "Ideas\n\n\nWhat is the difference between supervised learning and unsupervised learning?\nWhat is the quintienstial example of unsupervised learning\n\nclustering\n\nNames of some example clustering algs?"
  },
  {
    "objectID": "slides/lec-22.html#all-clustering-algs",
    "href": "slides/lec-22.html#all-clustering-algs",
    "title": "CISC482 - Lecture22",
    "section": "All clustering algs",
    "text": "All clustering algs"
  },
  {
    "objectID": "slides/lec-22.html#k-means",
    "href": "slides/lec-22.html#k-means",
    "title": "CISC482 - Lecture22",
    "section": "K-Means",
    "text": "K-Means\n\n\nCluster is a set of samples with similar or dissimilar characteristics?\nGrouping samples into classes with similar characteristics is called clustering.\nCentroid ?\n\nis a point that represents the center of each cluster.\n\nSamples that are closer to a cluster’s centroid are said to be similar to each other and considered part of the cluster.\nWhat do we mean by closer? Can you give a precise definition?"
  },
  {
    "objectID": "slides/lec-22.html#k-means-algorithm",
    "href": "slides/lec-22.html#k-means-algorithm",
    "title": "CISC482 - Lecture22",
    "section": "K-means Algorithm",
    "text": "K-means Algorithm\n\n\nStep 0: Select the number of clusters, \\(k\\).\nStep 1: Randomly initialize samples as cluster centroids.\nStep 2: For each sample, calculate the distance between that sample and each cluster’s centroid, and assign the sample to the cluster with the closest centroid.\nStep 3: For each cluster, calculate the mean of all samples in the cluster. This mean becomes the new centroid. -Step 4: Repeat steps 2 and 3 until a stopping criterion is met. Such as reaching a certain number of iterations or the centroids staying the same."
  },
  {
    "objectID": "slides/lec-22.html#choosing-k",
    "href": "slides/lec-22.html#choosing-k",
    "title": "CISC482 - Lecture22",
    "section": "Choosing K",
    "text": "Choosing K\n\n\nThe number of clusters is not often obvious, especially if the data has more than two features.\nThe elbow method is the most common technique to determine the optimal number of clusters for the data.\nThe intuition is that good groups should be close together.\nHow can we measure how close things are together?\n\nThe sum of squared distanced between all samples and their centroid\nwithin-cluster sum of squares - WCSS"
  },
  {
    "objectID": "slides/lec-22.html#sweet-spot",
    "href": "slides/lec-22.html#sweet-spot",
    "title": "CISC482 - Lecture22",
    "section": "Sweet Spot",
    "text": "Sweet Spot"
  },
  {
    "objectID": "slides/lec-22.html#motivation",
    "href": "slides/lec-22.html#motivation",
    "title": "CISC482 - Lecture22",
    "section": "Motivation",
    "text": "Motivation\n\n\nMaking the writing process more collaborative\nLearn from one another and to think carefully about the role of writing in this course\nAllows students to clarify your own ideas\n\nYou explain your ideas to classmates and the reviewer formulates questions about your writing\n\nPeer review provides professional experience for you having your writing reviewed\nPeer review minimizes last minute drafting"
  },
  {
    "objectID": "slides/lec-22.html#instructions",
    "href": "slides/lec-22.html#instructions",
    "title": "CISC482 - Lecture22",
    "section": "Instructions",
    "text": "Instructions\nInstructions"
  },
  {
    "objectID": "slides/lec-22.html#groups",
    "href": "slides/lec-22.html#groups",
    "title": "CISC482 - Lecture22",
    "section": "Groups",
    "text": "Groups\nYou should have been e-mailed view only access to their drafts\n\nSupriya -> Review Jaydon and Carter\nAnmol -> Review Carter and Iana\nJaydon -> Review Iana and Chhandak\nCarter -> Review Prashant and Haven\nIana -> Review Haven and Chhandak\nHaven -> Review Prashant and Shivay\nChhandak -> Review Shivay and Supriya\nPrashant -> Review Supriya and Anmol\nShivay -> Review Anmol and Jaydon\n\n\n\n\n\n\n\nTip\n\n\nIf someone does not have their draft report ready, you can skip theirs (luck you!)\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-13.html#schedule",
    "href": "slides/lec-13.html#schedule",
    "title": "CISC482 - Lecture13",
    "section": "Schedule",
    "text": "Schedule\n\nReading 6-1: Mar 08 @ 12PM, Wednesday\nHW4 - Mar 08 @ Midnight\nProposal: Mar 22, Wednesday"
  },
  {
    "objectID": "slides/lec-13.html#today",
    "href": "slides/lec-13.html#today",
    "title": "CISC482 - Lecture13",
    "section": "Today",
    "text": "Today\n\nReview Linear Regression\nReview Logistic Regression\nPractice Problems"
  },
  {
    "objectID": "slides/lec-13.html#give-me-the-function-models",
    "href": "slides/lec-13.html#give-me-the-function-models",
    "title": "CISC482 - Lecture13",
    "section": "Give me the function models",
    "text": "Give me the function models\n\n\nSimple Linear Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x\\)\n\nSimple Polynomial Linear Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x + ... + \\beta_k x^k\\)\n\nMultiple Linear Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k\\)\n\nMultiple (Variable) Polynomial Regression\n\n\\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_1^2 + \\beta_3 x_1 x_2 + \\beta_4 x_2 + \\beta_5 x_2^2\\)"
  },
  {
    "objectID": "slides/lec-13.html#logistic-regression",
    "href": "slides/lec-13.html#logistic-regression",
    "title": "CISC482 - Lecture13",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nWe often use logistic regression for classification problems? Then why do we call it logistic regression? What is being regressed?\nWhat is the function for logistic regression?\n\n\\(\\hat{p}(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+ e^{\\beta_0 + \\beta_1 x}}\\)\n\nWhat do we call this non-linear function? HINT - what shape does it make?"
  },
  {
    "objectID": "slides/lec-13.html#why-not-linear-regression",
    "href": "slides/lec-13.html#why-not-linear-regression",
    "title": "CISC482 - Lecture13",
    "section": "Why not Linear Regression?",
    "text": "Why not Linear Regression?\n\nLinear Regression is strongly affected by outliers\nLinear Regression is strongly affected by imbalanced classes"
  },
  {
    "objectID": "slides/lec-13.html#visual",
    "href": "slides/lec-13.html#visual",
    "title": "CISC482 - Lecture13",
    "section": "Visual",
    "text": "Visual"
  },
  {
    "objectID": "slides/lec-08.html#schedule",
    "href": "slides/lec-08.html#schedule",
    "title": "CISC482 - Lecture08",
    "section": "Schedule",
    "text": "Schedule\n\nReading 4-1: Feb 10 @ 12PM, Friday\nReading 4-2: Feb 15 @ 12PM, Wednesday\nHW3: Feb 15 @ Midnight, Wednesday\nExam 1 - Feb 15 in class\nTopic Ideas - Feb 22 @ Midnight"
  },
  {
    "objectID": "slides/lec-08.html#cs-faculty-candidate",
    "href": "slides/lec-08.html#cs-faculty-candidate",
    "title": "CISC482 - Lecture08",
    "section": "CS Faculty Candidate",
    "text": "CS Faculty Candidate\n\nRazuan Hossain is here!\nPlease attend a meet and greet at 3:15 in SBSC 112\nExtra Credit!"
  },
  {
    "objectID": "slides/lec-08.html#today",
    "href": "slides/lec-08.html#today",
    "title": "CISC482 - Lecture08",
    "section": "Today",
    "text": "Today\n\nClass Activity, building graphs\nExam Review Sheet"
  },
  {
    "objectID": "slides/lec-08.html#class-activity-1",
    "href": "slides/lec-08.html#class-activity-1",
    "title": "CISC482 - Lecture08",
    "section": "Class Activity",
    "text": "Class Activity\n📋 Class Activity 04 - Practice Seaborn"
  },
  {
    "objectID": "slides/lec-08.html#you-can-do-it",
    "href": "slides/lec-08.html#you-can-do-it",
    "title": "CISC482 - Lecture08",
    "section": "You can do it!",
    "text": "You can do it!\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "slides/lec-18.html#schedule",
    "href": "slides/lec-18.html#schedule",
    "title": "CISC482 - Lecture18",
    "section": "Schedule",
    "text": "Schedule\n\nReading 7-2: Mar 31 @ 12PM, Friday\nReading 7-3: Apr 5 @ 12PM, Wednesday\nHW6 - Working on it… April 12 @ Midnight, Wednesday"
  },
  {
    "objectID": "slides/lec-18.html#today",
    "href": "slides/lec-18.html#today",
    "title": "CISC482 - Lecture18",
    "section": "Today",
    "text": "Today\n\nShort Review\nNaive Bayes\nClass Activity"
  },
  {
    "objectID": "slides/lec-18.html#terms",
    "href": "slides/lec-18.html#terms",
    "title": "CISC482 - Lecture18",
    "section": "Terms",
    "text": "Terms\n\nAn instance is labeled if\nSupervised learning is\nRegression vs Classification?\nWhat does KNN stand for?"
  },
  {
    "objectID": "slides/lec-18.html#metric-space",
    "href": "slides/lec-18.html#metric-space",
    "title": "CISC482 - Lecture18",
    "section": "Metric Space",
    "text": "Metric Space\nA metric space is an unordered pair \\((M, d)\\) where \\(M\\) is a set and \\(d\\) is a metric on \\(M\\), a function where \\(d: M \\times M \\rightarrow \\mathbb{R}\\). Where \\(d\\) satisfies the following axioms for all points \\(x,y,z \\in M\\)\n\nThe distance from a point to itself is 0: \\(d(x,x) = 0\\)\nThe distance between any two distinct points is always positive: If \\(x \\neq y, then \\; d(y,x) > 0\\)\nThe distance from x to y is always the same distance from y to x: \\(d(x,y) = d(y,x)\\)\nTriangle Inequality holds: \\(d(x,z) \\geq d(x,y) + d(y, z)\\)"
  },
  {
    "objectID": "slides/lec-18.html#basic-idea",
    "href": "slides/lec-18.html#basic-idea",
    "title": "CISC482 - Lecture18",
    "section": "Basic Idea",
    "text": "Basic Idea\n\nNaive Bayes classification is a supervised learning classifier that uses the number of times a feature occurs in each possible class to estimate the likelihood an instance is in the class.\nNaive Bayes is often used for applications with large amounts of text data.\n\nidentifying the author of a new document based on prior documents with known authors.\ndetecting spam emails"
  },
  {
    "objectID": "slides/lec-18.html#motivating-example",
    "href": "slides/lec-18.html#motivating-example",
    "title": "CISC482 - Lecture18",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nProfessor Castagno, I have a question about when HW6 is due. Is it due on Wednesday before class or at midnight. Thank you for your help!\n\nProffesssor Castagno, Congratulations!!!1 You won free trip to B@hamas. Give credit card to confirm sh1pping of tickets. Free no, expense trip! Respond now!\n\n\n\nWhich one is spam? How do you know?"
  },
  {
    "objectID": "slides/lec-18.html#example-problem-focus",
    "href": "slides/lec-18.html#example-problem-focus",
    "title": "CISC482 - Lecture18",
    "section": "Example Problem Focus",
    "text": "Example Problem Focus\n\nFor the rest of the class we will be doing examples on spam classification\nThis means we are give a document that has words in it and want to predict the class: ham or spam\nAll machine learn learning models need features to learn from, and we expect these to be numbers\nWe need to transform our document into a feature vector: \\(X\\)"
  },
  {
    "objectID": "slides/lec-18.html#two-approaches",
    "href": "slides/lec-18.html#two-approaches",
    "title": "CISC482 - Lecture18",
    "section": "Two Approaches",
    "text": "Two Approaches\nThere are two main approaches to Naive Bayes and they boil down to how to transform the document. But first some definitions:\n\nWe denote \\(D = \\{d_1, d_2, ..., d_i, ..., d_m \\}\\) as the set of all documents. Often called the corpus. \\(\\lvert D \\rvert = m\\).\nEvery \\(d_i\\) is composed of tokens (words) where spaces are ignored\nThe set of all unique tokens from \\(D\\) is called the vocabulary. Lets denote the cardinality of this set to be \\(n\\). So there are \\(n\\) unique words in the corpus."
  },
  {
    "objectID": "slides/lec-18.html#example",
    "href": "slides/lec-18.html#example",
    "title": "CISC482 - Lecture18",
    "section": "Example",
    "text": "Example\n\nD1 - The best part of waking up is Folgers in your cup. Fill your cup up with Folgers.\nD2 - You are all great students! I am so proud to be a teacher of great students!\n\n\nimport string\nvocabulary = set(d_1.split(' ') + d_2.split(' '))\nprint(vocabulary)\nprint(len(vocabulary))\n\n{'Fill', 'I', 'The', 'part', 'proud', 'waking', 'of', 'in', 'Folgers', 'be', 'up', 'Folgers.', 'great', 'teacher', 'with', 'to', 'best', 'so', 'students!', 'your', 'cup', 'a', 'is', 'all', 'are', 'am', 'You', 'cup.'}\n28"
  },
  {
    "objectID": "slides/lec-18.html#preprocess-text",
    "href": "slides/lec-18.html#preprocess-text",
    "title": "CISC482 - Lecture18",
    "section": "Preprocess Text",
    "text": "Preprocess Text\n\nRemove Punctuation\nLeading and Ending White space - ’ Hey ’\nReplace common occurring text patterns with a single word, Regular Expression -‘http://spam.me’ –> url\n\n‘$’ ‘£’& –> ‘mnsymb’\n‘55555’ – ‘shrtcode’\n‘867-5309’ –> ‘phonenumber’\n‘88’ –> ‘number’\n\nLower case\nPort Stemmer - ‘testing’ -> ‘test’\nRemove Stop words - ‘the’, ‘a’"
  },
  {
    "objectID": "slides/lec-18.html#creating-features-vectors",
    "href": "slides/lec-18.html#creating-features-vectors",
    "title": "CISC482 - Lecture18",
    "section": "Creating Features Vectors",
    "text": "Creating Features Vectors\nEvery document \\(d_i\\) has a feature vector, \\(x_i\\), that has \\(n\\) elements in it. Each element will represent information about a unique vocab word in our corpus.\n\nMultinomial Naive Bayes\n\nEach document is broken up to tokens. The feature vector is then the frequency count of the vocabulary words in the token set.\n\nBernoulli Naive Bayes\n\nEach document is broken up into tokens. The feature vector is then a boolean variable if the the vocab word was found in the document."
  },
  {
    "objectID": "slides/lec-18.html#multinomial-feature-vector",
    "href": "slides/lec-18.html#multinomial-feature-vector",
    "title": "CISC482 - Lecture18",
    "section": "Multinomial Feature Vector",
    "text": "Multinomial Feature Vector\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer_m = CountVectorizer()\nX_m = vectorizer_m.fit_transform(docs)\nprint(f\"Vocab: {len(vectorizer_m.get_feature_names_out())}\")\nprint(vectorizer_m.get_feature_names_out())\nprint(\"Feature Vector: \")\nprint(X_m.toarray())\n\n\nVocab: 24\n['all' 'am' 'are' 'be' 'best' 'cup' 'fill' 'folgers' 'great' 'in' 'is'\n 'of' 'part' 'proud' 'so' 'students' 'teacher' 'the' 'to' 'up' 'waking'\n 'with' 'you' 'your']\nFeature Vector: \n[[0 0 0 0 1 2 1 2 0 1 1 1 1 0 0 0 0 1 0 2 1 1 0 2]\n [1 1 1 1 0 0 0 0 2 0 0 1 0 1 1 2 1 0 1 0 0 0 1 0]]"
  },
  {
    "objectID": "slides/lec-18.html#bernoulli-feature-vector",
    "href": "slides/lec-18.html#bernoulli-feature-vector",
    "title": "CISC482 - Lecture18",
    "section": "Bernoulli Feature Vector",
    "text": "Bernoulli Feature Vector\n\n\nCode\nvectorizer_b = CountVectorizer(binary=True)\nX_b = vectorizer_b.fit_transform(docs)\nprint(f\"Vocab: {len(vectorizer_b.get_feature_names_out())}\")\nprint(vectorizer_b.get_feature_names_out())\nprint(\"Feature Vector: \")\nprint(X_b.toarray())\n\n\nVocab: 24\n['all' 'am' 'are' 'be' 'best' 'cup' 'fill' 'folgers' 'great' 'in' 'is'\n 'of' 'part' 'proud' 'so' 'students' 'teacher' 'the' 'to' 'up' 'waking'\n 'with' 'you' 'your']\nFeature Vector: \n[[0 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1]\n [1 1 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0]]"
  },
  {
    "objectID": "slides/lec-18.html#which-one-to-use",
    "href": "slides/lec-18.html#which-one-to-use",
    "title": "CISC482 - Lecture18",
    "section": "Which one to use?",
    "text": "Which one to use?\n\nMultinomial is the more complex model. Bernoulli is the simpler model\nLess Data -> Bernoulli -> Less overfitting\nAlot of Data -> Multinomial\nZybook only shows Multinomial"
  },
  {
    "objectID": "slides/lec-18.html#bayes-models",
    "href": "slides/lec-18.html#bayes-models",
    "title": "CISC482 - Lecture18",
    "section": "Bayes Models",
    "text": "Bayes Models\n\n\\(P(H|\\textbf{D}) = P(H ) \\frac{P(\\textbf{D} | H)}{P(\\textbf{D})}\\)\n\n\n\\(P(C = Spam|\\textbf{X}) = \\underbrace{P(C = Spam)}_{Prior} \\frac{P(\\textbf{X} | C = Spam)}{P(\\textbf{X})}\\)\n\n\n\\(P(C = Spam|\\textbf{X}) = P(C = Spam) \\frac{\\underbrace{P(\\textbf{X} | C = Spam)}_{Likelihood}}{P(\\textbf{X})}\\)\n\n\nRemember that \\(\\textbf{X} = x_1, x_2, ... x_n\\), one for each vocaublary word. Mulitnomial \\(x_i\\) is the _____ and for bernoulli it is the _____."
  },
  {
    "objectID": "slides/lec-18.html#the-prior",
    "href": "slides/lec-18.html#the-prior",
    "title": "CISC482 - Lecture18",
    "section": "The Prior",
    "text": "The Prior\n\\(P(C = Spam)\\) = ?\nIrrepspective of data, what is the probability that this is a spam document.\n\n\\(P(C = Spam)\\) = \\(\\frac{\\text{# Spam Documents}}{\\text{# All Documents}} = \\frac{m_s}{m}\\)\n\n\n\\(P(C = Ham)\\) = \\(1 - P(C = Spam)\\)"
  },
  {
    "objectID": "slides/lec-18.html#the-likelihood-the-hard-one",
    "href": "slides/lec-18.html#the-likelihood-the-hard-one",
    "title": "CISC482 - Lecture18",
    "section": "The Likelihood (the hard one)",
    "text": "The Likelihood (the hard one)\n\\(P(\\textbf{X} | C = Spam) = P(x_1, x_2, ... x_i, x_n | Spam)\\)\nThis is a very large joint probability! Very difficult to compute! But we have some tricks…\n\n\\(P(\\textbf{X} | Spam) = P(x_1 | x_2, ... , x_n, Spam) P(x_2, ... x_n | Spam)\\)\n\n\n\\(P(\\textbf{X} | Spam) = P(x_1 | x_2, ... , x_n, Spam) P(x_2 | x_3, ..., Spam) P(x_3, ... x_n | Spam)\\)\n\n\nThis is still just incredibly difficult to compute…. But we can make a naive assumption."
  },
  {
    "objectID": "slides/lec-18.html#the-naive-assumption",
    "href": "slides/lec-18.html#the-naive-assumption",
    "title": "CISC482 - Lecture18",
    "section": "The Naive Assumption",
    "text": "The Naive Assumption\n\\(P(x_1 | x_2, ..., x_n, Spam ) = P(x_1 | Spam)\\)\n\n\nWhat is this assumption saying?\nIs it true?\nIs it useful?\n\n\n\n\\(P(\\textbf{X} | Spam) = \\Pi P(x_i | Spam)\\)"
  },
  {
    "objectID": "slides/lec-18.html#calculating-px_i-cspam-bernoulli",
    "href": "slides/lec-18.html#calculating-px_i-cspam-bernoulli",
    "title": "CISC482 - Lecture18",
    "section": "Calculating \\(P(x_i | C=Spam)\\), Bernoulli",
    "text": "Calculating \\(P(x_i | C=Spam)\\), Bernoulli\n\nLooking at only spam documents\nNumber of spam documents where \\(x_i\\) appeared = \\(x_{i}^s\\)\nTotal number of spam documents = \\(m_s\\)\n\\(P(x_i | C=Spam) = \\frac{x_{i}^s}{m_s}\\)"
  },
  {
    "objectID": "slides/lec-18.html#calculating-px_i-cham-bernoulli",
    "href": "slides/lec-18.html#calculating-px_i-cham-bernoulli",
    "title": "CISC482 - Lecture18",
    "section": "Calculating \\(P(x_i | C=Ham)\\), Bernoulli",
    "text": "Calculating \\(P(x_i | C=Ham)\\), Bernoulli\n\nLooking at only ham documents\nNumber of ham documents where \\(x_i\\) appeared = \\(x_{i}^h\\)\nTotal number of ham documents = \\(m_h\\)\n\\(P(x_i | C=Ham) = \\frac{x_{i}^h}{m_h}\\)"
  },
  {
    "objectID": "slides/lec-18.html#putting-it-all-together",
    "href": "slides/lec-18.html#putting-it-all-together",
    "title": "CISC482 - Lecture18",
    "section": "Putting it all together",
    "text": "Putting it all together\n\n\\(P(C=Spam | \\textbf{X}) = P(C=Spam)\\dfrac{P(\\textbf{X} | C=Spam)}{P(\\textbf{X})}\\)\n\n\n\\(P(C=Spam | \\textbf{X}) \\propto P(C=Spam) \\; P(\\textbf{X} | C=Spam)\\)\n\n\n\\(P(C=Spam | \\textbf{X}) \\propto P(C=Spam) \\; \\Pi P(x_i | C=Spam)\\)\n\n\n\\(P(C=Spam | \\textbf{X}) \\propto \\dfrac{m_s}{m} \\; \\Pi \\dfrac{x_{i}^s}{m_s}\\)\n\n\n\n\n\n\n\n\nWarning\n\n\nAnyone see a problem here? What happens if a vocab word does not appear in the document?"
  },
  {
    "objectID": "slides/lec-18.html#laplacian-smoothing-for-naive-bayes",
    "href": "slides/lec-18.html#laplacian-smoothing-for-naive-bayes",
    "title": "CISC482 - Lecture18",
    "section": "Laplacian Smoothing for Naive Bayes",
    "text": "Laplacian Smoothing for Naive Bayes\n\n\\(P(C=Spam | \\textbf{X}) \\propto \\dfrac{m_s}{m} \\; \\Pi \\dfrac{x_{i}^s + 1}{m_s + 2}\\)\n\\(P(C=Ham | \\textbf{X}) \\propto \\dfrac{m_h}{m} \\; \\Pi \\dfrac{x_{i}^h + 1}{m_h + 2}\\)\nCompute both numbers. Whichever class has a higher prob, classify as that one\n\n\n\n\n\n\n\n\nWarning\n\n\nAny astute computer scientists see a possible problem…."
  },
  {
    "objectID": "slides/lec-18.html#floating-point-number-inaccuracies",
    "href": "slides/lec-18.html#floating-point-number-inaccuracies",
    "title": "CISC482 - Lecture18",
    "section": "Floating point number inaccuracies",
    "text": "Floating point number inaccuracies\n\nimport math\nprobs = [0.01] * 20\nfinal_prob = math.prod(probs)\nprint(f\"Probabilities: {probs}\")\nprint(f\"Final Probability: {final_prob}\")\n\nProbabilities: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\nFinal Probability: 1.0000000000000005e-40\n\n\n\n\nimport math\nprobs = [0.01] * 10_000\nfinal_prob = math.prod(probs)\nprint(f\"Final Probability: {final_prob}\")\n\nFinal Probability: 0.0\n\n\n\n\n\nAny one have any ideas to fix this?"
  },
  {
    "objectID": "slides/lec-18.html#log-function",
    "href": "slides/lec-18.html#log-function",
    "title": "CISC482 - Lecture18",
    "section": "Log function",
    "text": "Log function\n\n\n\\(P(C=Spam | \\textbf{X}) \\propto \\dfrac{m_s}{m} \\; \\prod \\dfrac{x_{i}^s + 1}{m_s + 2}\\)\n\\(P(C=Spam | \\textbf{X}) \\propto \\ln \\left( \\dfrac{m_s}{m} \\; \\prod \\dfrac{x_{i}^s + 1}{m_s + 2} \\right)\\)\nWhat is this \\(\\log(a \\cdot b) = ?\\)\n\\(\\log(a \\cdot b) = \\log(a) + \\log(b)\\)\n\\(P(C=Spam | \\textbf{X}) \\propto \\ln \\left(\\dfrac{m_s}{m} \\right) + \\; \\sum \\ln \\left(\\dfrac{x_{i}^s + 1}{m_s + 2}\\right)\\)"
  },
  {
    "objectID": "slides/lec-18.html#log-functon-graph",
    "href": "slides/lec-18.html#log-functon-graph",
    "title": "CISC482 - Lecture18",
    "section": "Log Functon Graph",
    "text": "Log Functon Graph\n\n\n\nx = np.linspace(0, 1, 1000000)\ny = np.log(x)\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nWe can transfrom our probabilties with log and make everything numerically stable!!\n\n\n\n\n\n\n\n\n\n\nhttps://jeremybyu.github.io/intro-ds-website"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "TBD"
  },
  {
    "objectID": "project-tips-resources.html",
    "href": "project-tips-resources.html",
    "title": "Project tips + resources",
    "section": "",
    "text": "You took Philosphy 106 (ethics) and you discussed the relationship between employers and employees. You submitted a signature assignment of a paper on unions (pro or against, it doesn’t matter). A possible dataset you could choose is the Right to Work Dataset and do an analysis on the relationship between state laws and their effect on union participation.\nYou took a BIO 101 class and discussed the biologcal mechanisms of diabetes. A possible dataset you could choose is the Diabetes Dataset and create a model that predict diabetes based on diagnostic measurements.\nYou took ARTS 104 (Creativity: Methods and Practices). A possible dataset you could choose is the Best Artworks of All Time and do a clustering analysis to see if there exists any patterns about great artists in our times (country, age, sex, etc.).\nYou took SOCIO 101. A possible dataset you could choose is the Student Performance Dataset that looks at student achievement in secondary education. You could create a regression model that predicts performance based upon a variety of factors.\n\n\n\n\n\nKaggle Datasets\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "project-tips-resources.html#tips",
    "href": "project-tips-resources.html#tips",
    "title": "Project tips + resources",
    "section": "Tips",
    "text": "Tips\n\nAsk questions if any of the expectations are unclear.\nCode: In your write up your code should be hidden (echo = FALSE) so that your document is neat and easy to read. However your document should include all your code such that if I re-knit your qmd file I should be able to obtain the results you presented.\n\nException: If you want to highlight something specific about a piece of code, you’re welcome to show that portion.\n\nMerge conflicts will happen, issues will arise, and that’s fine! Commit and push often, and ask questions when stuck.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works."
  },
  {
    "objectID": "project-tips-resources.html#formatting-communication-tips",
    "href": "project-tips-resources.html#formatting-communication-tips",
    "title": "Project tips + resources",
    "section": "Formatting + communication tips",
    "text": "Formatting + communication tips\n\n\nHeaders\n\nUse headers to clearly label each section.\nInspect the document outline to review your headers and sub-headers.\n\n\n\nReferences\n\nInclude all references in a section called “References” at the end of the report.\nThis course does not have specific requirements for formatting citations and references.\n\n\n\nAppendix\n\nIf you have additional work that does not fit or does not belong in the body of the report, you may put it at the end of the document in section called “Appendix”.\nThe items in the appendix should be properly labeled.\nThe appendix should only be for additional material. The reader should be able to fully understand your report without viewing content in the appendix.\n\n\n\nResize figures\nResize plots and figures, so you have more space for the narrative.\n\n\nArranging plots\nArrange plots in a grid, instead of one after the other. This is especially useful when displaying plots for exploratory data analysis and to check assumptions.\nIf you’re using ggplot2 functions, the patchwork package makes it easy to arrange plots in a grid. See the documentation and examples here.\n\n\nPlot titles and axis labels\nBe sure all plot titles and axis labels are visible and easy to read.\n\nUse informative titles, not variable names, for titles and axis labels.\n\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\nTODO example plot\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of STA 210.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document."
  },
  {
    "objectID": "project-tips-resources.html#additional-resources",
    "href": "project-tips-resources.html#additional-resources",
    "title": "Project tips + resources",
    "section": "Additional resources",
    "text": "Additional resources"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Intro to Data Science",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "📖 Read Assignments 5-1 📖 Read Assignments 5-2"
  },
  {
    "objectID": "weeks/week-6.html#participate",
    "href": "weeks/week-6.html#participate",
    "title": "Week 6",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 10 - Regression 1 🖥️ Lecture 11 - Regression 2"
  },
  {
    "objectID": "weeks/week-6.html#practice",
    "href": "weeks/week-6.html#practice",
    "title": "Week 6",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 05 - Practice Regression"
  },
  {
    "objectID": "weeks/week-6.html#perform",
    "href": "weeks/week-6.html#perform",
    "title": "Week 6",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 4 - Regression"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 8-1 📖 Read Assignments 8-2"
  },
  {
    "objectID": "weeks/week-12.html#participate",
    "href": "weeks/week-12.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 21 - KMeans 🖥️ Lecture 22 - Peer Review"
  },
  {
    "objectID": "weeks/week-12.html#practice",
    "href": "weeks/week-12.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nN/A"
  },
  {
    "objectID": "weeks/week-12.html#perform",
    "href": "weeks/week-12.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nDraft Report and Peer Review"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 6-3"
  },
  {
    "objectID": "weeks/week-10.html#participate",
    "href": "weeks/week-10.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 17 - KNN 🖥️ Lecture 18 - Naive Bayes"
  },
  {
    "objectID": "weeks/week-10.html#practice",
    "href": "weeks/week-10.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nTBD"
  },
  {
    "objectID": "weeks/week-10.html#perform",
    "href": "weeks/week-10.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nProject Proposal! ⌨️ HW 6 - Supervised Learning"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "📖 Read Assignments 2-1 and 2-2 in Zybooks."
  },
  {
    "objectID": "weeks/week-2.html#participate",
    "href": "weeks/week-2.html#participate",
    "title": "Week 2",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 3 - Probability and Stats\n🖥️ Lecture 4 - Probability and Stats"
  },
  {
    "objectID": "weeks/week-2.html#practice",
    "href": "weeks/week-2.html#practice",
    "title": "Week 2",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 02 - Practice NumPy"
  },
  {
    "objectID": "weeks/week-2.html#perform",
    "href": "weeks/week-2.html#perform",
    "title": "Week 2",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 2 - Analyze Data"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "📖 Read Assignments 5-3"
  },
  {
    "objectID": "weeks/week-7.html#participate",
    "href": "weeks/week-7.html#participate",
    "title": "Week 7",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 12 - Regression 3 🖥️ Lecture 13 - Regression 4"
  },
  {
    "objectID": "weeks/week-7.html#practice",
    "href": "weeks/week-7.html#practice",
    "title": "Week 7",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 06 - Practice Regression"
  },
  {
    "objectID": "weeks/week-7.html#perform",
    "href": "weeks/week-7.html#perform",
    "title": "Week 7",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 4 - Regression"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "📖 Read Assignments 2-3 and 3-1 in Zybooks."
  },
  {
    "objectID": "weeks/week-3.html#participate",
    "href": "weeks/week-3.html#participate",
    "title": "Week 3",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 5 - Probability Inference\n🖥️ Lecture 6 - Data Wrangling"
  },
  {
    "objectID": "weeks/week-3.html#practice",
    "href": "weeks/week-3.html#practice",
    "title": "Week 3",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 03 - Practice Pandas"
  },
  {
    "objectID": "weeks/week-3.html#perform",
    "href": "weeks/week-3.html#perform",
    "title": "Week 3",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 3 - Cleaning Data and Graphing"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 7-2 📖 Read Assignments 7-3"
  },
  {
    "objectID": "weeks/week-11.html#participate",
    "href": "weeks/week-11.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 20 - SVM"
  },
  {
    "objectID": "weeks/week-11.html#practice",
    "href": "weeks/week-11.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 09 - Naive Bayes"
  },
  {
    "objectID": "weeks/week-11.html#perform",
    "href": "weeks/week-11.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nDraft Report due next Wednesday!"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "📖 Read Assignments 3-2 and 4-1 in Zybooks."
  },
  {
    "objectID": "weeks/week-4.html#participate",
    "href": "weeks/week-4.html#participate",
    "title": "Week 4",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 7 - Probability Inference\n🖥️ Lecture 8 - Data Wrangling"
  },
  {
    "objectID": "weeks/week-4.html#practice",
    "href": "weeks/week-4.html#practice",
    "title": "Week 4",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 04 - Practice Seaborn"
  },
  {
    "objectID": "weeks/week-4.html#perform",
    "href": "weeks/week-4.html#perform",
    "title": "Week 4",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 3 - Cleaning Data, Graphing, Probability"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 5-3"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 14 - Model Performance 1 🖥️ Lecture 15 - Model Performance 1"
  },
  {
    "objectID": "weeks/week-8.html#practice",
    "href": "weeks/week-8.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 07 - Model Evaluation"
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nProject Proposal! ⌨️ HW 5 - Model Performance"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "📖 Read Assignments 4-2"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 9 - EDA"
  },
  {
    "objectID": "weeks/week-5.html#practice",
    "href": "weeks/week-5.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 04 - Practice Seaborn"
  },
  {
    "objectID": "weeks/week-5.html#perform",
    "href": "weeks/week-5.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\nNone"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus 📖 Read the project description 📖 Read Chapter 1 in Zybook"
  },
  {
    "objectID": "weeks/week-1.html#participate",
    "href": "weeks/week-1.html#participate",
    "title": "Week 1",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 1 - Welcome to CISC 482\n🖥️ Lecture 2 - What is Data Science"
  },
  {
    "objectID": "weeks/week-1.html#practice",
    "href": "weeks/week-1.html#practice",
    "title": "Week 1",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 01 - Practice Mardown"
  },
  {
    "objectID": "weeks/week-1.html#perform",
    "href": "weeks/week-1.html#perform",
    "title": "Week 1",
    "section": "Perform",
    "text": "Perform\n⌨️ HW 1- Practice Markdown"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 6-3"
  },
  {
    "objectID": "weeks/week-9.html#participate",
    "href": "weeks/week-9.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 16 - Model Performance 1 🖥️ Lecture 17 - Model Performance 1"
  },
  {
    "objectID": "weeks/week-9.html#practice",
    "href": "weeks/week-9.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\n📋 Class Activity 07 - Model Evaluation"
  },
  {
    "objectID": "weeks/week-9.html#perform",
    "href": "weeks/week-9.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nProject Proposal! ⌨️ HW 5 - Model Performance"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 8",
    "section": "",
    "text": "📖 Read Assignments 8-1 📖 Read Assignments 8-2"
  },
  {
    "objectID": "weeks/week-13.html#participate",
    "href": "weeks/week-13.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\n🖥️ Lecture 23 - Hierarchial Clustering 🖥️ Lecture 24 - PCA"
  },
  {
    "objectID": "weeks/week-13.html#practice",
    "href": "weeks/week-13.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nN/A"
  },
  {
    "objectID": "weeks/week-13.html#perform",
    "href": "weeks/week-13.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\nFinal Report"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "We will be using Google Collab for all Class Activities and Homework. Please just use your Springfield College e-mail for access."
  }
]