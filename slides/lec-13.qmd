---
title: "CISC482 - Lecture13"
subtitle: "Regression 2"
author: "Dr. Jeremy Castagno"
footer:  "[https://jeremybyu.github.io/intro-ds-website](https://jeremybyu.github.io/intro-ds-website/)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    show-slide-number: all 
    chalkboard: true
    code-copy: false
    revealjs-plugins:
      - noswipe
editor: visual
execute:
  freeze: auto

---


```{python}
#| context: setup
#| include: false
import pandas as pd
import seaborn as sns 
import numpy as np
from sklearn.datasets import make_blobs
from palmerpenguins import load_penguins, load_penguins_raw
import matplotlib.pyplot as plt
df = load_penguins()
df.dropna(inplace=True)
np.set_printoptions(precision=1, suppress=True, formatter={'float_kind': "{:.1f}".format})
pd.options.display.float_format = '{:,.2f}'.format
np.random.seed(10)
```

# Class Business


## Schedule

- Reading 6-1:  Mar 08 @ 12PM, Wednesday
- [HW4](https://colab.research.google.com/drive/1EdakWIOXLV-1UjUytpgKj6yHMbq-Pvz3?usp=sharing) - Mar 08 @ Midnight
- [Proposal](#project-proposal): Mar 22, Wednesday

## Today

- Review Linear Regression
- Review Logistic Regression

# Review Linear Regression

## Give me the function models

::: incremental
- Simple Linear Regression
  - $\hat{y} = \beta_0 + x + \beta_1$
- Simple Polynomial Linear Regression 
  - $\hat{y} = \beta_0 + \beta_1 x + ... + \beta_k x^k$
- Multiple Linear Regression 
  - $\hat{y} = \beta_0 + \beta_1 x_1 + ... \beta_k x_k$
- Multiple (Variable) Polynomial Regression 
  - $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_3 x_1 x_2 + \beta_4 x_2 + \beta_5 x_2^2$
:::



# Class Activity

## Class Activity

[Class Activity](https://colab.research.google.com/drive/1JFqbWKsVqVGG8q5kPqoUHah-dsVFlENH?usp=share_link)

# Non Linear Regression

- Linear Regression - Linear Combination  of features
  - $\hat{y} = \beta_0 + \beta_1 x_1 + ... \beta_k x_k$
- Parameters $\beta_k$ are simply multiplied by features and added together
- Non-linear mathematical parameter's with respect to $\beta_k$
- $f(x, \beta)$

# Example Non Linear

- Trigonometric - $sin(\beta_0 x)$
- Exponential - $e^{\beta_0 + \beta_1 x}$
- Logistic - $\frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

::: callout-tip
This class will be focus on using logistic regression. But lets take a step back and talk about a new problem.
:::



## Classification {.smaller}

- We are now going to be focusing on problems where we are interested in predicting the *group* or the *class* of something
- Give $X = {x_1, x_2, ..., x_k}$ what is the class of this object.
  - Given the bill_length is 10 and bill_depth is 20 what is the species of this penguin?
  - A person arrives at the emergency room with a set of symptoms. Which of the three medical conditions does the individual have?
- Lets first focus where there are only 2 classes. Lets just call them (+) positive and  (-) negative.

## Example Classification



```{python}
#| echo: False
n = 20
x_neg = np.random.uniform(0, 5.5, size=n)
y_neg = np.zeros(n)

x_pos = np.random.uniform(4.5, 10, size=n)
y_pos = np.ones(n)

x = np.concatenate([x_neg, x_pos])
y = np.concatenate([y_neg, y_pos])

ax = sns.scatterplot(x=x,y=y, c=y, cmap='viridis');
ax.set_xlabel("X")
ax.set_ylabel("Class");
```

## Linear Regression for Classification

- Linear Regression is generally not suitable for classification purposes
- However, you *can* use it to draw a line that whose value predicts the class
- E.g. if $\hat{y} > 0.5 -> Positive$

## LR Classification (Okay)

```{python}
#| echo: False
from sklearn.linear_model import LinearRegression
model = LinearRegression()

X = x[:, np.newaxis] # n X 1 matrix
reg = model.fit(X, y) # X needs to be a matrix
slope, intercept  = reg.coef_[0], reg.intercept_
x_decision = (0.5 - intercept) / slope


ax = sns.scatterplot(x=x,y=y, c=y, cmap='viridis');
ax.axline((0, intercept), slope=slope, color='r', label='Regressed Line');
ax.scatter(x_decision, 0.5, marker='X', c='k', zorder=10)
ax.axhline(0.5, linestyle='--', c='k')
ax.axvline(x_decision, linestyle='--', c='k')
ax.set_xlabel("X")
ax.set_ylabel("Class")
plt.legend();
```


## LR Classification (BAD)

```{python}
#| echo: False
#| 
x = np.append(x, np.linspace(28, 29, 10))
y = np.append(y, np.ones(10))

X = x[:, np.newaxis] # n X 1 matrix
reg = model.fit(X, y) # X needs to be a matrix
slope, intercept  = reg.coef_[0], reg.intercept_
x_decision = (0.5 - intercept) / slope


ax = sns.scatterplot(x=x,y=y, c=y, cmap='viridis');
ax.axline((0, intercept), slope=slope, color='r', label='Regressed Line');
ax.scatter(x_decision, 0.5, marker='X', c='k', zorder=10)
ax.axvline(x_decision, linestyle='--', c='k')
ax.set_xlabel("X")
ax.set_ylabel("Class")
plt.legend();
```


## LR Classification (Really BAD)

```{python}
#| echo: False
#| 
x = np.append(x, np.linspace(100,101 , 100))
y = np.append(y, np.ones(100))

X = x[:, np.newaxis] # n X 1 matrix
reg = model.fit(X, y) # X needs to be a matrix
slope, intercept  = reg.coef_[0], reg.intercept_
x_decision = (0.5 - intercept) / slope


ax = sns.scatterplot(x=x,y=y, c=y, cmap='viridis');
ax.axline((0, intercept), slope=slope, color='r', label='Regressed Line');
ax.scatter(x_decision, 0.5, marker='X', c='k', zorder=10)
ax.axvline(x_decision, linestyle='--', c='k')
ax.set_xlabel("X")
ax.set_ylabel("Class")
plt.legend();
```

## Logistic Regression

::: columns
::: {.column width="50%"}

::: incremental
- Use a specific exponential function
- $\hat{y} = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$
- Whats the range of this function?
- $\hat{p} = \sigma(z)$, where
  - $z = \beta_0 + \beta_1 x$
  - $\sigma(z) = \frac{e^{z}}{1 + e^{z}}$
  - $\sigma(z) = \frac{1}{1 + e^{-z}}$
:::
:::


::: {.column width="50%"}
::: fragment
![](./lec12_images/sigmoid.jpg)
:::
:::
:::


## Example


```{python}
#| echo: False

xmin, xmax = -5, 5
n_samples = 100
np.random.seed(0)
x = np.random.normal(size=n_samples)
y = (x > 0).astype(float)
x[x > 0] *= 4
x += 0.3 * np.random.normal(size=n_samples)

X = x[:, np.newaxis];
```

```{python}
#| echo: True
from sklearn.linear_model import LinearRegression, LogisticRegression

linear_model = LinearRegression()
logistic_model = LogisticRegression(C=1e5)

linear_model.fit(X, y)
logistic_model.fit(X, y)

print(f"LINEAR MODEL: {linear_model.coef_[0]:.1f} * x + {linear_model.intercept_:.1f}")
print(f"LOGISTIC MODEL: sigmoid({logistic_model.coef_[0][0]:.1f} * x + {logistic_model.intercept_[0]:.1f})");

```

## Visual


```{python}
from scipy.special import expit
ax = sns.scatterplot(x=x,y=y, c=y, cmap='viridis');
ax.axline((0, linear_model.intercept_), slope=linear_model.coef_[0], color='r', label='Linear Regression Model');

x_test = np.linspace(-5, 10, 300)
loss = expit(x_test* logistic_model.coef_ + logistic_model.intercept_).ravel()
ax.plot(x_test, loss, label="Logistic Regression Model", color="blue", linewidth=3)

ax.axhline(0.5, linestyle='--', c='k')
ax.set_xlabel("X")
ax.set_ylabel("Class (Y)")
plt.legend(
    loc="lower right",
    fontsize="small",
);

```
::: callout-note
What do you notice?
:::

## Multivariable Classification

```{python}
# make 3-class dataset for classification
from sklearn.datasets import make_blobs
centers = [[-5, 0], [0, 1.5]]
X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)
transformation = [[0.4, 0.2], [-0.4, 1.2]]
X = np.dot(X, transformation)

ax = sns.scatterplot(x=X[:,0], y=X[:,1], c=y, cmap='viridis')
ax.set_xlabel("X")
ax.set_ylabel("Y");

```

## In 3D

```{python}
# make 3-class dataset for classification


fig = plt.figure(figsize = (10, 7))
ax = plt.axes(projection ="3d")
ax.scatter(X[:, 0], X[:, 1], y, label='points', c=y, cmap='viridis')
ax.set_xlabel("X")
ax.set_ylabel("Y")
ax.set_zlabel("Class")
ax.legend();
```

## Multivariable Logistic Regression

```{python}
#| echo: True
#| code-fold: True
logistic_model = LogisticRegression(C=1e5)
logistic_model.fit(X, y)

x_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)
y_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)
XX, YY = np.meshgrid(x_range, y_range)
P = expit(XX* logistic_model.coef_[0][0] + YY * logistic_model.coef_[0][0] + logistic_model.intercept_)


with sns.plotting_context("talk"):
  fig = plt.figure(figsize = (10, 7))
  ax = plt.axes(projection ="3d")
  # ZZ = np.ones_like(XX) * 0.5 
  # ax.plot_surface(XX, YY, ZZ,  **kwargs, zorder=1)
  ax.scatter(X[:,0], X[:,1], y, c=y, cmap='jet', zorder=10, edgecolor='k')
  ax.plot_surface(XX, YY, P, alpha=1.0, cmap='viridis', edgecolor='k');
```

## Multivariable Logistic Regression

```{python}
#| echo: True
#| code-fold: True

with sns.plotting_context("talk"):
  fig = plt.figure(figsize = (10, 7))
  ax = plt.axes(projection ="3d")
  ZZ = np.ones_like(XX) * 0.5 
  ax.plot_surface(XX, YY, ZZ,  zorder=1, edgecolor='k')
  ax.plot_surface(XX, YY, P, alpha=1.0, cmap='viridis', edgecolor='k');
```

## View Decision Boundary

```{python}
#| echo: True
#| code-fold: True
from sklearn.inspection import DecisionBoundaryDisplay
disp = DecisionBoundaryDisplay.from_estimator(
    logistic_model, X, response_method="predict", cmap=plt.cm.viridis, alpha=0.5
)
disp.ax_.scatter(X[:,0], X[:,1], c=y, cmap='viridis', edgecolor='k');
disp.ax_.set_xlabel("X")
disp.ax_.set_ylabel("Y")
```


# How it works {.smaller}


## Loss Function

::: incremental
- Lets create a likelihood function $L$ that is a function of our parameters $\beta$. We want to find parameters $\beta$ that make $L$ as big as possible.
- $L(\beta) = \sum_{i=1}^n \underbrace{y_i \cdot \sigma(\beta \; x_i)}_{y = 1} + \underbrace{(1-y_i) \cdot (1 - \sigma(\beta \; x_i))}_{y = 0}$ 
- Convert to a loss function $J$. We also take the logarithm. This ensures our cost function is convex. 
- $J(\beta) = -\sum_{i=1}^n y_i \cdot log(\sigma(\beta \; x_i)) + (1-y_i) \cdot log(\sigma(\beta \; x_i))$ 
- This does not have a closed form solution. AKA you can not just take derivative and solve the parameters $\beta$.
-  We have to use a numerical solver. For example, gradient descent.
:::

## Convex vs Non Convex

![](./lec12_images/convex_vs_nonconvex.png)

## Convex Function, Soccer Ball

![](./lec12_images/gradient_descent_example.gif)


## Gradient Descent

![](./lec12_images/example_gradient.gif)




<!-- https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d -->















