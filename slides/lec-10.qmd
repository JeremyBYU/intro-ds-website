---
title: "CISC482 - Lecture10"
subtitle: "Regression"
author: "Dr. Jeremy Castagno"
footer:  "[https://jeremybyu.github.io/intro-ds-website](https://jeremybyu.github.io/intro-ds-website/)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    show-slide-number: all 
    chalkboard: true
    code-copy: false
    revealjs-plugins:
      - noswipe
editor: visual
execute:
  freeze: auto

---


```{python}
#| context: setup
#| include: false
import pandas as pd
import seaborn as sns 
import numpy as np
from sklearn.datasets import make_blobs
from palmerpenguins import load_penguins, load_penguins_raw
df = load_penguins()
sns.set_style('whitegrid')
sns.set_context("poster", font_scale=1.25)
np.set_printoptions(precision=1, suppress=True, formatter={'float_kind': "{:.1f}".format})
pd.options.display.float_format = '{:,.2f}'.format
np.random.seed(1)
```

# Class Business


## Schedule

- Topic Ideas - Feb 22 @ Midnight
- Reading 5-1: Feb 22 @ 12PM, Wednesday
- Reading 5-2: Feb 24 @ 12PM, Friday
- Reading 5-3: Mar 01 @ 12PM, Wednesday

## Today

- Introduction to Regression
- Review Topic Ideas
- More Exploratory Data Analysis

# Introduction to Regression

## Input and Output

- An **input** feature takes values without being impacted by any other features.
- An **output** feature has values that vary in **response** to variation in some other feature(s).
  -  We often called this the **response** variable
- In an **experiment**, input features are often controlled by researchters, and output features are observed
- We often visualize these with scatter plots

## Example Data



```{python}
df.head()
```

## Scatter Plot

```{python}
sns.scatterplot(data=df, x='bill_length_mm', y='body_mass_g');
print(f"Correlation: {df.bill_length_mm.corr(df.body_mass_g):.2f}")
```


## Describing

- The direction: positive if larger values of one feature correspond to larger values of the other feature.
- The form: linear pattern or a nonlinear pattern.  Sometimes two features may not have an obvious form.
- The strength: how closely the observations in a scatter plot follow the form's pattern.

## Questions

![](./lec10_images/form_relationship.png){fig-align="center"}

## Correlation

- **Correlation** is a statistical measure that expresses the extent to which two variables are linearly related 
- They change together at a constant rate
-  Itâ€™s a common tool for describing simple relationships without making a statement about cause and effect.
- Range from -1 to 0 to +1

## Regression Model

- A **model** for an output feature $y$ using input feature(s) $X$ is a *function* $f(X)$ that **predicts** an expected value $\hat{y}$ for a given value of $X$.and
- $\hat{y} = f(X)$
- A **regression** model is one that has a **numerical** output feature and input features.

## Regression Example

- What is the error of our model (Residual)

![](./lec10_images/motivation_plot.png){fig-align="center"}


# Simple Linear Regression

## Main Equation

::: incremental
- Set of data: $\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}$
  - x: input, y: output
- Model: $\hat{y} = \beta_0 + \beta_1 x + \epsilon$
  - $\hat{y}$ = prediction
  - $\beta_0$ = y-intercept 
  - $\beta_1$ = slope 
  - $\epsilon = error$
:::


## Visual Explanation


![](./lec10_images/residual_error.png){fig-align="center"}

::: callout-tip
How can we formalize what we are **optimizing** for?
:::

## Formulation

:::{.smaller}
::: incremental
- We want to **find** the $\beta_0$ and $\beta_1$ parameters that reduce the combined error.
- A *loss function* that takes as input our parameters and whose output is our model error.
-  $f(\beta_0, \beta_1) = \sum\limits_{i=1}^{n}[y_i - \hat{y}_i]^2 = \sum\limits_{i=1}^{n}\epsilon_i^2$
- Sum of the Squared Residuals (SSR/SSE). Big or small?
:::
:::

::: fragment
$$
SSR = \sum\limits_{i=1}^{n}[y_i - \hat{y}_i]^2 = [y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)]^2 \\ 
= [y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i]^2
$$
:::

## Derivation

- Will not be going through the derivation
- Basic idea is take the partial derivaitve of the funciton with respect to the parameters and set equal to 0.
- Do a bunch of algebra and you arrive at some nice equations
- Full derivation is [here](http://spia.uga.edu/faculty_pages/mlynch/teaching/ols/OLSDerivation.pdf) and [here](https://sta210-s22.github.io/website/supplemental/slr-derivations.html)

## Classic Stats Result

- $\beta_1 (slope) = \frac{\sum\limits_{i=1}^{n}[(x_i-\bar{x})(y_i- \bar{y})]}{\sum\limits_{i=1}^{n} (x_i - \bar{x})^2}$ 
- $\beta_0$ (intercept) - $\bar{y} - \beta_1 \bar{x}$ 

```{python}
#| echo: True
df_r = df[['bill_length_mm', 'body_mass_g']].dropna()
x = df_r.bill_length_mm
y = df_r.body_mass_g
x_bar = x.mean()
y_bar = y.mean()
print(f"Xbar: {x_bar:.1f}")
print(f"Ybar: {y_bar:.1f}")
```

## Computation

```{python}
#| echo: True
numerator = np.sum((x - x_bar) * (y - y_bar))
denominator = np.sum((x - x_bar)**2)
slope = numerator / denominator
intercept = y_bar - slope * x_bar
print(f"Slope: {slope:.1f}; \nIntercept: {intercept:.1f}");
```

```{python}
ax = sns.scatterplot(data=df, x='bill_length_mm', y='body_mass_g');
point_y = x.min() * slope + intercept
ax.axline((x.min(), point_y), slope=slope, color='r', linestyle='--', label='Regressed Line');
```

## Another Classic Stats Result

$$
\begin{aligned}\hat{\beta}_1 &= \frac{\text{Cov}(x,y)}{s_x^2}\end{aligned}
$$ 

The correlation between $x$ and $y$ is $r = \frac{\text{Cov}(x,y)}{s_x s_y}$.
Thus, $\text{Cov}(x,y) = r s_xs_y$.
Plugging this into above, we have

$$
\hat{\beta}_1 = \frac{\text{Cov}(x,y)}{s_x^2} = r\frac{s_ys_x}{s_x^2} = r\frac{s_y}{s_x}
$$

$\beta_0$ (intercept) - $\bar{y} - \beta_1 \bar{x}$ 

## Computation

```{python}
#| echo: True
r = df_r.bill_length_mm.corr(df_r.body_mass_g)
slope = r * (df_r.body_mass_g.std() / df_r.bill_length_mm.std())
intercept = y_bar - slope * x_bar
print(f"Slope: {slope:.1f}; \nIntercept: {intercept:.1f}")
```

## Another Full Example

![](./lec10_images/example_regression.png){fig-align="center"}

## Matrix Math Result

$$
\begin{align*}
A = \begin{bmatrix}
x_1 & 1\\
x_2 & 1 \\ 
... & ... \\
x_n & 1
\end{bmatrix}
\qquad
y = \begin{bmatrix}
y_1 \\
y_2 \\
... \\
y_n
\end{bmatrix}
\end{align*}
\qquad
\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix}
$$

$$
\hat{y_1} = x_1 \cdot \beta_0 + 1 \cdot \beta_1 \\
... \\
\hat{y} = A \beta 
$$

::: fragment
$$
\beta = \begin{bmatrix} \beta_0 \\ \beta_1 \end{bmatrix} = (A^TA)^{-1} A^Ty
$$
:::
## 

## Computation of Matrix Result 

```{python}
#| echo: True
n = len(df_r)
print(x.shape, type(x))
x = x.values
print(x.shape,type(x))
x = np.expand_dims(x, axis=1) # convert to matrix
print(x.shape)
A = np.append(x, np.ones(shape=(n, 1)), axis=1) # add one column
print(A.shape)
y = y.values
print(x[:5, :]) # sample of matrix
# intercept = y_bar - slope * x_bar
# print(f"Slope: {slope:.1f}; \nIntercept: {intercept:.1f}")
```

## Computation of Matrix Result 

```{python}
#| echo: True
beta = np.linalg.inv(A.T @ A) @ A.T @ y
print(f"Slope: {beta[0]:.1f}; \nIntercept: {beta[1]:.1f}")
```
 

<!-- ## Mean Squared Error

- $SSE = \sum\limits_{i=1}^{n}[y_i - \hat{y}_i]^2$
- $SSE/n$ is called mean squared errors or (MSE)
- $\sqrt{SSE/n}$ - Root Mean Squared Error (RMSE) -->


# Class Activity

## Class Activity

[Practice Regression](https://colab.research.google.com/drive/1S9CF1esM6dci019I2WzaFlt1Nt4AcBJi?usp=sharing)









