---
title: "CISC482 - Lecture24"
subtitle: "Principal Component Analysis"
author: "Dr. Jeremy Castagno"
footer:  "[https://jeremybyu.github.io/intro-ds-website](https://jeremybyu.github.io/intro-ds-website/)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    show-slide-number: all 
    chalkboard: true
    code-copy: false
    revealjs-plugins:
      - noswipe
editor: visual
execute:
  freeze: auto

---


```{python}
#| context: setup
#| include: false
import pandas as pd
import seaborn as sns 
import numpy as np
from sklearn.datasets import make_blobs
from palmerpenguins import load_penguins, load_penguins_raw
import matplotlib.pyplot as plt
import matplotlib
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.datasets import make_blobs, make_regression
from sklearn.model_selection import train_test_split
df = load_penguins()
df.dropna(inplace=True)
np.set_printoptions(precision=1, suppress=True, formatter={'float_kind': "{:.1f}".format})
pd.options.display.float_format = '{:,.2f}'.format
np.random.seed(10)
font = {'family' : 'normal',
        'weight' : 'bold',
        'size'   : 16}

matplotlib.rc('font', **font)
```


```{python}
def show_images_by_digit(digit_to_see, images, y):
    if digit_to_see in list(range(10)):
        indices = np.where(y == digit_to_see) # pull indices for num of interest
        for digit_num in range(0,50): 
            plt.subplot(5,10, digit_num+1) #create subplots
            #reshape images
            image = images[indices][digit_num]
            if image.shape != (8,8):
              image = image.reshape(8,8)
            plt.imshow(image, cmap='gray') #plot the data
            plt.xticks([]) #removes numbered labels on x-axis
            plt.yticks([]) #removes numbered labels on y-axis

```


```{python}

def plot_components(X, y):
    x_min, x_max = np.min(X, 0), np.max(X, 0)
    X = (X - x_min) / (x_max - x_min)
    plt.figure(figsize=(10, 6))
    for i in range(X.shape[0]):
        plt.text(X[i, 0], X[i, 1], str(y[i]), 
                 color=plt.cm.Set1(y[i]), 
                 fontdict={'size': 15})

    plt.xticks([]), plt.yticks([]), plt.ylim([-0.1,1.1]), plt.xlim([-0.1,1.1])
    plt.xlabel("PC1")
    plt.ylabel("PC2")

```

# Class Business


## Schedule

- Returning Draft Reports
- [Final Report](https://jeremybyu.github.io/intro-ds-website/project-description.html#final-report) - April 28 @ Midnight
- [Final Presentation](https://jeremybyu.github.io/intro-ds-website/project-description.html#slides) - May 5, Final

## Today

- Recap Peer Review
- Review Hierarchal Clustering
- Principal Component Analysis

# Peer Review

## My Review

- Overall we did a great job! You can find my and your peers feedback on Brightspace.
  - Its also inside your *shared* Google Drive folder under **PeerReview**
- I particularly appreciated the detailed walkthrough that many of you did.
- It was very interesting seeing all your ideas!
- I did see a few common errors that I wanted to bring up.

## Common Issues 1

- Don't have empty cells.
- Donâ€™t have grammatical errors.
- Label all your axes in your figures.
- The primary goal of this paper is to create a **predictive model**. 
  - You need to be clear in your **introduction** what your response variable is and if you are doing **classification** or **regression**.

## Common Issues 2

- Be sure to have a good amount of detail in your report that explains **WHY** you are doing what you are doing.
- I recommend making tables showing your results. You can do this by creating a data frame of your results or by using [this website]( https://www.tablesgenerator.com/markdown_tables ) to make markdown tables.
- Please review [this document](https://colab.research.google.com/github/bebi103a/bebi103a.github.io/blob/master/lessons/00/intro_to_latex.ipynb) to learn how to write equations in google collab.

## Common Issues 3

- If you are doing regression, please make [Residual Plots](https://www.scikit-yb.org/en/latest/api/regressor/residuals.html) and report $R^2$ metric.
- If you are doing classification, please report metrics such as accuracy, precision, recall, and f1 score.


# Hierarchical clustering


## Types of Clustering

- **Agglomerative** hierarchical clustering is _______
- **Divisive** hierarchical clustering is ____

## Measures of Similarity

- The **single linkage** method calculates the distance between a pair of samples, one from each cluster, that are the _____.
- The **complete linkage** method calculates the distance between a pair of samples, one from each cluster, that are the _____.
- The **centroid linkage** method calculates the distance between the ________ of two clusters.

## Questions - 1

![](./lec23_images/question_1_single.png){fig-align="center"}

:::question
Which two samples should be used to determine similarity using the Single linkage?
Complete Linkage?
:::

## Questions - 1

![](./lec23_images/question_1_single.png){fig-align="center"}

:::question
Which two samples should be used to determine similarity using the Single linkage?
Complete Linkage?
:::


## Terminology

- A _______ is a ________ that shows the *order* in which clusters are grouped together and the *distances* between clusters.
- Read it from BOTTOM up!

##

::: columns

::: {.column width="50%"}


- A _____ is a branch of a dendogram/vertical line.
- A _____ is a horizontal line that connects two ______, height gives the distance between clusters.
- A _____ is the terminal end of each _______ in a dendrogram, which represents a single sample.


:::

::: {.column width="50%"}

:::fragment

![](./lec23_images/dendogram.png)
:::



:::

:::


## Question Threshold

![](./lec23_images/question_treshold.png)

:::question
::: {.incremental .medium-font}
- How many total samples?
- How many clusters would there be with dashed blue line?
- How many clusters would there be with dashed red line?
:::
:::


# Principal Component Analysis


## Terms

- **Principal component analysis**, or PCA, is a **dimensionality** reduction technique.
- It can be used to **compress** data. 
- Lets say we have $n=10$ points that such that  $x \in\mathbf{R}^2$ 
- PCA *can* reduce the data be $n=10$ points inside $R^{1}$
- It does this by finding the component, or feature *vector* that contains the largest variability
- Lets looks at an example


## Example 1 Start

![](./lec24_images/pca_example1.png)

Find me a **vector** or **axis** that when you project the data to it maximizes the variance 

## Projection

![](./lec24_images/projection.png)

## Example 1 Animation


![](./lec24_images/pca_anim.gif)


## Example 2 Start


::: columns

::: {.column width="70%"}

```{python}
#| echo: true
#| code-fold: true
x = np.arange(0, 10) + np.random.randn(10) * 0.1
y = x * 1 + np.random.randn(10) * 0.3
sns.scatterplot(x=x,y=y)
```
:::

::: {.column width="30%"}

Find me a **vector** or **axis** that when you project the data to it maximizes the variance 

:::

:::

## Example 2 - First Principle Component


::: columns

::: {.column width="70%"}

```{python}
#| echo: true
#| code-fold: true
x = np.arange(0, 10) + np.random.randn(10) * 0.1
y = x * 1 + np.random.randn(10) * 0.3
ax = sns.scatterplot(x=x,y=y)
ax.plot([0, 10], [0, 10], ls='dashed', c='r')
```
:::

::: {.column width="30%"}

Do you see how this vector will maximize the variance? The vector is $[1, 1]$ or $[.707, .707]$ when normalized. 

:::

:::

## Intuition

![](./lec24_images/pca_intuition.png)


## Back to Example 2

::: columns

::: {.column width="50%"}
```{python}
#| echo: true
#| code-fold: true
X = np.column_stack([x, y])
df = pd.DataFrame(X, columns=['x', 'y'])
df
```
:::

::: {.column width="50%"}

::: fragment
```{python}
#| echo: true
#| code-fold: true

pc1 = np.array([[.707], [.707]])
print(f"PC1 = {pc1.flatten()}")
pc1_vals = X @ pc1
df = pd.DataFrame(pc1_vals, columns=['PC1'])
df
```
:::
:::

:::


## Compression from Example 2

::: columns

::: {.column width="50%"}
```{python}
#| echo: true
#| code-fold: true
sns.scatterplot(x=x,y=y, label="Raw");
```
:::

::: {.column width="50%"}

```{python}
#| echo: true
#| code-fold: true
points = pc1.flatten() * pc1_vals
ax = sns.scatterplot(x=x,y=y, label='Raw')
ax.scatter(x=points[:, 0],y=points[:, 1], label='Compressed')
plt.legend();
```
:::

:::


# Higher Dimensions

## 3D

```{python}
#| echo: true
#| code-fold: true
x = np.arange(0, 10) + np.random.randn(10) * 0.1
y = np.arange(0, 10) + np.random.randn(10) * 0.1
z = x * 1 + y * 2 + np.random.randn(10) * 1
fig = plt.figure()
ax = fig.add_subplot(projection='3d')

X = np.column_stack([x, y, z])

ax.scatter(x, y, z, marker='o')
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
```

## PCA

```{python}
#| echo: true
#| code-fold: true
#| output-location: column
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
# fit and transform data
X_pca = pca.fit_transform(X)
print(X_pca.shape)
X_comp = pca.inverse_transform(X_pca)

fig = plt.figure()
ax = fig.add_subplot(projection='3d')

ax.scatter(x, y, z, marker='o', label='Raw')
ax.scatter(X_comp[:, 0], X_comp[:, 1], X_comp[:, 2], marker='x', label='compressed')
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')

```

# Wine Data Set

## The Data

```{python}

import pandas as pd

df = pd.read_csv('https://archive.ics.uci.edu/ml/'
                      'machine-learning-databases/wine/wine.data',
                      header=None)


FeatureNames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', \
'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] 
print(f"Features: {FeatureNames}")
df = df.rename(columns={0: 'group'})
print(f"Classes: {np.unique(df.group, return_counts=True)}")
df.head()
```

Can we visualize this data? Why or why not?

## Split Data and Scale

```{python}
#| echo: true
#| code-fold: true
from sklearn.preprocessing import StandardScaler

df_train, df_test = train_test_split(
    df, test_size=0.30, random_state=0)

y_train = df_train['group']
y_test = df_test['group']

scaler = StandardScaler().set_output(transform="pandas")
scaled_X_train = scaler.fit_transform(df_train.iloc[:, 1:])
scaled_X_test = scaler.fit_transform(df_test.iloc[:, 1:])
scaled_X_train.head()
```

$z = (x - \mu) / \sigma$

## PCA

::: columns

::: {.column width="70%"}
```{python}
#| echo: true
#| code-fold: true
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(scaled_X_train)
plt.scatter(X_train_pca[:, 0], X_train_pca[:,1], c=y_train, cmap='viridis', edgecolor='k');
```
:::

::: {.column width="30%"}
```{python}
print(X_train_pca)
```
:::
:::





## Separate with Logistic Regression


```{python}
#| echo: true
#| code-fold: true
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(C=1e5)
model.fit(X_train_pca, y_train)

disp = DecisionBoundaryDisplay.from_estimator(
    model, X_train_pca, response_method="predict", cmap=plt.cm.viridis, alpha=0.5, xlabel='PC1', ylabel='PC2'
)
disp.ax_.scatter(X_train_pca[:, 0], X_train_pca[:,1], c=y_train, cmap='viridis', edgecolor='k');

```

## Why is this a big deal?

- We just separated this think with only **2** dimensions! We started with 13 Dimensions
- Remember we did **not** reduce features. We **extracted** two new features, PC1 and PC2 that best expalin the data
- PC1 is a combination of meany of the 13 features
```{python}
print(f"PC1 : {pca.components_[0,:]}")
```

# Even higher Dimensions (IMAGES!)


## MNIST Digits - 8 X 8 Images


```{python}
#| echo: true
#| code-fold: true
from sklearn.datasets import load_digits
mnist = load_digits()
X = mnist.data
y = mnist.target
images = mnist.images

fig, axes = plt.subplots(2, 10, figsize=(16, 6))
for i in range(20):
    axes[i//10, i %10].imshow(images[i], cmap='gray');
    axes[i//10, i %10].axis('off')
    axes[i//10, i %10].set_title(f"target: {y[i]}")
    
plt.tight_layout()

```

## MINST Data


```{python}
#| echo: true
#| code-fold: true
pd.DataFrame(X)
```


## More Images - Digit 1

```{python}
show_images_by_digit(1, images, y)
```

## More Images - Digit 8

```{python}
show_images_by_digit(8, images, y)
```

## Apply PCA


```{python}
#| code-fold: true
#| echo: true
pca = PCA(n_components=64)
pca_data = pca.fit_transform(X)
percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)
cum_var_explained = np.cumsum(percentage_var_explained) * 100

plt.plot(range(1,65), cum_var_explained, linewidth=2)
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Variance Explained");
plt.xticks(np.arange(0, 64, 4));
plt.ylim(0,100);
```

## Compression - Using 10 PC - Digit 8


```{python}

pca_2 = PCA(n_components=10)
pca_2comp = pca_2.fit_transform(X)
compressed_images = pca_2.inverse_transform(pca_2comp)

show_images_by_digit(8, compressed_images, y)

```


## Compression - Using 2 PC - Digit 8


```{python}

pca_2 = PCA(n_components=2)
pca_2comp = pca_2.fit_transform(X)
compressed_images = pca_2.inverse_transform(pca_2comp)

show_images_by_digit(8, compressed_images, y)

```

## Plot PC1 and PC2 

```{python}
pca_2comp = pca_data[:, :2]
plot_components(pca_2comp, y)

```
