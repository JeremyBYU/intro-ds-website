---
title: "CISC482 - Lecture14"
subtitle: "Evaluating Model Performance 1"
author: "Dr. Jeremy Castagno"
footer:  "[https://jeremybyu.github.io/intro-ds-website](https://jeremybyu.github.io/intro-ds-website/)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    show-slide-number: all 
    chalkboard: true
    code-copy: false
    revealjs-plugins:
      - noswipe
editor: visual
execute:
  freeze: auto

---


```{python}
#| context: setup
#| include: false
import pandas as pd
import seaborn as sns 
import numpy as np
from sklearn.datasets import make_blobs
from palmerpenguins import load_penguins, load_penguins_raw
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, LogisticRegression
df = load_penguins()
df.dropna(inplace=True)
np.set_printoptions(precision=1, suppress=True, formatter={'float_kind': "{:.2f}".format})
pd.options.display.float_format = '{:,.2f}'.format
np.random.seed(10)
```

# Class Business


## Schedule

- Reading 6-1:  Mar 08 @ 12PM, Wednesday
- Reading 6-2:  Mar 10 @ 12PM, Friday
- [Proposal](https://jeremybyu.github.io/intro-ds-website/project-description.html#project-proposal): Mar 22, Wednesday
- HW5 - Mar 29 @ Midnight, Wednesday

## Today

- Overfit/Underfit
- Bias/Variance Trade off
- Regression Metric
- Binary Classification Metrics


# Model Error

## Modelling

- We *approximate* an output feature $y$, using input features $X$ with function $f$ such that $\hat{y} = f(X)$
  - Example: Predicting penguin body mass by bill length
  -  $\text{body mass} = \hat{y} = mx + b$
- We have to choose $f$ and $X$: simple linear model, polynomial model, multiple linear regression, logistic regression, etc.
- Example is $\hat{y} = \beta_0 + \beta_1 x$ or is $\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2$ 


```{python}
n = 16
x = np.linspace(0, 30, num=n)
noise = np.random.normal(loc=0, scale=4, size=n)
y = -0.1* (x - 15)**2 + 10 + noise
```

## Underfit

:::fragment
- **Underfit** - model is too simple to fit the data well.
:::

```{python}
sns.regplot(x=x,y=y, ci=None, scatter_kws={"color": "black"}, line_kws={"color": "red"});
```

## Underfit Problems

- An underfit model will miss the underlying trend 
- Will score poorly in metrics

## Overfit

:::fragment
- **Overfit** - model is too complex to fit the data well.
:::

```{python}
coeffs = np.polyfit(x, y, deg=15)
regressed_fn = np.poly1d(coeffs)
y_hat = regressed_fn(x)
ax = sns.scatterplot(x=x,y=y, color="k");
ax.plot(x, y_hat, color='red');
```

## Overfit Problems

- Fitting the data too closely
- Incorporating too much noise (meaningless variation)
- Misses the general trend of the data despite scoring well in some metrics
- In fact, what is the error for this model?

## Optimal

- This model would be best fit with a quadratic model

```{python}
coeffs = np.polyfit(x, y, deg=2)
regressed_fn = np.poly1d(coeffs)
y_hat = regressed_fn(x)
ax = sns.scatterplot(x=x,y=y, color="k");
ax.plot(x, y_hat, color='red');
```

## Note

::: callout-important
A model that is overfit or underfit is a **bad** predictor of outcomes outside of the data set and should not be used. In the field of data science, models tend to be overfit, so model selection techniques focus on choosing the **least complex** model that captures the general trend.
:::

## Find Most Underfit and Most Overfit

![](./lec14_images/overfit_underfit.png){fig-align="center"}


# Bias and Variance

## Breaking down Error

- The total error of a model is how much the observed values differ from predicted values. Total error is broken down into three pieces: 
  - **Bias** - model's prediction differs from the observed values due to the *assumptions* built into the model.
  - **Variance** - spread/variance of predictions 
  - **Irreducible error** -  error inherent to the data (noise)

## Visual Explanation

![](./lec14_images/bias_variance.png){fig-align="center"}


## Bias-Variance Tradeoff


:::incremental
- Choosing a more complex model (more features, a more complicated mathematical expression, etc.) means the model's predictions are closer to the observed sample values, which decreases the bias.
- However, doing so makes the model's predictions more spread out to meet the observed values, increasing the variance.
- An optimal model should be just complex enough to capture the general trend of the data (low bias) without incorporating too much of the noise from the sample (low variance).
:::

## Visual Example

![](./lec14_images/bias_variance_tradeoff.png){fig-align="center"}

## Problem 1

![](./lec14_images/example_problem_bias_variance.png){fig-align="center"}

## Problem 2

![](./lec14_images/example_problem_bias_variance_2.png){fig-align="center"}


# Regression Metrics

## Dataset

- We will be using the penguin dataset

```{python}
#| echo: True
#| code-fold: True
X = df['bill_length_mm'].values[:, np.newaxis]
y = df['body_mass_g'].values
df.head()
```

## Two statistics {.smaller}

-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)

    $$
    R^2 = \frac{\text{variation explained by regression}}{\text{total variation in the data}} = \frac{\sum (\hat{y}_i - \bar{y})^2}{\sum (y_i - \bar{y})^2} \\
      R^2 = 1 - \frac{\sum (\hat{y}_i - y_i)}{\sum (y_i - \bar{y})^2}
    $$

-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)

$$
RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}}
$$

. . .

::: question
What indicates a good model fit?
Higher or lower $R^2$?
Higher or lower RMSE?
:::

## R-squared {.smaller}

-   Ranges between 0 (terrible predictor) and 1 (perfect predictor), Unitless
-   Calculate with `model.score(X, y)`:

```{python}
#| echo: true
model = LinearRegression()
model.fit(X,y)
r_squared = model.score(X,y)
print(f"R^2 = {r_squared:.2f}")
```

```{python}
#| echo: true
x = X[:, 0]
regressed_fn = np.poly1d(np.polyfit(x, y, 1))
y_hat = regressed_fn(x)
r_squared = np.sum(np.square(y_hat - y.mean())) / np.sum(np.square(y.mean() - y))
print(f"R^2 = {r_squared:.2f}")
```

## More Examples

```{python}
#| echo: true
r = np.corrcoef(y, y_hat)[0, 1] # matrix, get first row second column
r_squared = r ** 2
print(f"R^2 = {r_squared:.2f}")
```

## Graph

```{python}
#| echo: true
ax = sns.scatterplot(x=X[:,0], y=y, color="black")
ax.plot(X, model.predict(X), color="red", linewidth=3)
```

## $R^2$ Example

![](./lec14_images/r_2_example.png)

## Interpreting R-squared {.smaller}

::: poll

The $R^2$ of the model for predicting penguin mass from bill length is 25%.
Which of the following is the correct interpretation of this value?

::: nonincremental
-   Bill Length correctly predicts 25% of penguin mass.
-   25% of the variability in penguin mass can be explained by bill length.
-   25% of the time penugin mass can be predicted by bill length.
:::
:::


## RMSE {.smaller}

-   Ranges between 0 (perfect predictor) and infinity (terrible predictor)
-   Same units as the outcome variable
-   Calculate with `means_squared_error(y_true, y_pred)`:

    ```{python}
    #| echo: true
    from sklearn import metrics
    rmse = metrics.mean_squared_error(y, model.predict(X))
    print(f"RMSE: {rmse:.2f}")
    ```

-   The value of RMSE is not very meaningful on its own, but it's useful for comparing across models.
-  Comparing a model that uses bill length for a predictor or using flipper length


## RMSE Example

![](./lec14_images/mse_comparison.png)


# Binary Classification Metrics

## True/False Positive/Negative

* True Positive (**TP**) is an outcome that was correctly identified as positive 
* True Negative (**TN**) is an outcome that was correctly identified as negative.
* False Positive (**FP**) is an outcome that was **in**correctly identified as positive 
* False Negative (**TN**) is an outcome that was **in**correctly identified as negative


## Confustion Matrix

|                   | Positive (predicted) | Negative (predicted) |
|:-----------------:|:--------------------:|:--------------------:|
| Positive (actual) | 170                  |          21          |
| Negative (actual) | 1                    |          377         |


## Metrics

- Accuracy - useful
- Precison - very useful
- Recall - very useful

## Accuracy

::: incremental
- Accuracy: $\frac{\text{# Correctly Predicted}}{\text{Total}}$
- $\frac{TP + TN}{TP + TN + FP + FN}$
:::

## Precision

::: incremental
- Tell how precise your prediction is
- $\frac{TP}{TP + FP}$
- The higher this number, the less False Positives you have
- My research - Identifying an emegency landing location nearby. A precison gives confidence that it truly is safe to land at the location the model predicts.
:::


## Recall

::: incremental
- the proportion of positives that were correctly predicted
- $\frac{TP}{TP + FN}$
- The higher this number, the less False Negative you have. 
- My Research - A high recall means I found nearly *all* the rooftops in the city that you could land on. 
:::


## Example Question


|                   | Positive (predicted) | Negative (predicted) |
|:-----------------:|:--------------------:|:--------------------:|
| Positive (actual) | 170                  |          21          |
| Negative (actual) | 1                    |          377         |

::: question
What is the Accuracy, Precision, and Recall?
:::

# Tradeoff Between Precision and Recall

## Tradeoff {.smaller}

- In logisitc regression you specify a **threshold** to use a prediciton. By deafult we use 0.5 or 50%. But that is arbitary and you move that threshold

![](./lec14_images/threshold_05.png){fig-align="center"}

## Low Threshold

![](./lec14_images/low_threshold.png){fig-align="center"}

## High Threshold

![](./lec14_images/threshold_high.png){fig-align="center"}







