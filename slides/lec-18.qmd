---
title: "CISC482 - Lecture18"
subtitle: "Supervised Learning - Naive Bayes"
author: "Dr. Jeremy Castagno"
footer:  "[https://jeremybyu.github.io/intro-ds-website](https://jeremybyu.github.io/intro-ds-website/)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    show-slide-number: all 
    chalkboard: true
    code-copy: false
    revealjs-plugins:
      - noswipe
editor: visual
execute:
  freeze: auto

---


```{python}
#| context: setup
#| include: false
import pandas as pd
import seaborn as sns 
import numpy as np
from sklearn.datasets import make_blobs
from palmerpenguins import load_penguins, load_penguins_raw
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.datasets import make_blobs, make_regression
df = load_penguins()
df.dropna(inplace=True)
np.set_printoptions(precision=1, suppress=True, formatter={'float_kind': "{:.1f}".format})
pd.options.display.float_format = '{:,.2f}'.format
np.random.seed(10)
```

# Class Business


## Schedule

- Reading 7-2:  Mar 31 @ 12PM, Friday
- Reading 7-3:  Apr 5 @ 12PM, Wednesday
- [HW6]() - Working on it... April 12 @ Midnight, Wednesday

## Today

- Short Review
- Naive Bayes
- Class Activity

# Review

## Terms

- An instance is **labeled** if 
- **Supervised learning** is 
- Regression vs Classification?
- What does KNN stand for?

## Metric Space
A **metric space** is an unordered pair $(M, d)$ where $M$ is a set and $d$ is a metric on $M$, a function where $d: M \times M \rightarrow \mathbb{R}$. Where $d$ satisfies the following axioms for all points $x,y,z \in M$

1. The distance from a point to itself is 0: $d(x,x) = 0$
2. The distance between any two *distinct* points is always positive: If $x \neq y, then \; d(y,x) > 0$
3. The distance from x to y is always the same distance from y to x: $d(x,y) = d(y,x)$
4. Triangle Inequality holds: $d(x,z) \geq d(x,y) + d(y, z)$



# Naive Bayes

## Basic Idea

- Naive Bayes classification is a supervised learning classifier that uses the number of times a feature occurs in each possible class to estimate the **likelihood** an instance is in the class. 
- Naive Bayes is often used for applications with large amounts of text data. 
  - identifying the author of a new document based on prior documents with known authors.
  - detecting spam emails

## Motivating Example


::: columns


::: {.column width="50%"}

Professor Castagno,
I have a question about when HW6 is due.
Is it due on Wednesday before class or at midnight. Thank you for your help!


:::

::: {.column width="50%"}

Proffesssor Castagno,
Congratulations!!!1 You won free trip to B@hamas. Give credit card to confirm sh1pping of tickets.  Free no, expense trip! Respond now!

:::

:::

::: question
Which one is spam? How do you *know*?
:::

## Example Problem Focus

- For the rest of the class we will be doing examples on spam classification
- This means we are give a document that has **words** in it and want to predict the **class**: *ham* or *spam*
- All machine learn learning models need features to learn from, and we expect these to be numbers
- We need to transform our document into a feature vector: $X$

## Two Approaches

There are two main approaches to Naive Bayes and they boil down to how to transform the document. But first some definitions:

- We denote $D = \{d_1, d_2, ..., d_i, ..., d_m \}$ as the set of all documents. Often called the corpus. $\lvert D \rvert = m$. 
- Every $d_i$ is composed of *tokens* (words) where spaces are ignored
- The set of **all** unique tokens from $D$ is called the *vocabulary*. Lets denote the cardinality of this set to be $n$. So there are $n$ unique words in the corpus.


## Example

::: question
D1 - The best part of waking up is Folgers in your cup. Fill your cup up with Folgers.

D2 - You are all great students! I am so proud to be a teacher of great students!
:::

```{python}
d_1 = "The best part of waking up is Folgers in your cup. Fill your cup up with Folgers."
d_2 = "You are all great students! I am so proud to be a teacher of great students!"
docs = [d_1, d_2]
```

```{python}
#| echo: True
import string
vocabulary = set(d_1.split(' ') + d_2.split(' '))
print(vocabulary)
print(len(vocabulary))
```

## Preprocess Text {.smaller}

- Remove Punctuation
- Leading and Ending White space - ' Hey '
- Replace common occurring text patterns with a single word, Regular Expression
  -'http://spam.me' --> url
  - '$' 'Â£'& --> 'mnsymb'
  - '55555' -- 'shrtcode'
  - '867-5309' --> 'phonenumber'
  - '88' --> 'number'
- Lower case
- Port Stemmer - 'testing' -> 'test'
- Remove Stop words - 'the', 'a'

<!-- vocabulary = [word.translate(str.maketrans('', '', string.punctuation )) for word in vocabulary]
vocabulary = set(vocabulary) -->

## Creating Features Vectors

Every document $d_i$ has a feature vector, $x_i$, that has $n$ elements in it. Each element will represent information about a unique vocab word in our corpus.

- Multinomial Naive Bayes
  - Each document is broken up to tokens. The feature vector is then the **frequency** count of the vocabulary words in the token set.
- Bernoulli Naive Bayes 
  - Each document is broken up into tokens. The feature vector is then a boolean variable if the the vocab word was found in the document.


## Multinomial Feature Vector

```{python}
#| echo: True
#| code-fold: true
from sklearn.feature_extraction.text import CountVectorizer
vectorizer_m = CountVectorizer()
X_m = vectorizer_m.fit_transform(docs)
print(f"Vocab: {len(vectorizer_m.get_feature_names_out())}")
print(vectorizer_m.get_feature_names_out())
print("Feature Vector: ")
print(X_m.toarray())
```


## Bernoulli Feature Vector

```{python}
#| echo: True
#| code-fold: true
vectorizer_b = CountVectorizer(binary=True)
X_b = vectorizer_b.fit_transform(docs)
print(f"Vocab: {len(vectorizer_b.get_feature_names_out())}")
print(vectorizer_b.get_feature_names_out())
print("Feature Vector: ")
print(X_b.toarray())
```

## Which one to use?

- Multinomial is the more complex model. Bernoulli is the simpler model
- Less Data -> Bernoulli -> Less overfitting
- Alot of Data -> Multinomial
- Zybook only shows Multinomial

# Naive Bayes - How to use Feature Vectors?


## Bayes Models

::: fragment
$P(H|\textbf{D}) = P(H ) \frac{P(\textbf{D} | H)}{P(\textbf{D})}$
:::

::: fragment
$P(C = Spam|\textbf{X}) = \underbrace{P(C = Spam)}_{Prior} \frac{P(\textbf{X} | C = Spam)}{P(\textbf{X})}$
:::

::: fragment
$P(C = Spam|\textbf{X}) = P(C = Spam) \frac{\underbrace{P(\textbf{X} | C = Spam)}_{Likelihood}}{P(\textbf{X})}$
:::

:::fragment
Remember that $\textbf{X} = x_1, x_2, ... x_n$, one for each vocaublary word. Mulitnomial $x_i$ is the _____ and for bernoulli it is the _____.
:::

## The Prior

$P(C = Spam)$ = ?

**Irrepspective** of data, what is the probability that this is a spam document. 


:::fragment
$P(C = Spam)$ = $\frac{\text{# Spam Documents}}{\text{# All Documents}} = \frac{m_s}{m}$
:::

:::fragment
$P(C = Ham)$ = $1 - P(C = Spam)$
:::


## The Likelihood (the hard one)

$P(\textbf{X} | C = Spam) = P(x_1, x_2, ... x_i, x_n | Spam)$

This is a very large joint probability! Very difficult to compute! But we have some tricks...

::: fragment
$P(\textbf{X} | Spam) = P(x_1 | x_2, ... , x_n, Spam) P(x_2, ... x_n | Spam)$
:::

::: fragment
$P(\textbf{X} | Spam) = P(x_1 | x_2, ... , x_n, Spam) P(x_2 | x_3, ...,  Spam) P(x_3, ... x_n | Spam)$
:::

:::fragment
This is still just incredibly difficult to compute.... But we can make a **naive** assumption.
:::

## The Naive Assumption

$P(x_1 | x_2, ..., x_n, Spam ) = P(x_1 | Spam)$

::: incremental
- What is this assumption saying? 
- Is it true?
- Is it useful?
:::

::: fragment
$P(\textbf{X} | Spam) = \Pi P(x_i | Spam)$
:::

## Calculating $P(x_i | C=Spam)$, Bernoulli

- Looking at only *spam* documents
- Number of *spam* **documents** where $x_i$ appeared = $x_{i}^s$
- Total number of *spam* **documents** = $m_s$
- $P(x_i | C=Spam) = \frac{x_{i}^s}{m_s}$

## Calculating $P(x_i | C=Ham)$, Bernoulli

- Looking at only *ham* documents
- Number of *ham* **documents** where $x_i$ appeared = $x_{i}^h$
- Total number of *ham* **documents** = $m_h$
- $P(x_i | C=Ham) = \frac{x_{i}^h}{m_h}$


## Putting it all together

:::fragment
$P(C=Spam | \textbf{X}) = P(C=Spam)\dfrac{P(\textbf{X} | C=Spam)}{P(\textbf{X})}$
:::

:::fragment
$P(C=Spam | \textbf{X}) \propto P(C=Spam) \; P(\textbf{X} | C=Spam)$
:::

:::fragment
$P(C=Spam | \textbf{X}) \propto P(C=Spam) \; \Pi P(x_i | C=Spam)$
:::


:::fragment
$P(C=Spam | \textbf{X}) \propto \dfrac{m_s}{m} \; \Pi \dfrac{x_{i}^s}{m_s}$
:::

::: fragment
::: callout-warning
Anyone see a problem here? What happens if a vocab word does not appear in the document?
:::
:::

## Laplacian Smoothing for Naive Bayes


- $P(C=Spam | \textbf{X}) \propto \dfrac{m_s}{m} \; \Pi \dfrac{x_{i}^s + 1}{m_s + 2}$
- $P(C=Ham | \textbf{X}) \propto \dfrac{m_h}{m} \; \Pi \dfrac{x_{i}^h + 1}{m_h + 2}$
- Compute both numbers. Whichever class has a higher prob, classify as that one

::: fragment
::: callout-warning
Any astute computer scientists see a possible problem....
:::
:::

## Floating point number inaccuracies

```{python}
#| echo: true
import math
probs = [0.01] * 20
final_prob = math.prod(probs)
print(f"Probabilities: {probs}")
print(f"Final Probability: {final_prob}")
```

::: fragment
```{python}
#| echo: true
import math
probs = [0.01] * 10_000
final_prob = math.prod(probs)
print(f"Final Probability: {final_prob}")
```
:::

::: fragment
::: question
Any one have any ideas to fix this?
:::
:::

## Log function {.smaller}

::: incremental
- $P(C=Spam | \textbf{X}) \propto \dfrac{m_s}{m} \; \prod \dfrac{x_{i}^s + 1}{m_s + 2}$
- $P(C=Spam | \textbf{X}) \propto \ln \left( \dfrac{m_s}{m} \; \prod \dfrac{x_{i}^s + 1}{m_s + 2} \right)$
- What is this $\log(a \cdot b) = ?$
- $\log(a \cdot b) = \log(a) + \log(b)$
- $P(C=Spam | \textbf{X}) \propto \ln \left(\dfrac{m_s}{m} \right) + \; \sum \ln \left(\dfrac{x_{i}^s + 1}{m_s + 2}\right)$
:::

## Log Functon Graph

::: columns

:::{.column width="50%"}
```{python}
#| echo: true

x = np.linspace(0, 1, 1000000)
y = np.log(x)
plt.plot(x, y)
```
:::

:::{.column width="50%"}
::: callout-tip
We can transfrom our probabilties with log and make everything numerically stable!!
:::
:::
:::





<!-- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/210-naive-bayes.pdf -->
  


  <!-- https://deepnote.com/@emmanuelmicron/Spam-Detection-using-Multinomial-Naive-Bayes-Model-a9a6bc89-e723-4c32-8679-86a5ff1978f3 -->